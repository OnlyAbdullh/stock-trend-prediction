{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import data and sort",
   "id": "74bf51213a69900"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T11:04:04.296928300Z",
     "start_time": "2026-01-25T11:04:04.070287100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ],
   "id": "1a6f272b95bc95a6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T11:04:42.073390600Z",
     "start_time": "2026-01-25T11:04:42.036431600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# df = pd.read_csv('../data/interim/train_clean_2010.csv')\n",
    "# df = pd.read_csv('../data/interim/train_features_2010.csv')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n"
   ],
   "id": "68e0a2664facbdcb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extract Features",
   "id": "653973b6766da54f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T11:05:49.542642300Z",
     "start_time": "2026-01-25T11:05:00.581009800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHECKPOINT_FILE = 'dataset_checkpoint.pkl'\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    print(f\"Loading preprocessed dataset from {CHECKPOINT_FILE}...\")\n",
    "    with open(CHECKPOINT_FILE, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "\n",
    "    ticker_data = checkpoint['ticker_data']\n",
    "    samples = checkpoint['samples']\n",
    "    FEATURE_COLS = checkpoint['feature_cols']\n",
    "\n",
    "    print(f\"✓ Loaded {len(samples):,} samples from {len(ticker_data)} tickers\")\n",
    "    print(f\"✓ Features: {FEATURE_COLS}\")\n",
    "\n",
    "else:\n",
    "    print(\"No checkpoint found. Processing data from scratch...\")\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading CSV...\")\n",
    "    df = pd.read_csv('../data/interim/train_clean_2010.csv')\n",
    "\n",
    "    # Extract Features\n",
    "    print(\"Engineering features...\")\n",
    "    FEATURE_COLS = []\n",
    "    by_ticker = df.groupby('ticker')\n",
    "\n",
    "    df['return_1d'] = by_ticker['close'].pct_change()\n",
    "    FEATURE_COLS.append('return_1d')\n",
    "\n",
    "    df['log_return'] = np.log(df['close'] / by_ticker['close'].shift(1))\n",
    "    FEATURE_COLS.append('log_return')\n",
    "\n",
    "    df['volume_z'] = by_ticker['volume'].transform(\n",
    "        lambda x: (x - x.mean()) / (x.std() + 1e-6)\n",
    "    )\n",
    "    FEATURE_COLS.append('volume_z')\n",
    "\n",
    "    df['hl_range'] = (df['high'] - df['low']) / df['close']\n",
    "    FEATURE_COLS.append('hl_range')\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    print(f\"Data shape after cleaning: {df.shape}\")\n",
    "\n",
    "    # Convert to numpy arrays for FAST indexing\n",
    "    print(\"Converting to numpy arrays...\")\n",
    "    ticker_data = {}\n",
    "    samples = []\n",
    "    window_size = 60\n",
    "    horizon = 30\n",
    "\n",
    "    for ticker, group in tqdm(df.groupby('ticker'), desc=\"Processing tickers\"):\n",
    "        group = group.sort_values('date').reset_index(drop=True)\n",
    "        n = len(group)\n",
    "\n",
    "        if n < window_size + horizon:\n",
    "            continue\n",
    "\n",
    "        # Store as numpy arrays (CRITICAL for speed!)\n",
    "        ticker_data[ticker] = {\n",
    "            'features': group[FEATURE_COLS].values.astype(np.float32),\n",
    "            'close': group['close'].values.astype(np.float32)\n",
    "        }\n",
    "\n",
    "        # Generate sample indices\n",
    "        for i in range(window_size, n - horizon):\n",
    "            samples.append((ticker, i))\n",
    "\n",
    "    print(f\"✓ Processed {len(samples):,} samples from {len(ticker_data)} tickers\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    print(f\"Saving checkpoint to {CHECKPOINT_FILE}...\")\n",
    "    checkpoint = {\n",
    "        'ticker_data': ticker_data,\n",
    "        'samples': samples,\n",
    "        'feature_cols': FEATURE_COLS,\n",
    "        'window_size': window_size,\n",
    "        'horizon': horizon\n",
    "    }\n",
    "    with open(CHECKPOINT_FILE, 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "    print(\"✓ Checkpoint saved!\")\n"
   ],
   "id": "dcadedc732b217c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Processing data from scratch...\n",
      "Loading CSV...\n",
      "Engineering features...\n",
      "Data shape after cleaning: (12525922, 13)\n",
      "Converting to numpy arrays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tickers: 100%|██████████| 5000/5000 [00:12<00:00, 404.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 12,075,922 samples from 5000 tickers\n",
      "Saving checkpoint to dataset_checkpoint.pkl...\n",
      "✓ Checkpoint saved!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sliding window + label generation",
   "id": "dcac99cf952e378a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T11:08:43.957545Z",
     "start_time": "2026-01-25T11:08:43.935357400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "class FastStockDataset(Dataset):\n",
    "    \"\"\"Ultra-fast dataset using pre-converted numpy arrays\"\"\"\n",
    "    def __init__(self, ticker_data, samples, window_size=60, horizon=30):\n",
    "        self.ticker_data = ticker_data\n",
    "        self.samples = samples\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ticker, i = self.samples[idx]\n",
    "        data = self.ticker_data[ticker]\n",
    "\n",
    "        # Fast numpy slicing (not pandas!)\n",
    "        X = data['features'][i - self.window_size:i]\n",
    "\n",
    "        close_now = data['close'][i - 1]\n",
    "        close_future = data['close'][i + self.horizon]\n",
    "        y = float(close_future > close_now)\n",
    "\n",
    "        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "class StockSequenceDataset(Dataset):\n",
    "    \"\"\"Lazy-loading dataset - only computes windows when accessed\"\"\"\n",
    "    def __init__(self, df, feature_cols, window_size=60, horizon=30):\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "\n",
    "        # Pre-compute and cache ticker groups (lightweight)\n",
    "        self.ticker_groups = {}\n",
    "        self.samples = []  # (ticker, end_idx)\n",
    "\n",
    "        for ticker, group in df.groupby('ticker'):\n",
    "            # Sort once and store\n",
    "            group = group.sort_values('date').reset_index(drop=True)\n",
    "            n = len(group)\n",
    "\n",
    "            if n < window_size + horizon:\n",
    "                continue\n",
    "\n",
    "            # Store the group\n",
    "            self.ticker_groups[ticker] = group\n",
    "\n",
    "            # Generate valid sample indices\n",
    "            for i in range(window_size, n - horizon):\n",
    "                self.samples.append((ticker, i))\n",
    "\n",
    "        print(f\"Dataset initialized: {len(self.samples)} samples from {len(self.ticker_groups)} tickers\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ticker, i = self.samples[idx]\n",
    "        group = self.ticker_groups[ticker]\n",
    "\n",
    "        # Extract window of features\n",
    "        X = group[self.feature_cols][i - self.window_size:i].values\n",
    "\n",
    "        # Compute label: will price go up in 'horizon' days?\n",
    "        close_now = group['close'].iloc[i - 1]\n",
    "        close_future = group['close'].iloc[i + self.horizon]\n",
    "        y = float(close_future > close_now)\n",
    "\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ],
   "id": "fd46696748ccd9a8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T11:09:41.793429700Z",
     "start_time": "2026-01-25T11:09:41.774713700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(\"\\nCreating dataset...\")\n",
    "# import time\n",
    "# start = time.time()\n",
    "# dataset = FastStockDataset(df, FEATURE_COLS, window_size=60, horizon=30)\n",
    "# print(f\"Dataset created in {time.time() - start:.2f}s\")\n",
    "#\n",
    "# # Estimate dataset size\n",
    "# total_size_gb = (len(dataset) * 60 * len(FEATURE_COLS) * 4) / 1024**3\n",
    "# print(f\"Estimated dataset size: {total_size_gb:.2f} GB\")\n",
    "dataset = FastStockDataset(ticker_data, samples, window_size=60, horizon=30)\n"
   ],
   "id": "ebfb36eb27093cb7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T11:09:51.831694500Z",
     "start_time": "2026-01-25T11:09:44.819890400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Test 1: Check if workers are actually starting\n",
    "print(\"=\" * 60)\n",
    "print(\"DIAGNOSTIC TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 2: Single sample speed\n",
    "print(\"\\n1. Testing single sample access...\")\n",
    "start = time.time()\n",
    "try:\n",
    "    x, y = dataset[0]\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   ✓ Single sample: {elapsed:.4f}s\")\n",
    "    if elapsed > 0.01:\n",
    "        print(f\"   ⚠ WARNING: Too slow! Should be <0.001s\")\n",
    "    print(f\"   Shape: {x.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ✗ ERROR: {e}\")\n",
    "\n",
    "# Test 3: DataLoader with NO workers\n",
    "print(\"\\n2. Testing DataLoader with num_workers=0...\")\n",
    "test_loader = DataLoader(dataset, batch_size=128, num_workers=0)\n",
    "start = time.time()\n",
    "try:\n",
    "    xb, yb = next(iter(test_loader))\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   ✓ First batch (no workers): {elapsed:.2f}s\")\n",
    "    print(f\"   Shapes: {xb.shape}, {yb.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ✗ ERROR: {e}\")\n",
    "\n",
    "# Test 4: DataLoader WITH workers\n",
    "print(\"\\n3. Testing DataLoader with num_workers=4...\")\n",
    "test_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "start = time.time()\n",
    "try:\n",
    "    xb, yb = next(iter(test_loader))\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   ✓ First batch (4 workers): {elapsed:.2f}s\")\n",
    "\n",
    "    # Try second batch\n",
    "    start = time.time()\n",
    "    xb, yb = next(iter(test_loader))\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   ✓ Second batch: {elapsed:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"   ✗ ERROR: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test 5: GPU availability\n",
    "print(\"\\n4. GPU Check...\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Test 6: Simple training step\n",
    "print(\"\\n5. Testing single training step...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_loader = DataLoader(dataset, batch_size=128, num_workers=0)\n",
    "xb, yb = next(iter(test_loader))\n",
    "\n",
    "from torch import nn\n",
    "model = nn.Linear(x.shape[1], 1).to(device)  # Simple model\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "start = time.time()\n",
    "try:\n",
    "    xb_flat = xb.view(xb.shape[0], -1).to(device)\n",
    "    yb = yb.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xb_flat).squeeze()\n",
    "    loss = criterion(logits, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   ✓ Training step: {elapsed:.4f}s\")\n",
    "    print(f\"   Loss: {loss.item():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ✗ ERROR: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "e563c896c0c59900",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIAGNOSTIC TEST\n",
      "============================================================\n",
      "\n",
      "1. Testing single sample access...\n",
      "   ✓ Single sample: 0.0335s\n",
      "   ⚠ WARNING: Too slow! Should be <0.001s\n",
      "   Shape: torch.Size([60, 4])\n",
      "\n",
      "2. Testing DataLoader with num_workers=0...\n",
      "   ✓ First batch (no workers): 0.05s\n",
      "   Shapes: torch.Size([128, 60, 4]), torch.Size([128])\n",
      "\n",
      "3. Testing DataLoader with num_workers=4...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T10:32:04.366692700Z",
     "start_time": "2026-01-25T10:32:04.301369900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optimized DataLoader settings for RTX 3060\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4096,         # Large batch for GPU\n",
    "    shuffle=True,\n",
    "    num_workers=4,           # Workers to prepare batches\n",
    "    pin_memory=True,         # Fast CPU→GPU transfer\n",
    "    persistent_workers=True, # Keep workers alive\n",
    "    prefetch_factor=3        # 12 batches ready (4 workers × 3)\n",
    ")\n",
    "\n",
    "print(f\"DataLoader ready with {len(loader)} batches\")"
   ],
   "id": "37fb4602ea44329d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader ready with 2949 batches\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T09:35:13.927516Z",
     "start_time": "2026-01-25T09:35:08.581373200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xb , yb = next(iter(loader))\n",
    "print(xb.shape, yb.shape)"
   ],
   "id": "553711136bf04064",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 60, 4]) torch.Size([8192])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GRU",
   "id": "5b31ad73898c549c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T10:32:21.562043200Z",
     "start_time": "2026-01-25T10:32:21.544896600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size=256, num_layers=2):  # Bigger!\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,      # Stack 2 layers\n",
    "            batch_first=True,\n",
    "            dropout=0.2                  # Add dropout between layers\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)\n",
    "        x = self.fc1(h[-1])\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x).squeeze(1)\n",
    "\n",
    "# Create bigger model"
   ],
   "id": "85abbc01fede7e1a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SETUP TRAINING",
   "id": "f7cdfaf4774d1284"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T10:32:26.127498600Z",
     "start_time": "2026-01-25T10:32:23.452378500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_features = len(FEATURE_COLS)\n",
    "\n",
    "model = GRUClassifier(num_features, hidden_size=256, num_layers=2).to(device)\n",
    "\n",
    "# Mixed precision for RTX 3060\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "id": "384cba698a96cfd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training loop",
   "id": "4f0aa3f218f460bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T10:39:38.990911400Z",
     "start_time": "2026-01-25T10:32:41.194074900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for batch_idx, (xb, yb) in enumerate(progress_bar):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # Faster than zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update progress bar every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}: Average Loss = {avg_loss:.4f}\")"
   ],
   "id": "79da96a6d5de3f98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/2949 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accuracy check",
   "id": "96738ae38e42e270"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nEvaluating model...\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in tqdm(loader, desc=\"Evaluating\"):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(xb)\n",
    "\n",
    "        preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "\n",
    "        correct += (preds == yb.long()).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Class balance check\n",
    "print(f\"Positive samples: {sum(all_labels)/len(all_labels):.2%}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9. SAVE MODEL\n",
    "# ============================================================\n",
    "torch.save(model.state_dict(), 'gru_model.pth')\n",
    "print(\"\\nModel saved to 'gru_model.pth'\")"
   ],
   "id": "bd85fa86153394dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
