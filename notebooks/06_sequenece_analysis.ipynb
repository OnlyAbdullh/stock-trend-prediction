{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:01:51.701714300Z",
     "start_time": "2026-01-28T15:01:51.565490900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stock Price Prediction - Feature Analysis for RNN Sequence Selection\n",
    "# Analyzing features to determine optimal sequence length for LSTM/GRU models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STOCK PRICE PREDICTION - FEATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nObjective: Determine optimal sequence length for RNN input\")\n",
    "print(\"Target: Predict if close price > current price after 30 trading days\")\n",
    "print(\"=\"*80)"
   ],
   "id": "484f1387dbb2e619",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STOCK PRICE PREDICTION - FEATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Objective: Determine optimal sequence length for RNN input\n",
      "Target: Predict if close price > current price after 30 trading days\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:02:08.778229800Z",
     "start_time": "2026-01-28T15:01:54.435440300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "14e5e8d25a80716a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] LOADING DATA...\n",
      "Total records: 12,269,060\n",
      "Unique tickers: 4,925\n",
      "date range: 2010-01-04 00:00:00 to 2024-09-23 00:00:00\n",
      "\n",
      "Memory usage: 1645.91 MB\n",
      "\n",
      "================================================================================\n",
      "DATA OVERVIEW\n",
      "================================================================================\n",
      "     ticker       date       open       high        low      close     volume  \\\n",
      "0  ticker_1 2010-01-04  27.875437  28.009543  27.570655  27.662090  2142300.0   \n",
      "1  ticker_1 2010-01-05  27.729151  27.814489  27.131774  27.302454  2856000.0   \n",
      "2  ticker_1 2010-01-06  27.278065  27.729145  27.278065  27.595039  2035400.0   \n",
      "3  ticker_1 2010-01-07  27.637703  27.643798  27.375590  27.497503  1993400.0   \n",
      "4  ticker_1 2010-01-08  27.424356  27.613320  27.253676  27.582842  1306400.0   \n",
      "\n",
      "   dividends  stock_splits  missing_days    return  return_is_outlier  \n",
      "0        0.0           0.0             0       NaN              False  \n",
      "1        0.0           0.0             0 -0.013001              False  \n",
      "2        0.0           0.0             0  0.010716              False  \n",
      "3        0.0           0.0             0 -0.003535              False  \n",
      "4        0.0           0.0             0  0.003104              False  \n",
      "\n",
      "Data types:\n",
      "ticker                       object\n",
      "date                 datetime64[ns]\n",
      "open                        float64\n",
      "high                        float64\n",
      "low                         float64\n",
      "close                       float64\n",
      "volume                      float64\n",
      "dividends                   float64\n",
      "stock_splits                float64\n",
      "missing_days                  int64\n",
      "return                      float64\n",
      "return_is_outlier              bool\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "ticker                  0\n",
      "date                    0\n",
      "open                    0\n",
      "high                    0\n",
      "low                     0\n",
      "close                   0\n",
      "volume                  0\n",
      "dividends               0\n",
      "stock_splits            0\n",
      "missing_days            0\n",
      "return               4925\n",
      "return_is_outlier       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:02:49.365466Z",
     "start_time": "2026-01-28T15:02:15.140526800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20',\n",
    "        'close_30d_future'\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\nâœ“ Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "6ec32e490f9d4f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[2] ENGINEERING FEATURES...\n",
      "================================================================================\n",
      " - Target variable...\n",
      "\n",
      "âœ“ Feature engineering complete!\n",
      "Total features created: 25\n",
      "Rows with complete features: 12,121,310\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "863e4fa68b614557"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "f1d478538e04da36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:05:27.786150900Z",
     "start_time": "2026-01-28T15:02:59.452853100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Optimized Scaling Analysis - V3 Only (Memory Efficient)\n",
    "=========================================================\n",
    "This script performs comprehensive normalization analysis with:\n",
    "1. Winsorization (enhanced)\n",
    "2. Log transformation for skewed features\n",
    "3. Special handling for cumulative features\n",
    "4. Two-way comparison only (Before vs V3)\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler as ZScoreScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE DEFINITIONS\n",
    "# ============================================================\n",
    "\n",
    "feature_columns = [\n",
    "    # Price Features (2)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (6)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (2)\n",
    "    'PVT_cumsum',\n",
    "    'MOBV',\n",
    "\n",
    "    # Directional Movement (1)\n",
    "    'MTM',\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',\n",
    "    'VHF',\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',\n",
    "]\n",
    "\n",
    "# Feature categorization\n",
    "no_need_scaling = [\n",
    "    'is_up_day',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'price_position_20',\n",
    "]\n",
    "\n",
    "robust_scaling_features = [\n",
    "    'distance_from_high',\n",
    "    'downside_deviation_10',\n",
    "    'high_low_ratio',\n",
    "    'low_to_close_ratio',\n",
    "    'max_drawdown_20',\n",
    "    'parkinson_volatility',\n",
    "    'recent_high_20',\n",
    "    'volatility_20',\n",
    "    'VHF',\n",
    "    'MOBV',\n",
    "    'PVT_cumsum'\n",
    "]\n",
    "\n",
    "zscore_features = [\n",
    "    'ADTM',\n",
    "    'daily_return',\n",
    "    'MA_60_slope',\n",
    "    'MTM',\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'PSY',\n",
    "    'RSI_14',\n",
    "]\n",
    "\n",
    "standard_scaler_features = [\n",
    "    'K'\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def analyze_distribution(df, feature, prefix=\"\"):\n",
    "    \"\"\"Comprehensive distribution analysis\"\"\"\n",
    "    data = df[feature].dropna()\n",
    "\n",
    "    stats_dict = {\n",
    "        'count': len(data),\n",
    "        'mean': data.mean(),\n",
    "        'std': data.std(),\n",
    "        'min': data.min(),\n",
    "        'q1': data.quantile(0.01),\n",
    "        'q5': data.quantile(0.05),\n",
    "        'q25': data.quantile(0.25),\n",
    "        'median': data.median(),\n",
    "        'q75': data.quantile(0.75),\n",
    "        'q95': data.quantile(0.95),\n",
    "        'q99': data.quantile(0.99),\n",
    "        'max': data.max(),\n",
    "        'skewness': stats.skew(data),\n",
    "        'kurtosis': stats.kurtosis(data),\n",
    "        'n_outliers_3std': ((data < (data.mean() - 3*data.std())) |\n",
    "                            (data > (data.mean() + 3*data.std()))).sum(),\n",
    "        'outlier_pct': ((data < (data.mean() - 3*data.std())) |\n",
    "                        (data > (data.mean() + 3*data.std()))).sum() / len(data) * 100\n",
    "    }\n",
    "\n",
    "    return pd.Series(stats_dict)\n",
    "\n",
    "\n",
    "def apply_winsorization(df, features, lower_pct=0.005, upper_pct=0.995):\n",
    "    \"\"\"Apply winsorization to specified features - IN PLACE for memory efficiency\"\"\"\n",
    "    winsor_params = {}\n",
    "\n",
    "    for feature in tqdm(features, desc=\"Winsorizing\"):\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        lower = df[feature].quantile(lower_pct)\n",
    "        upper = df[feature].quantile(upper_pct)\n",
    "        df[feature] = df[feature].clip(lower, upper)\n",
    "        winsor_params[feature] = {'lower': lower, 'upper': upper}\n",
    "\n",
    "    return winsor_params\n",
    "\n",
    "\n",
    "def apply_log_transform(df, features):\n",
    "    \"\"\"Apply log transformation for highly skewed features - IN PLACE\"\"\"\n",
    "    log_params = {}\n",
    "\n",
    "    for feature in tqdm(features, desc=\"Log transforming\"):\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Handle negative values: log(|x|) * sign(x)\n",
    "        data = df[feature]\n",
    "        df[feature] = np.log1p(np.abs(data)) * np.sign(data)\n",
    "        log_params[feature] = 'applied'\n",
    "\n",
    "    return log_params\n",
    "\n",
    "\n",
    "def handle_cumulative_features(df, ticker_col='ticker'):\n",
    "    \"\"\"Convert cumulative features to differences - IN PLACE\"\"\"\n",
    "    cumulative_features = ['PVT_cumsum', 'MOBV']\n",
    "\n",
    "    for feature in cumulative_features:\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Calculate percentage change per ticker\n",
    "        pct_change = df.groupby(ticker_col)[feature].pct_change()\n",
    "        # Fill first NaN with 0\n",
    "        pct_change = pct_change.fillna(0)\n",
    "        # Replace inf with 0\n",
    "        pct_change = pct_change.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "        # Replace the original feature with percentage change\n",
    "        df[feature] = pct_change\n",
    "\n",
    "\n",
    "def apply_scaling(df, robust_feats, zscore_feats, standard_feats):\n",
    "    \"\"\"Apply appropriate scaling to features - IN PLACE\"\"\"\n",
    "\n",
    "    # RobustScaler\n",
    "    if robust_feats:\n",
    "        robust_scaler = RobustScaler()\n",
    "        valid_robust = [f for f in robust_feats if f in df.columns]\n",
    "        if valid_robust:\n",
    "            df[valid_robust] = robust_scaler.fit_transform(df[valid_robust])\n",
    "\n",
    "    # Z-Score (StandardScaler)\n",
    "    if zscore_feats:\n",
    "        z_scaler = ZScoreScaler()\n",
    "        valid_zscore = [f for f in zscore_feats if f in df.columns]\n",
    "        if valid_zscore:\n",
    "            df[valid_zscore] = z_scaler.fit_transform(df[valid_zscore])\n",
    "\n",
    "    # StandardScaler\n",
    "    if standard_feats:\n",
    "        std_scaler = StandardScaler()\n",
    "        valid_std = [f for f in standard_feats if f in df.columns]\n",
    "        if valid_std:\n",
    "            df[valid_std] = std_scaler.fit_transform(df[valid_std])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. INITIAL ANALYSIS\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š OPTIMIZED SCALING ANALYSIS (V3 ONLY)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStep 1: Analyzing original distribution...\")\n",
    "\n",
    "# Assuming df_features is already loaded\n",
    "# If not, uncomment and load:\n",
    "# df_features = pd.read_csv('data/processed/data.csv')\n",
    "\n",
    "before_stats = {}\n",
    "for feature in tqdm(feature_columns, desc=\"Analyzing features\"):\n",
    "    before_stats[feature] = analyze_distribution(df_features, feature)\n",
    "\n",
    "df_before = pd.DataFrame(before_stats).T\n",
    "df_before['scaling_method'] = df_before.index.map(\n",
    "    lambda x: 'none' if x in no_need_scaling\n",
    "    else 'robust' if x in robust_scaling_features\n",
    "    else 'zscore' if x in zscore_features\n",
    "    else 'standard'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” OUTLIER ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ”¥ Features with HIGH OUTLIERS (>2% outliers):\")\n",
    "high_outliers = df_before[df_before['outlier_pct'] > 2.0].sort_values('outlier_pct', ascending=False)\n",
    "if len(high_outliers) > 0:\n",
    "    print(high_outliers[['outlier_pct', 'skewness', 'min', 'max', 'scaling_method']])\n",
    "else:\n",
    "    print(\"None found!\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Features with EXTREME SKEWNESS (|skew| > 5):\")\n",
    "extreme_skew = df_before[abs(df_before['skewness']) > 5].sort_values('skewness', key=abs, ascending=False)\n",
    "if len(extreme_skew) > 0:\n",
    "    print(extreme_skew[['skewness', 'kurtosis', 'outlier_pct', 'scaling_method']])\n",
    "else:\n",
    "    print(\"None found!\")\n",
    "\n",
    "print(\"\\nðŸ’¥ Features with EXTREME VALUES:\")\n",
    "extreme_vals = df_before[(df_before['max'] > 100) | (df_before['min'] < -10)]\n",
    "if len(extreme_vals) > 0:\n",
    "    print(extreme_vals[['min', 'max', 'mean', 'std', 'outlier_pct', 'scaling_method']])\n",
    "else:\n",
    "    print(\"None found!\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. PREPARE V3 (HYBRID) VERSION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”§ PREPARING V3 (HYBRID) VERSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a copy for V3\n",
    "df_v3 = df_features.copy()\n",
    "\n",
    "# Step 1: Handle cumulative features first\n",
    "print(\"\\n[V3] Step 1: Converting cumulative features to percentage change...\")\n",
    "if 'ticker' in df_v3.columns:\n",
    "    handle_cumulative_features(df_v3, 'ticker')\n",
    "    # Update robust scaling features list\n",
    "    robust_scaling_features_v3 = robust_scaling_features.copy()\n",
    "else:\n",
    "    robust_scaling_features_v3 = robust_scaling_features.copy()\n",
    "\n",
    "# Step 2: Log transform for highly skewed features (skew > 10)\n",
    "print(\"\\n[V3] Step 2: Identifying highly skewed features...\")\n",
    "highly_skewed = df_before[abs(df_before['skewness']) > 10].index.tolist()\n",
    "features_to_log = [f for f in highly_skewed if f in ['high_low_ratio', 'volatility_20',\n",
    "                                                       'parkinson_volatility', 'recent_high_20']]\n",
    "if features_to_log:\n",
    "    print(f\"  â†’ Log transforming: {features_to_log}\")\n",
    "    log_params = apply_log_transform(df_v3, features_to_log)\n",
    "\n",
    "# Step 3: Aggressive winsorization (0.5% - 99.5%)\n",
    "print(\"\\n[V3] Step 3: Applying aggressive winsorization...\")\n",
    "features_to_winsorize_v3 = [\n",
    "    'daily_return', 'MA_60_slope', 'MTM', 'PVT_cumsum', 'MOBV',\n",
    "    'recent_high_20', 'volatility_20', 'parkinson_volatility',\n",
    "    'high_low_ratio', 'distance_from_high', 'low_to_close_ratio',\n",
    "    'downside_deviation_10', 'VHF'\n",
    "]\n",
    "# Remove already log-transformed features from winsorization\n",
    "features_to_winsorize_v3 = [f for f in features_to_winsorize_v3 if f not in features_to_log]\n",
    "params_v3 = apply_winsorization(df_v3, features_to_winsorize_v3, 0.005, 0.995)\n",
    "\n",
    "print(\"\\nWinsorization parameters:\")\n",
    "for feat, vals in params_v3.items():\n",
    "    print(f\"  {feat:25s}: [{vals['lower']:>12.4f}, {vals['upper']:>12.4f}]\")\n",
    "\n",
    "# Step 4: Apply scaling\n",
    "print(\"\\n[V3] Step 4: Applying scaling transformations...\")\n",
    "apply_scaling(df_v3, robust_scaling_features_v3, zscore_features, standard_scaler_features)\n",
    "\n",
    "print(\"âœ“ V3 version prepared and scaled!\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. ANALYSIS AFTER SCALING\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ANALYSIS AFTER SCALING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "after_stats_v3 = {}\n",
    "for feature in tqdm(feature_columns, desc=\"Analyzing scaled features\"):\n",
    "    if feature in df_v3.columns:\n",
    "        after_stats_v3[feature] = analyze_distribution(df_v3, feature)\n",
    "\n",
    "df_after_v3 = pd.DataFrame(after_stats_v3).T\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. QUALITY METRICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ QUALITY METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_quality_score(df_after, features_to_check):\n",
    "    \"\"\"Calculate normalization quality score\"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    for feat in features_to_check:\n",
    "        if feat not in df_after.index:\n",
    "            continue\n",
    "\n",
    "        mean = df_after.loc[feat, 'mean']\n",
    "        std = df_after.loc[feat, 'std']\n",
    "\n",
    "        # Score: closer to (0, 1) is better\n",
    "        mean_score = max(0, 1 - abs(mean) * 2)  # Penalty if |mean| > 0.5\n",
    "        std_score = max(0, 1 - abs(std - 1.0) * 2)  # Penalty if std far from 1\n",
    "\n",
    "        total_score = (mean_score + std_score) / 2\n",
    "        scores[feat] = {\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'mean_score': mean_score,\n",
    "            'std_score': std_score,\n",
    "            'total_score': total_score\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(scores).T\n",
    "\n",
    "# Features that should be normalized (exclude 'none' category)\n",
    "features_to_evaluate = [f for f in feature_columns if f not in no_need_scaling]\n",
    "\n",
    "print(\"\\nðŸ“Š V3 (HYBRID) QUALITY SCORE:\")\n",
    "quality_v3 = calculate_quality_score(df_after_v3, features_to_evaluate)\n",
    "print(f\"Average Score: {quality_v3['total_score'].mean():.3f}\")\n",
    "print(\"\\nTop 5 best performers:\")\n",
    "print(quality_v3.nlargest(5, 'total_score')[['mean', 'std', 'total_score']])\n",
    "print(\"\\nWorst 5 performers:\")\n",
    "print(quality_v3.nsmallest(5, 'total_score')[['mean', 'std', 'total_score']])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. COMPARISON TABLE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ BEFORE vs AFTER COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'before_mean': df_before['mean'],\n",
    "    'before_std': df_before['std'],\n",
    "    'before_skew': df_before['skewness'],\n",
    "    'before_outliers': df_before['outlier_pct'],\n",
    "\n",
    "    'after_mean': df_after_v3['mean'],\n",
    "    'after_std': df_after_v3['std'],\n",
    "    'after_skew': df_after_v3['skewness'],\n",
    "\n",
    "    'method': df_before['scaling_method']\n",
    "})\n",
    "\n",
    "# Calculate improvement\n",
    "comparison['std_improvement'] = abs(comparison['before_std'] - 1.0) - abs(comparison['after_std'] - 1.0)\n",
    "comparison['skew_improvement'] = abs(comparison['before_skew']) - abs(comparison['after_skew'])\n",
    "\n",
    "print(\"\\nâœ… Features with BEST STD improvement:\")\n",
    "improved = comparison.sort_values('std_improvement', ascending=False).head(10)\n",
    "print(improved[['before_std', 'after_std', 'std_improvement', 'method']])\n",
    "\n",
    "print(\"\\nâœ… Features with BEST SKEW improvement:\")\n",
    "skew_improved = comparison.sort_values('skew_improvement', ascending=False).head(10)\n",
    "print(skew_improved[['before_skew', 'after_skew', 'skew_improvement', 'method']])\n",
    "\n",
    "print(\"\\nâš ï¸ Features that still need work:\")\n",
    "needs_work = comparison[~comparison.index.isin(no_need_scaling)]\n",
    "needs_work['quality'] = abs(needs_work['after_mean']) + abs(needs_work['after_std'] - 1.0)\n",
    "worst = needs_work.sort_values('quality', ascending=False).head(10)\n",
    "print(worst[['after_mean', 'after_std', 'after_skew', 'method']])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. SAVE RESULTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ’¾ SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_file = \"scaling_analysis_v3_only.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    df_before.to_excel(writer, sheet_name='00_Before_Scaling')\n",
    "    df_after_v3.to_excel(writer, sheet_name='01_After_V3_Hybrid')\n",
    "    comparison.to_excel(writer, sheet_name='02_Comparison')\n",
    "    quality_v3.to_excel(writer, sheet_name='03_Quality_Score')\n",
    "\n",
    "print(f\"âœ“ Report saved: {output_file}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. VISUALIZATIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“¸ GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "IMAGE_DIR = \"Images_scaling_v3\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "for feature in tqdm(feature_columns, desc=\"Creating plots\"):\n",
    "    if feature not in df_features.columns:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Determine scaling method\n",
    "    if feature in robust_scaling_features:\n",
    "        method = 'RobustScaler'\n",
    "    elif feature in zscore_features:\n",
    "        method = 'StandardScaler (Z-Score)'\n",
    "    elif feature in standard_scaler_features:\n",
    "        method = 'StandardScaler'\n",
    "    else:\n",
    "        method = 'No Scaling'\n",
    "\n",
    "    fig.suptitle(f'{feature} - {method}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Original\n",
    "    axes[0].hist(df_features[feature].dropna(), bins=100, alpha=0.7,\n",
    "                 color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title('BEFORE (Original)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Value')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    mean_orig = df_features[feature].mean()\n",
    "    std_orig = df_features[feature].std()\n",
    "    skew_orig = stats.skew(df_features[feature].dropna())\n",
    "    axes[0].text(0.02, 0.98,\n",
    "                 f'Mean: {mean_orig:.3f}\\nStd: {std_orig:.3f}\\nSkew: {skew_orig:.2f}',\n",
    "                 transform=axes[0].transAxes, verticalalignment='top',\n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    # 2. V3 (Hybrid)\n",
    "    if feature in df_v3.columns:\n",
    "        axes[1].hist(df_v3[feature].dropna(), bins=100, alpha=0.7,\n",
    "                     color='purple', edgecolor='black')\n",
    "        axes[1].set_title('AFTER (V3: Hybrid)', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Value')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        mean_v3 = df_v3[feature].mean()\n",
    "        std_v3 = df_v3[feature].std()\n",
    "        skew_v3 = stats.skew(df_v3[feature].dropna())\n",
    "        axes[1].text(0.02, 0.98,\n",
    "                     f'Mean: {mean_v3:.3f}\\nStd: {std_v3:.3f}\\nSkew: {skew_v3:.2f}',\n",
    "                     transform=axes[1].transAxes, verticalalignment='top',\n",
    "                     bbox=dict(boxstyle='round', facecolor='purple', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'{feature}_comparison.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"âœ“ Saved {len(feature_columns)} visualizations to: {IMAGE_DIR}/\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "RESULTS:\n",
    "========\n",
    "âœ“ Average Quality Score: {quality_v3['total_score'].mean():.3f}\n",
    "âœ“ Features improved: {(comparison['std_improvement'] > 0).sum()} / {len(comparison)}\n",
    "âœ“ Average STD improvement: {comparison['std_improvement'].mean():.3f}\n",
    "âœ“ Average SKEW improvement: {comparison['skew_improvement'].mean():.3f}\n",
    "\n",
    "FILES GENERATED:\n",
    "1. Excel Report: {output_file}\n",
    "2. Visualizations: {IMAGE_DIR}/ ({len(feature_columns)} images)\n",
    "\n",
    "TRANSFORMATIONS APPLIED:\n",
    "- Cumulative features converted to percentage change\n",
    "- Log transform for highly skewed features (|skew| > 10)\n",
    "- Aggressive winsorization (0.5% - 99.5%)\n",
    "- RobustScaler for outlier-prone features\n",
    "- StandardScaler (Z-Score) for distribution-based features\n",
    "- StandardScaler for stochastic features\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Review Excel report for detailed statistics\n",
    "2. Check visualizations for distribution changes\n",
    "3. Use df_v3 for model training (scaled and ready)\n",
    "4. Monitor features in \"needs work\" list for further optimization\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ… ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# Optional: Save the scaled dataset\n",
    "# ============================================================\n",
    "print(\"\\nðŸ’¾ Saving scaled dataset...\")\n",
    "# Uncomment to save:\n",
    "# df_v3.to_csv('data/processed/data_scaled_v3.csv', index=False)\n",
    "# print(\"âœ“ Scaled dataset saved to: data/processed/data_scaled_v3.csv\")"
   ],
   "id": "f833ad515343a7cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“Š OPTIMIZED SCALING ANALYSIS (V3 ONLY)\n",
      "================================================================================\n",
      "\n",
      "Step 1: Analyzing original distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:40<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ” OUTLIER ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ”¥ Features with HIGH OUTLIERS (>2% outliers):\n",
      "                    outlier_pct  skewness       min          max  \\\n",
      "recent_high_20         2.429314  3.209126  0.106040  1010.080017   \n",
      "distance_from_high     2.048682 -2.500268 -0.992353     0.022222   \n",
      "\n",
      "                   scaling_method  \n",
      "recent_high_20             robust  \n",
      "distance_from_high         robust  \n",
      "\n",
      "ðŸ“ˆ Features with EXTREME SKEWNESS (|skew| > 5):\n",
      "                         skewness      kurtosis  outlier_pct scaling_method\n",
      "daily_return          2131.687999  6.183759e+06     0.146941         zscore\n",
      "volatility_20          508.418004  3.244742e+05     0.103881         robust\n",
      "MA_60_slope             78.938633  3.239143e+04     1.049713         zscore\n",
      "PVT_cumsum              29.463500  1.268447e+03     0.633406         robust\n",
      "MOBV                    24.705407  1.218629e+03     0.899498         robust\n",
      "high_low_ratio          21.816197  2.971159e+03     1.365516         robust\n",
      "parkinson_volatility    14.469786  1.920736e+03     1.483447         robust\n",
      "price_to_MA60            6.647672  7.637033e+02     1.555017         zscore\n",
      "price_to_MA20            5.875331  4.820860e+02     1.586739         zscore\n",
      "\n",
      "ðŸ’¥ Features with EXTREME VALUES:\n",
      "                         min           max          mean           std  \\\n",
      "daily_return   -9.750000e-01  2.573333e+02  6.663312e-04  8.726612e-02   \n",
      "recent_high_20  1.060400e-01  1.010080e+03  3.313228e+01  4.181002e+01   \n",
      "PVT_cumsum     -3.776399e+08  4.411416e+09  7.364432e+06  7.580017e+07   \n",
      "MOBV           -7.958665e+09  2.102035e+10  5.156853e+07  2.903180e+08   \n",
      "MTM            -4.004200e+02  3.858000e+02  5.910726e-02  4.304235e+00   \n",
      "K              -3.552714e-05  1.138889e+02  5.139039e+01  3.014927e+01   \n",
      "\n",
      "                outlier_pct scaling_method  \n",
      "daily_return       0.146941         zscore  \n",
      "recent_high_20     2.429314         robust  \n",
      "PVT_cumsum         0.633406         robust  \n",
      "MOBV               0.899498         robust  \n",
      "MTM                1.762487         zscore  \n",
      "K                  0.000000       standard  \n",
      "\n",
      "================================================================================\n",
      "ðŸ”§ PREPARING V3 (HYBRID) VERSION\n",
      "================================================================================\n",
      "\n",
      "[V3] Step 1: Converting cumulative features to percentage change...\n",
      "\n",
      "[V3] Step 2: Identifying highly skewed features...\n",
      "  â†’ Log transforming: ['high_low_ratio', 'volatility_20', 'parkinson_volatility']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log transforming: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[V3] Step 3: Applying aggressive winsorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Winsorizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Winsorization parameters:\n",
      "  daily_return             : [     -0.1137,       0.1333]\n",
      "  MA_60_slope              : [     -0.3472,       0.4468]\n",
      "  MTM                      : [    -17.7100,      16.5700]\n",
      "  PVT_cumsum               : [     -0.9642,       1.0049]\n",
      "  MOBV                     : [     -1.6696,       1.6758]\n",
      "  recent_high_20           : [      0.7000,     254.9470]\n",
      "  distance_from_high       : [     -0.5059,       0.0000]\n",
      "  low_to_close_ratio       : [      0.5273,       1.0000]\n",
      "  downside_deviation_10    : [      0.0005,       0.0819]\n",
      "  VHF                      : [      0.1405,       0.7235]\n",
      "\n",
      "[V3] Step 4: Applying scaling transformations...\n",
      "âœ“ V3 version prepared and scaled!\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ANALYSIS AFTER SCALING\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing scaled features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:41<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ QUALITY METRICS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š V3 (HYBRID) QUALITY SCORE:\n",
      "Average Score: 0.749\n",
      "\n",
      "Top 5 best performers:\n",
      "                       mean  std  total_score\n",
      "price_to_MA5  -4.151231e-18  1.0          1.0\n",
      "price_to_MA20  7.116396e-18  1.0          1.0\n",
      "price_to_MA60 -7.116396e-18  1.0          1.0\n",
      "K             -1.613050e-16  1.0          1.0\n",
      "RSI_14         4.554494e-16  1.0          1.0\n",
      "\n",
      "Worst 5 performers:\n",
      "                    mean        std  total_score\n",
      "recent_high_20  0.453922   1.337096     0.208981\n",
      "volatility_20   0.315513   1.400812     0.283675\n",
      "PVT_cumsum      0.076808  14.331306     0.423192\n",
      "high_low_ratio  0.335193   1.216633     0.448174\n",
      "MOBV            0.016036   6.595293     0.483964\n",
      "\n",
      "================================================================================\n",
      "ðŸ“‹ BEFORE vs AFTER COMPARISON\n",
      "================================================================================\n",
      "\n",
      "âœ… Features with BEST STD improvement:\n",
      "                         before_std  after_std  std_improvement    method\n",
      "MOBV                   2.903180e+08   6.595293     2.903180e+08    robust\n",
      "PVT_cumsum             7.580017e+07  14.331306     7.580015e+07    robust\n",
      "recent_high_20         4.181002e+01   1.337096     4.047292e+01    robust\n",
      "K                      3.014927e+01   1.000000     2.914927e+01  standard\n",
      "RSI_14                 1.707854e+01   1.000000     1.607854e+01    zscore\n",
      "PSY                    1.456826e+01   1.000000     1.356826e+01    zscore\n",
      "MTM                    4.304235e+00   1.000000     3.304235e+00    zscore\n",
      "price_to_MA5           3.671306e-02   1.000000     9.632869e-01    zscore\n",
      "downside_deviation_10  1.343409e-02   1.059274     9.272916e-01    robust\n",
      "price_to_MA20          8.162504e-02   1.000000     9.183749e-01    zscore\n",
      "\n",
      "âœ… Features with BEST SKEW improvement:\n",
      "                       before_skew  after_skew  skew_improvement  method\n",
      "daily_return           2131.687999    0.349846       2131.338153  zscore\n",
      "volatility_20           508.418004   21.023761        487.394243  robust\n",
      "MA_60_slope              78.938633    0.300250         78.638383  zscore\n",
      "PVT_cumsum               29.463500    0.351437         29.112063  robust\n",
      "MOBV                     24.705407    0.029668         24.675739  robust\n",
      "high_low_ratio           21.816197    5.277721         16.538476  robust\n",
      "parkinson_volatility     14.469786    4.781234          9.688552  robust\n",
      "downside_deviation_10     4.493302    2.468173          2.025129  robust\n",
      "MTM                      -1.686393   -0.247600          1.438793  zscore\n",
      "low_to_close_ratio       -2.464992   -1.988893          0.476099  robust\n",
      "\n",
      "âš ï¸ Features that still need work:\n",
      "                       after_mean  after_std  after_skew  method\n",
      "PVT_cumsum               0.076808  14.331306    0.351437  robust\n",
      "MOBV                     0.016036   6.595293    0.029668  robust\n",
      "recent_high_20           0.453922   1.337096    2.781419  robust\n",
      "volatility_20            0.315513   1.400812   21.023761  robust\n",
      "high_low_ratio           0.335193   1.216633    5.277721  robust\n",
      "parkinson_volatility     0.305523   1.120844    4.781234  robust\n",
      "VHF                      0.126636   0.764625    0.853321  robust\n",
      "downside_deviation_10    0.301545   1.059274    2.468173  robust\n",
      "distance_from_high      -0.330701   0.996515   -2.100966  robust\n",
      "max_drawdown_20         -0.268612   0.944818   -2.022940  robust\n",
      "\n",
      "================================================================================\n",
      "ðŸ’¾ SAVING RESULTS\n",
      "================================================================================\n",
      "âœ“ Report saved: scaling_analysis_v3_only.xlsx\n",
      "\n",
      "================================================================================\n",
      "ðŸ“¸ GENERATING VISUALIZATIONS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating plots: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:46<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved 25 visualizations to: Images_scaling_v3/\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "RESULTS:\n",
      "========\n",
      "âœ“ Average Quality Score: 0.749\n",
      "âœ“ Features improved: 21 / 25\n",
      "âœ“ Average STD improvement: 14644729.957\n",
      "âœ“ Average SKEW improvement: 111.293\n",
      "\n",
      "FILES GENERATED:\n",
      "1. Excel Report: scaling_analysis_v3_only.xlsx\n",
      "2. Visualizations: Images_scaling_v3/ (25 images)\n",
      "\n",
      "TRANSFORMATIONS APPLIED:\n",
      "- Cumulative features converted to percentage change\n",
      "- Log transform for highly skewed features (|skew| > 10)\n",
      "- Aggressive winsorization (0.5% - 99.5%)\n",
      "- RobustScaler for outlier-prone features\n",
      "- StandardScaler (Z-Score) for distribution-based features\n",
      "- StandardScaler for stochastic features\n",
      "\n",
      "NEXT STEPS:\n",
      "1. Review Excel report for detailed statistics\n",
      "2. Check visualizations for distribution changes\n",
      "3. Use df_v3 for model training (scaled and ready)\n",
      "4. Monitor features in \"needs work\" list for further optimization\n",
      "\n",
      "\n",
      "âœ… ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¾ Saving scaled dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features_scaled.describe()",
   "id": "ac79b62b439c3d7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "cb8e9af69968a3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "61c369a7c5beb523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "cd34fedd16ffb2a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "3db3374b37d5161f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 5000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 â­â­â­\n",
    "    'MOBV',                 # MI = 0.0209 â­â­\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 â­\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ac1312d7bbf04afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "5cacce17fbd4b72f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20','close_30d_future',\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\nâœ“ Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "4177fe98c9aaf3a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "ef062214d42f8b5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "e51fd72a1a8f1763"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: DATA QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] DATA QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "# ============================================================\n",
    "feature_columns = [\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 â­â­â­\n",
    "    'MOBV',                 # MI = 0.0209 â­â­\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 â­\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features\n",
    "\n",
    "]\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values in engineered features:\")\n",
    "missing_stats = df_features[feature_columns].isnull().sum()\n",
    "missing_pct = (missing_stats / len(df_features) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_stats,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df_features[feature_columns].describe())\n",
    "\n",
    "IMAGE_DIR = \"Images2\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Visualization: Distribution of features\n",
    "for feature in feature_columns:\n",
    "    data = df_features[feature].dropna()\n",
    "\n",
    "    # Remove extreme outliers for better visualization (keep 1st-99th percentile)\n",
    "    q1, q99 = data.quantile([0.01, 0.99])\n",
    "    data_filtered = data[(data >= q1) & (data <= q99)]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))  # wider figure\n",
    "    plt.hist(data_filtered, bins=100, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'{feature} Distribution\\n(1st-99th percentile)', fontsize=14)\n",
    "    plt.xlabel('Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(IMAGE_DIR, f'feature_{feature}.png'),\n",
    "        dpi=300,\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ],
   "id": "c24ea41df1ba4346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "46b315725a20227c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "f9e8fcac0ffa929d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "f8a5745e7bf6e107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "bc2220133a071d7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 1000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 â­â­â­\n",
    "    'MOBV',                 # MI = 0.0209 â­â­\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 â­\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features:\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\",\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "eac3f8e6a29c7983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6686ea2b553342fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
