{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:01:51.701714300Z",
     "start_time": "2026-01-28T15:01:51.565490900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stock Price Prediction - Feature Analysis for RNN Sequence Selection\n",
    "# Analyzing features to determine optimal sequence length for LSTM/GRU models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STOCK PRICE PREDICTION - FEATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nObjective: Determine optimal sequence length for RNN input\")\n",
    "print(\"Target: Predict if close price > current price after 30 trading days\")\n",
    "print(\"=\"*80)"
   ],
   "id": "484f1387dbb2e619",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STOCK PRICE PREDICTION - FEATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Objective: Determine optimal sequence length for RNN input\n",
      "Target: Predict if close price > current price after 30 trading days\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:02:08.778229800Z",
     "start_time": "2026-01-28T15:01:54.435440300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "14e5e8d25a80716a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] LOADING DATA...\n",
      "Total records: 12,269,060\n",
      "Unique tickers: 4,925\n",
      "date range: 2010-01-04 00:00:00 to 2024-09-23 00:00:00\n",
      "\n",
      "Memory usage: 1645.91 MB\n",
      "\n",
      "================================================================================\n",
      "DATA OVERVIEW\n",
      "================================================================================\n",
      "     ticker       date       open       high        low      close     volume  \\\n",
      "0  ticker_1 2010-01-04  27.875437  28.009543  27.570655  27.662090  2142300.0   \n",
      "1  ticker_1 2010-01-05  27.729151  27.814489  27.131774  27.302454  2856000.0   \n",
      "2  ticker_1 2010-01-06  27.278065  27.729145  27.278065  27.595039  2035400.0   \n",
      "3  ticker_1 2010-01-07  27.637703  27.643798  27.375590  27.497503  1993400.0   \n",
      "4  ticker_1 2010-01-08  27.424356  27.613320  27.253676  27.582842  1306400.0   \n",
      "\n",
      "   dividends  stock_splits  missing_days    return  return_is_outlier  \n",
      "0        0.0           0.0             0       NaN              False  \n",
      "1        0.0           0.0             0 -0.013001              False  \n",
      "2        0.0           0.0             0  0.010716              False  \n",
      "3        0.0           0.0             0 -0.003535              False  \n",
      "4        0.0           0.0             0  0.003104              False  \n",
      "\n",
      "Data types:\n",
      "ticker                       object\n",
      "date                 datetime64[ns]\n",
      "open                        float64\n",
      "high                        float64\n",
      "low                         float64\n",
      "close                       float64\n",
      "volume                      float64\n",
      "dividends                   float64\n",
      "stock_splits                float64\n",
      "missing_days                  int64\n",
      "return                      float64\n",
      "return_is_outlier              bool\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "ticker                  0\n",
      "date                    0\n",
      "open                    0\n",
      "high                    0\n",
      "low                     0\n",
      "close                   0\n",
      "volume                  0\n",
      "dividends               0\n",
      "stock_splits            0\n",
      "missing_days            0\n",
      "return               4925\n",
      "return_is_outlier       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:02:49.365466Z",
     "start_time": "2026-01-28T15:02:15.140526800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20',\n",
    "        'close_30d_future'\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\n‚úì Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "6ec32e490f9d4f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[2] ENGINEERING FEATURES...\n",
      "================================================================================\n",
      " - Target variable...\n",
      "\n",
      "‚úì Feature engineering complete!\n",
      "Total features created: 25\n",
      "Rows with complete features: 12,121,310\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "863e4fa68b614557"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "f1d478538e04da36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:20:17.090765600Z",
     "start_time": "2026-01-28T16:11:50.083880300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Optimized Scaling Analysis - Final Version\n",
    "===========================================\n",
    "This script implements the improvements suggested in result.txt:\n",
    "1. Aggressive winsorization (1%, 99%) for problematic features\n",
    "2. Converting cumulative features (PVT_cumsum, MOBV) to percentage change\n",
    "3. Box-Cox transformation for highly skewed features\n",
    "4. Final clipping at [-4, 4] standard deviations\n",
    "5. Comparison: Before vs After only\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import boxcox\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE DEFINITIONS\n",
    "# ============================================================\n",
    "\n",
    "feature_columns = [\n",
    "    # Price Features (2)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (6)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (2)\n",
    "    'PVT_cumsum',\n",
    "    'MOBV',\n",
    "\n",
    "    # Directional Movement (1)\n",
    "    'MTM',\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',\n",
    "    'VHF',\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',\n",
    "]\n",
    "\n",
    "# Feature categorization\n",
    "no_need_scaling = [\n",
    "    'is_up_day',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'price_position_20',\n",
    "]\n",
    "\n",
    "robust_scaling_features = [\n",
    "    'distance_from_high',\n",
    "    'downside_deviation_10',\n",
    "    'high_low_ratio',\n",
    "    'low_to_close_ratio',\n",
    "    'max_drawdown_20',\n",
    "    'parkinson_volatility',\n",
    "    'recent_high_20',\n",
    "    'volatility_20',\n",
    "    'VHF',\n",
    "    'MOBV',\n",
    "    'PVT_cumsum'\n",
    "]\n",
    "\n",
    "zscore_features = [\n",
    "    'ADTM',\n",
    "    'daily_return',\n",
    "    'MA_60_slope',\n",
    "    'MTM',\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'PSY',\n",
    "    'RSI_14',\n",
    "]\n",
    "\n",
    "standard_scaler_features = [\n",
    "    'K'\n",
    "]\n",
    "\n",
    "# Problematic features identified in result.txt\n",
    "problematic_features = {\n",
    "    'volatility_20': (1, 99),\n",
    "    'high_low_ratio': (1, 99),\n",
    "    'parkinson_volatility': (1, 99),\n",
    "    'price_to_MA5': (0.5, 99),\n",
    "    'price_to_MA20': (0.5, 99),\n",
    "    'price_to_MA60': (0.5, 99),\n",
    "}\n",
    "\n",
    "# Highly skewed features for Box-Cox\n",
    "positive_skewed_features = [\n",
    "    'volatility_20',\n",
    "    'high_low_ratio',\n",
    "    'parkinson_volatility'\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def analyze_distribution(df, feature, prefix=\"\"):\n",
    "    \"\"\"Comprehensive distribution analysis\"\"\"\n",
    "    data = df[feature].dropna()\n",
    "\n",
    "    stats_dict = {\n",
    "        'count': len(data),\n",
    "        'mean': data.mean(),\n",
    "        'std': data.std(),\n",
    "        'min': data.min(),\n",
    "        'q1': data.quantile(0.01),\n",
    "        'q5': data.quantile(0.05),\n",
    "        'q25': data.quantile(0.25),\n",
    "        'median': data.median(),\n",
    "        'q75': data.quantile(0.75),\n",
    "        'q95': data.quantile(0.95),\n",
    "        'q99': data.quantile(0.99),\n",
    "        'max': data.max(),\n",
    "        'skewness': stats.skew(data),\n",
    "        'kurtosis': stats.kurtosis(data),\n",
    "        'n_outliers_3std': ((data < (data.mean() - 3*data.std())) |\n",
    "                            (data > (data.mean() + 3*data.std()))).sum(),\n",
    "        'outlier_pct': ((data < (data.mean() - 3*data.std())) |\n",
    "                        (data > (data.mean() + 3*data.std()))).sum() / len(data) * 100\n",
    "    }\n",
    "\n",
    "    return pd.Series(stats_dict)\n",
    "\n",
    "\n",
    "def apply_aggressive_winsorization(df, feature_percentiles):\n",
    "    \"\"\"\n",
    "    Apply aggressive winsorization based on result.txt recommendations\n",
    "    Different percentiles for different features\n",
    "    \"\"\"\n",
    "    winsor_params = {}\n",
    "\n",
    "    print(\"\\n[STEP 1] Applying Aggressive Winsorization...\")\n",
    "    for feature, (lower_pct, upper_pct) in tqdm(feature_percentiles.items(), desc=\"Winsorizing\"):\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        lower = df[feature].quantile(lower_pct / 100)\n",
    "        upper = df[feature].quantile(upper_pct / 100)\n",
    "\n",
    "        before_outliers = ((df[feature] < lower) | (df[feature] > upper)).sum()\n",
    "        df[feature] = df[feature].clip(lower, upper)\n",
    "\n",
    "        winsor_params[feature] = {\n",
    "            'lower_pct': lower_pct,\n",
    "            'upper_pct': upper_pct,\n",
    "            'lower': lower,\n",
    "            'upper': upper,\n",
    "            'clipped_values': before_outliers\n",
    "        }\n",
    "\n",
    "    return winsor_params\n",
    "\n",
    "\n",
    "def convert_cumulative_to_pct_change(df, ticker_col='ticker'):\n",
    "    \"\"\"\n",
    "    Convert cumulative features (PVT_cumsum, MOBV) to percentage change\n",
    "    As recommended in result.txt\n",
    "    \"\"\"\n",
    "    print(\"\\n[STEP 2] Converting Cumulative Features to Percentage Change...\")\n",
    "\n",
    "    cumulative_features = ['PVT_cumsum', 'MOBV']\n",
    "    conversion_info = {}\n",
    "\n",
    "    for feature in tqdm(cumulative_features, desc=\"Converting\"):\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Store original stats\n",
    "        orig_mean = df[feature].mean()\n",
    "        orig_std = df[feature].std()\n",
    "\n",
    "        # Calculate percentage change per ticker\n",
    "        pct_change = df.groupby(ticker_col)[feature].pct_change()\n",
    "\n",
    "        # Fill first NaN with 0\n",
    "        pct_change = pct_change.fillna(0)\n",
    "\n",
    "        # Replace inf with 0\n",
    "        pct_change = pct_change.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "        # Clip extreme percentage changes (as suggested: -0.5 to 0.5)\n",
    "        pct_change = pct_change.clip(-0.5, 0.5)\n",
    "\n",
    "        # Store new feature name\n",
    "        new_feature_name = f'{feature}_pct_change'\n",
    "        df[new_feature_name] = pct_change\n",
    "\n",
    "        # Drop original cumulative feature\n",
    "        df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "        conversion_info[feature] = {\n",
    "            'new_name': new_feature_name,\n",
    "            'original_mean': orig_mean,\n",
    "            'original_std': orig_std,\n",
    "            'new_mean': pct_change.mean(),\n",
    "            'new_std': pct_change.std()\n",
    "        }\n",
    "\n",
    "        print(f\"  ‚úì {feature} ‚Üí {new_feature_name}\")\n",
    "        print(f\"    Original: mean={orig_mean:.2f}, std={orig_std:.2f}\")\n",
    "        print(f\"    New:      mean={pct_change.mean():.4f}, std={pct_change.std():.4f}\")\n",
    "\n",
    "    # Update feature lists\n",
    "    for i, feat in enumerate(feature_columns):\n",
    "        if feat == 'PVT_cumsum':\n",
    "            feature_columns[i] = 'PVT_cumsum_pct_change'\n",
    "        elif feat == 'MOBV':\n",
    "            feature_columns[i] = 'MOBV_pct_change'\n",
    "\n",
    "    for i, feat in enumerate(robust_scaling_features):\n",
    "        if feat == 'PVT_cumsum':\n",
    "            robust_scaling_features[i] = 'PVT_cumsum_pct_change'\n",
    "        elif feat == 'MOBV':\n",
    "            robust_scaling_features[i] = 'MOBV_pct_change'\n",
    "\n",
    "    return conversion_info\n",
    "\n",
    "\n",
    "def apply_boxcox_transform(df, features):\n",
    "    \"\"\"\n",
    "    Apply Box-Cox transformation for highly skewed positive features\n",
    "    As recommended in result.txt\n",
    "    \"\"\"\n",
    "    print(\"\\n[STEP 3] Applying Box-Cox Transformation...\")\n",
    "\n",
    "    boxcox_params = {}\n",
    "\n",
    "    for feature in tqdm(features, desc=\"Box-Cox Transform\"):\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Store original skewness\n",
    "            orig_skew = stats.skew(df[feature].dropna())\n",
    "\n",
    "            # Ensure all values are positive\n",
    "            data = df[feature].values\n",
    "            data_shifted = data - data.min() + 1  # Shift to make all positive\n",
    "\n",
    "            # Apply Box-Cox\n",
    "            transformed, lambda_param = boxcox(data_shifted)\n",
    "\n",
    "            # Replace in dataframe\n",
    "            df[feature] = transformed\n",
    "\n",
    "            # Calculate new skewness\n",
    "            new_skew = stats.skew(df[feature].dropna())\n",
    "\n",
    "            boxcox_params[feature] = {\n",
    "                'lambda': lambda_param,\n",
    "                'shift': data.min() - 1,\n",
    "                'original_skew': orig_skew,\n",
    "                'new_skew': new_skew\n",
    "            }\n",
    "\n",
    "            print(f\"  ‚úì {feature}: skew {orig_skew:.2f} ‚Üí {new_skew:.2f} (Œª={lambda_param:.3f})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó {feature}: Failed - {str(e)}\")\n",
    "            boxcox_params[feature] = {'error': str(e)}\n",
    "\n",
    "    return boxcox_params\n",
    "\n",
    "\n",
    "def apply_scaling(df, robust_feats, zscore_feats, standard_feats):\n",
    "    \"\"\"Apply appropriate scaling to features\"\"\"\n",
    "    print(\"\\n[STEP 4] Applying Scaling...\")\n",
    "\n",
    "    # RobustScaler\n",
    "    if robust_feats:\n",
    "        robust_scaler = RobustScaler()\n",
    "        valid_robust = [f for f in robust_feats if f in df.columns]\n",
    "        if valid_robust:\n",
    "            print(f\"  ‚Üí RobustScaler: {len(valid_robust)} features\")\n",
    "            df[valid_robust] = robust_scaler.fit_transform(df[valid_robust])\n",
    "\n",
    "    # Z-Score (StandardScaler)\n",
    "    if zscore_feats:\n",
    "        z_scaler = StandardScaler()\n",
    "        valid_zscore = [f for f in zscore_feats if f in df.columns]\n",
    "        if valid_zscore:\n",
    "            print(f\"  ‚Üí StandardScaler (Z-Score): {len(valid_zscore)} features\")\n",
    "            df[valid_zscore] = z_scaler.fit_transform(df[valid_zscore])\n",
    "\n",
    "    # StandardScaler\n",
    "    if standard_feats:\n",
    "        std_scaler = StandardScaler()\n",
    "        valid_std = [f for f in standard_feats if f in df.columns]\n",
    "        if valid_std:\n",
    "            print(f\"  ‚Üí StandardScaler: {len(valid_std)} features\")\n",
    "            df[valid_std] = std_scaler.fit_transform(df[valid_std])\n",
    "\n",
    "\n",
    "def final_clipping(df, features, threshold=20):\n",
    "    \"\"\"\n",
    "    Final clipping at ¬±4 standard deviations\n",
    "    As recommended in result.txt\n",
    "    \"\"\"\n",
    "    print(f\"\\n[STEP 5] Final Clipping at ¬±{threshold} std...\")\n",
    "\n",
    "    clipping_stats = {}\n",
    "\n",
    "    for feature in tqdm(features, desc=\"Clipping\"):\n",
    "        if feature not in df.columns or feature in no_need_scaling:\n",
    "            continue\n",
    "\n",
    "        before_count = len(df)\n",
    "        clipped_count = ((df[feature] < -threshold) | (df[feature] > threshold)).sum()\n",
    "\n",
    "        df[feature] = df[feature].clip(-threshold, threshold)\n",
    "\n",
    "        clipping_stats[feature] = {\n",
    "            'clipped_values': clipped_count,\n",
    "            'clipped_pct': (clipped_count / before_count) * 100\n",
    "        }\n",
    "\n",
    "    total_clipped = sum(s['clipped_values'] for s in clipping_stats.values())\n",
    "    print(f\"  ‚úì Total values clipped: {total_clipped:,}\")\n",
    "\n",
    "    return clipping_stats\n",
    "\n",
    "\n",
    "def calculate_quality_score(df_after, features_to_check):\n",
    "    \"\"\"Calculate normalization quality score\"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    for feat in features_to_check:\n",
    "        if feat not in df_after.index:\n",
    "            continue\n",
    "\n",
    "        mean = df_after.loc[feat, 'mean']\n",
    "        std = df_after.loc[feat, 'std']\n",
    "        skew = df_after.loc[feat, 'skewness']\n",
    "\n",
    "        # Score components (0-1 scale)\n",
    "        mean_score = max(0, 1 - abs(mean) * 2)  # Penalty if |mean| > 0.5\n",
    "        std_score = max(0, 1 - abs(std - 1.0) * 2)  # Penalty if std far from 1\n",
    "        skew_score = max(0, 1 - abs(skew) / 10)  # Penalty for high skewness\n",
    "\n",
    "        total_score = (mean_score + std_score + skew_score) / 3\n",
    "\n",
    "        scores[feat] = {\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'skewness': skew,\n",
    "            'mean_score': mean_score,\n",
    "            'std_score': std_score,\n",
    "            'skew_score': skew_score,\n",
    "            'total_score': total_score\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(scores).T\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"üìä OPTIMIZED SCALING ANALYSIS - FINAL VERSION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplementing improvements from result.txt:\")\n",
    "print(\"  1. Aggressive winsorization (1%, 99%) for problematic features\")\n",
    "print(\"  2. Convert PVT_cumsum & MOBV to percentage change\")\n",
    "print(\"  3. Box-Cox transform for highly skewed features\")\n",
    "print(\"  4. Final clipping at ¬±4 std\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# 1. LOAD DATA AND INITIAL ANALYSIS\n",
    "# ============================================================\n",
    "print(\"\\nüìÇ Loading data...\")\n",
    "\n",
    "# Load your data here\n",
    "# Uncomment and modify as needed:\n",
    "# df_features = pd.read_csv('data/processed/data.csv')\n",
    "\n",
    "# For demonstration, assuming df_features is already loaded\n",
    "if 'df_features' not in globals():\n",
    "    print(\"‚ö†Ô∏è  Please load df_features first!\")\n",
    "    print(\"   Example: df_features = pd.read_csv('your_data.csv')\")\n",
    "    exit()\n",
    "\n",
    "print(f\"‚úì Data loaded: {df_features.shape[0]:,} rows, {df_features.shape[1]} columns\")\n",
    "\n",
    "# Analyze original distribution\n",
    "print(\"\\n[INITIAL ANALYSIS] Analyzing original distribution...\")\n",
    "before_stats = {}\n",
    "for feature in tqdm(feature_columns, desc=\"Analyzing features\"):\n",
    "    if feature in df_features.columns:\n",
    "        before_stats[feature] = analyze_distribution(df_features, feature)\n",
    "\n",
    "df_before = pd.DataFrame(before_stats).T\n",
    "df_before['scaling_method'] = df_before.index.map(\n",
    "    lambda x: 'none' if x in no_need_scaling\n",
    "    else 'robust' if x in robust_scaling_features\n",
    "    else 'zscore' if x in zscore_features\n",
    "    else 'standard'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç OUTLIER ANALYSIS (BEFORE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüî• Features with HIGH OUTLIERS (>2% outliers):\")\n",
    "high_outliers = df_before[df_before['outlier_pct'] > 2.0].sort_values('outlier_pct', ascending=False)\n",
    "if len(high_outliers) > 0:\n",
    "    print(high_outliers[['outlier_pct', 'skewness', 'min', 'max', 'scaling_method']].head(10))\n",
    "else:\n",
    "    print(\"None found!\")\n",
    "\n",
    "print(\"\\nüìà Features with EXTREME SKEWNESS (|skew| > 5):\")\n",
    "extreme_skew = df_before[abs(df_before['skewness']) > 5].sort_values('skewness', key=abs, ascending=False)\n",
    "if len(extreme_skew) > 0:\n",
    "    print(extreme_skew[['skewness', 'kurtosis', 'outlier_pct', 'scaling_method']].head(10))\n",
    "else:\n",
    "    print(\"None found!\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. APPLY ALL TRANSFORMATIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß APPLYING TRANSFORMATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a copy for processing\n",
    "df_final = df_features.copy()\n",
    "\n",
    "# Step 1: Aggressive winsorization\n",
    "winsor_params = apply_aggressive_winsorization(df_final, problematic_features)\n",
    "\n",
    "# Step 2: Convert cumulative features to percentage change\n",
    "if 'ticker' in df_final.columns:\n",
    "    conversion_info = convert_cumulative_to_pct_change(df_final, 'ticker')\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  'ticker' column not found. Skipping cumulative conversion.\")\n",
    "    conversion_info = {}\n",
    "\n",
    "# Step 3: Box-Cox transformation for highly skewed features\n",
    "# Only apply to features that haven't been converted\n",
    "features_for_boxcox = [f for f in positive_skewed_features\n",
    "                       if f in df_final.columns and f not in ['PVT_cumsum', 'MOBV']]\n",
    "if features_for_boxcox:\n",
    "    boxcox_params = apply_boxcox_transform(df_final, features_for_boxcox)\n",
    "else:\n",
    "    boxcox_params = {}\n",
    "\n",
    "# Step 4: Apply scaling\n",
    "apply_scaling(df_final, robust_scaling_features, zscore_features, standard_scaler_features)\n",
    "\n",
    "# Step 5: Final clipping\n",
    "clipping_stats = final_clipping(df_final, feature_columns, threshold=4)\n",
    "\n",
    "print(\"\\n‚úÖ All transformations applied!\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. ANALYZE FINAL RESULTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ANALYZING FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "after_stats = {}\n",
    "for feature in tqdm(feature_columns, desc=\"Analyzing scaled features\"):\n",
    "    if feature in df_final.columns:\n",
    "        after_stats[feature] = analyze_distribution(df_final, feature)\n",
    "\n",
    "df_after = pd.DataFrame(after_stats).T\n",
    "\n",
    "# ============================================================\n",
    "# 4. QUALITY METRICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ QUALITY METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "features_to_evaluate = [f for f in feature_columns if f not in no_need_scaling and f in df_after.index]\n",
    "quality_scores = calculate_quality_score(df_after, features_to_evaluate)\n",
    "\n",
    "print(f\"\\nüìä FINAL QUALITY SCORE: {quality_scores['total_score'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Top 10 Best Performers:\")\n",
    "print(quality_scores.nlargest(10, 'total_score')[['mean', 'std', 'skewness', 'total_score']])\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Bottom 10 - Need More Work:\")\n",
    "print(quality_scores.nsmallest(10, 'total_score')[['mean', 'std', 'skewness', 'total_score']])\n",
    "\n",
    "# ============================================================\n",
    "# 5. COMPARISON\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã BEFORE vs AFTER COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'before_mean': df_before['mean'],\n",
    "    'before_std': df_before['std'],\n",
    "    'before_skew': df_before['skewness'],\n",
    "    'before_max': df_before['max'],\n",
    "    'before_outliers': df_before['outlier_pct'],\n",
    "\n",
    "    'after_mean': df_after['mean'],\n",
    "    'after_std': df_after['std'],\n",
    "    'after_skew': df_after['skewness'],\n",
    "    'after_max': df_after['max'],\n",
    "\n",
    "    'method': df_before['scaling_method']\n",
    "})\n",
    "\n",
    "# Calculate improvements\n",
    "comparison['mean_improvement'] = abs(comparison['before_mean']) - abs(comparison['after_mean'])\n",
    "comparison['std_improvement'] = abs(comparison['before_std'] - 1.0) - abs(comparison['after_std'] - 1.0)\n",
    "comparison['skew_improvement'] = abs(comparison['before_skew']) - abs(comparison['after_skew'])\n",
    "comparison['max_improvement'] = abs(comparison['before_max']) - abs(comparison['after_max'])\n",
    "\n",
    "print(\"\\n‚úÖ Features with BEST STD Improvement:\")\n",
    "improved = comparison.sort_values('std_improvement', ascending=False).head(10)\n",
    "print(improved[['before_std', 'after_std', 'std_improvement', 'method']])\n",
    "\n",
    "print(\"\\n‚úÖ Features with BEST SKEW Improvement:\")\n",
    "skew_improved = comparison.sort_values('skew_improvement', ascending=False).head(10)\n",
    "print(skew_improved[['before_skew', 'after_skew', 'skew_improvement', 'method']])\n",
    "\n",
    "print(\"\\n‚úÖ Features with BEST MAX Value Reduction:\")\n",
    "max_improved = comparison.sort_values('max_improvement', ascending=False).head(10)\n",
    "print(max_improved[['before_max', 'after_max', 'max_improvement', 'method']])\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Features Still Above Threshold (max > 50 after scaling):\")\n",
    "still_high = comparison[comparison['after_max'] > 50].sort_values('after_max', ascending=False)\n",
    "if len(still_high) > 0:\n",
    "    print(still_high[['after_mean', 'after_std', 'after_skew', 'after_max', 'method']])\n",
    "else:\n",
    "    print(\"‚úì None! All features are within acceptable range.\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. SAVE RESULTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_file = \"scaling_analysis_final.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    df_before.to_excel(writer, sheet_name='01_Before_Scaling')\n",
    "    df_after.to_excel(writer, sheet_name='02_After_Scaling')\n",
    "    comparison.to_excel(writer, sheet_name='03_Comparison')\n",
    "    quality_scores.to_excel(writer, sheet_name='04_Quality_Score')\n",
    "\n",
    "    # Transformation details\n",
    "    pd.DataFrame(winsor_params).T.to_excel(writer, sheet_name='05_Winsorization')\n",
    "    if conversion_info:\n",
    "        pd.DataFrame(conversion_info).T.to_excel(writer, sheet_name='06_Cumulative_Conversion')\n",
    "    if boxcox_params:\n",
    "        pd.DataFrame(boxcox_params).T.to_excel(writer, sheet_name='07_BoxCox_Transform')\n",
    "    pd.DataFrame(clipping_stats).T.to_excel(writer, sheet_name='08_Final_Clipping')\n",
    "\n",
    "print(f\"‚úì Excel report saved: {output_file}\")\n",
    "\n",
    "# # Save processed data\n",
    "# output_csv = \"data_scaled_final.csv\"\n",
    "# df_final.to_csv(output_csv, index=False)\n",
    "# print(f\"‚úì Scaled data saved: {output_csv}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. VISUALIZATIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì∏ GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "IMAGE_DIR = \"Images_scaling_final\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "for feature in tqdm(feature_columns, desc=\"Creating plots\"):\n",
    "    # Handle renamed features\n",
    "    original_feature = feature\n",
    "    if feature == 'PVT_cumsum_pct_change':\n",
    "        original_feature = 'PVT_cumsum'\n",
    "    elif feature == 'MOBV_pct_change':\n",
    "        original_feature = 'MOBV'\n",
    "\n",
    "    if original_feature not in df_features.columns or feature not in df_final.columns:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Determine scaling method\n",
    "    if feature in robust_scaling_features:\n",
    "        method = 'RobustScaler'\n",
    "    elif feature in zscore_features:\n",
    "        method = 'StandardScaler (Z-Score)'\n",
    "    elif feature in standard_scaler_features:\n",
    "        method = 'StandardScaler'\n",
    "    else:\n",
    "        method = 'No Scaling'\n",
    "\n",
    "    # Add transformation info\n",
    "    transform_info = []\n",
    "    if original_feature in problematic_features:\n",
    "        transform_info.append(f\"Winsorized ({problematic_features[original_feature][0]}%, {problematic_features[original_feature][1]}%)\")\n",
    "    if original_feature in positive_skewed_features and original_feature in boxcox_params:\n",
    "        transform_info.append(\"Box-Cox\")\n",
    "    if original_feature in ['PVT_cumsum', 'MOBV']:\n",
    "        transform_info.append(\"% Change\")\n",
    "    transform_info.append(\"Clipped ¬±4œÉ\")\n",
    "\n",
    "    title = f'{feature} - {method}'\n",
    "    if transform_info:\n",
    "        title += f'\\nTransforms: {\", \".join(transform_info)}'\n",
    "\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 1. Before (Original)\n",
    "    axes[0].hist(df_features[original_feature].dropna(), bins=100, alpha=0.7,\n",
    "                 color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title('BEFORE (Original)', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel('Value')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    mean_orig = df_features[original_feature].mean()\n",
    "    std_orig = df_features[original_feature].std()\n",
    "    skew_orig = stats.skew(df_features[original_feature].dropna())\n",
    "    max_orig = df_features[original_feature].max()\n",
    "\n",
    "    axes[0].text(0.02, 0.98,\n",
    "                 f'Mean: {mean_orig:.3f}\\nStd: {std_orig:.3f}\\nSkew: {skew_orig:.2f}\\nMax: {max_orig:.2f}',\n",
    "                 transform=axes[0].transAxes, verticalalignment='top',\n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7),\n",
    "                 fontsize=10)\n",
    "\n",
    "    # 2. After (Final)\n",
    "    axes[1].hist(df_final[feature].dropna(), bins=100, alpha=0.7,\n",
    "                 color='green', edgecolor='black')\n",
    "    axes[1].set_title('AFTER (Final Optimized)', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    mean_final = df_final[feature].mean()\n",
    "    std_final = df_final[feature].std()\n",
    "    skew_final = stats.skew(df_final[feature].dropna())\n",
    "    max_final = df_final[feature].max()\n",
    "\n",
    "    # Calculate quality score for this feature\n",
    "    if feature in quality_scores.index:\n",
    "        quality = quality_scores.loc[feature, 'total_score']\n",
    "        quality_text = f'\\n\\nQuality: {quality:.3f}'\n",
    "    else:\n",
    "        quality_text = ''\n",
    "\n",
    "    axes[1].text(0.02, 0.98,\n",
    "                 f'Mean: {mean_final:.3f}\\nStd: {std_final:.3f}\\nSkew: {skew_final:.2f}\\nMax: {max_final:.2f}{quality_text}',\n",
    "                 transform=axes[1].transAxes, verticalalignment='top',\n",
    "                 bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
    "                 fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    safe_filename = feature.replace('/', '_')\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'{safe_filename}_comparison.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"‚úì Saved {len([f for f in feature_columns if f in df_final.columns])} visualizations to: {IMAGE_DIR}/\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count improvements\n",
    "improved_features = (comparison['std_improvement'] > 0).sum()\n",
    "total_features = len(comparison)\n",
    "avg_quality = quality_scores['total_score'].mean()\n",
    "\n",
    "# Problematic features still remaining\n",
    "still_problematic = comparison[\n",
    "    (comparison['after_max'] > 50) |\n",
    "    (abs(comparison['after_skew']) > 5) |\n",
    "    (comparison['after_std'] > 2)\n",
    "]\n",
    "\n",
    "print(f\"\"\"\n",
    "TRANSFORMATION SUMMARY:\n",
    "=======================\n",
    "‚úì Aggressive Winsorization: {len(winsor_params)} features\n",
    "‚úì Cumulative ‚Üí % Change: {len(conversion_info)} features\n",
    "‚úì Box-Cox Transform: {len(boxcox_params)} features\n",
    "‚úì Final Clipping: {sum(s['clipped_values'] for s in clipping_stats.values()):,} values\n",
    "\n",
    "QUALITY METRICS:\n",
    "================\n",
    "‚úì Average Quality Score: {avg_quality:.3f}\n",
    "‚úì Features Improved: {improved_features} / {total_features} ({improved_features/total_features*100:.1f}%)\n",
    "‚úì Average STD Improvement: {comparison['std_improvement'].mean():.3f}\n",
    "‚úì Average SKEW Improvement: {comparison['skew_improvement'].mean():.3f}\n",
    "‚úì Average MAX Reduction: {comparison['max_improvement'].mean():.2f}\n",
    "\n",
    "REMAINING ISSUES:\n",
    "=================\n",
    "\"\"\")\n",
    "\n",
    "if len(still_problematic) > 0:\n",
    "    print(f\"‚ö†Ô∏è  {len(still_problematic)} features still need attention:\")\n",
    "    print(still_problematic[['after_std', 'after_skew', 'after_max', 'method']].to_string())\n",
    "else:\n",
    "    print(\"‚úÖ All features are within acceptable ranges!\")\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n"
   ],
   "id": "f833ad515343a7cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä OPTIMIZED SCALING ANALYSIS - FINAL VERSION\n",
      "================================================================================\n",
      "\n",
      "Implementing improvements from result.txt:\n",
      "  1. Aggressive winsorization (1%, 99%) for problematic features\n",
      "  2. Convert PVT_cumsum & MOBV to percentage change\n",
      "  3. Box-Cox transform for highly skewed features\n",
      "  4. Final clipping at ¬±4 std\n",
      "================================================================================\n",
      "\n",
      "üìÇ Loading data...\n",
      "‚úì Data loaded: 12,269,060 rows, 38 columns\n",
      "\n",
      "[INITIAL ANALYSIS] Analyzing original distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:44<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç OUTLIER ANALYSIS (BEFORE)\n",
      "================================================================================\n",
      "\n",
      "üî• Features with HIGH OUTLIERS (>2% outliers):\n",
      "                    outlier_pct  skewness       min          max  \\\n",
      "recent_high_20         2.429314  3.209126  0.106040  1010.080017   \n",
      "distance_from_high     2.048682 -2.500268 -0.992353     0.022222   \n",
      "\n",
      "                   scaling_method  \n",
      "recent_high_20             robust  \n",
      "distance_from_high         robust  \n",
      "\n",
      "üìà Features with EXTREME SKEWNESS (|skew| > 5):\n",
      "                         skewness      kurtosis  outlier_pct scaling_method\n",
      "daily_return          2131.687999  6.183759e+06     0.146941         zscore\n",
      "volatility_20          508.418004  3.244742e+05     0.103881         robust\n",
      "MA_60_slope             78.938633  3.239143e+04     1.049713         zscore\n",
      "PVT_cumsum              29.463500  1.268447e+03     0.633406         robust\n",
      "MOBV                    24.705407  1.218629e+03     0.899498         robust\n",
      "high_low_ratio          21.816197  2.971159e+03     1.365516         robust\n",
      "parkinson_volatility    14.469786  1.920736e+03     1.483447         robust\n",
      "price_to_MA60            6.647672  7.637033e+02     1.555017         zscore\n",
      "price_to_MA20            5.875331  4.820860e+02     1.586739         zscore\n",
      "\n",
      "================================================================================\n",
      "üîß APPLYING TRANSFORMATIONS\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Applying Aggressive Winsorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Winsorizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Converting Cumulative Features to Percentage Change...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì PVT_cumsum ‚Üí PVT_cumsum_pct_change\n",
      "    Original: mean=7364432.44, std=75800166.78\n",
      "    New:      mean=0.0004, std=0.0901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì MOBV ‚Üí MOBV_pct_change\n",
      "    Original: mean=51568530.72, std=290317988.96\n",
      "    New:      mean=0.0005, std=0.1217\n",
      "\n",
      "[STEP 3] Applying Box-Cox Transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Box-Cox Transform:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úó volatility_20: Failed - The `x` argument of `boxcox_normmax` must contain only positive, finite, real numbers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Box-Cox Transform:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:29<00:17, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì high_low_ratio: skew 2.14 ‚Üí 0.25 (Œª=-24.645)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Box-Cox Transform: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:09<00:00, 23.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì parkinson_volatility: skew 1.90 ‚Üí 0.23 (Œª=-38.613)\n",
      "\n",
      "[STEP 4] Applying Scaling...\n",
      "  ‚Üí RobustScaler: 11 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí StandardScaler (Z-Score): 9 features\n",
      "  ‚Üí StandardScaler: 1 features\n",
      "\n",
      "[STEP 5] Final Clipping at ¬±4 std...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:02<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Total values clipped: 4,750,854\n",
      "\n",
      "‚úÖ All transformations applied!\n",
      "\n",
      "================================================================================\n",
      "üìä ANALYZING FINAL RESULTS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing scaled features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:40<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ QUALITY METRICS\n",
      "================================================================================\n",
      "\n",
      "üìä FINAL QUALITY SCORE: 0.809\n",
      "\n",
      "‚úÖ Top 10 Best Performers:\n",
      "                       mean       std  skewness  total_score\n",
      "PSY           -4.874731e-16  1.000000 -0.014900     0.999503\n",
      "RSI_14         4.554494e-16  1.000000 -0.022583     0.999247\n",
      "K             -1.613050e-16  1.000000 -0.054845     0.998172\n",
      "price_to_MA60  0.000000e+00  1.000000 -0.137004     0.995433\n",
      "ADTM           3.373887e-07  0.999999 -0.235442     0.992151\n",
      "price_to_MA20  5.460569e-04  0.997785 -0.214904     0.990996\n",
      "price_to_MA5   1.497321e-03  0.993792 -0.186682     0.988640\n",
      "MA_60_slope   -8.859845e-03  0.805158  0.293164     0.854427\n",
      "MTM            3.960482e-03  0.758006 -0.094967     0.832865\n",
      "VHF            1.289829e-01  0.776772  0.975523     0.732675\n",
      "\n",
      "‚ö†Ô∏è  Bottom 10 - Need More Work:\n",
      "                           mean       std  skewness  total_score\n",
      "recent_high_20         0.395488  1.100372  1.841005     0.608060\n",
      "daily_return          -0.002208  0.371005  0.788241     0.638920\n",
      "MOBV_pct_change        0.010029  1.669144 -0.006960     0.659749\n",
      "PVT_cumsum_pct_change  0.007582  1.908907  0.005575     0.661426\n",
      "distance_from_high    -0.320351  0.951908 -1.805525     0.694187\n",
      "max_drawdown_20       -0.261183  0.908137 -1.665628     0.709115\n",
      "low_to_close_ratio    -0.274116  0.928764 -1.747960     0.711500\n",
      "high_low_ratio         0.065815  0.669623  0.245994     0.727672\n",
      "downside_deviation_10  0.280157  0.961025  1.775871     0.728049\n",
      "parkinson_volatility   0.059077  0.666746  0.225535     0.730928\n",
      "\n",
      "================================================================================\n",
      "üìã BEFORE vs AFTER COMPARISON\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Features with BEST STD Improvement:\n",
      "                       before_std  after_std  std_improvement    method\n",
      "recent_high_20          41.810020   1.100372        40.709648    robust\n",
      "K                       30.149267   1.000000        29.149267  standard\n",
      "RSI_14                  17.078535   1.000000        16.078535    zscore\n",
      "PSY                     14.568258   1.000000        13.568258    zscore\n",
      "MTM                      4.304235   0.758006         3.062241    zscore\n",
      "price_to_MA5             0.036713   0.993792         0.957079    zscore\n",
      "downside_deviation_10    0.013434   0.961025         0.947591    robust\n",
      "price_to_MA20            0.081625   0.997785         0.916160    zscore\n",
      "volatility_20            0.086670   0.963605         0.876935    robust\n",
      "distance_from_high       0.089791   0.951908         0.862117    robust\n",
      "\n",
      "‚úÖ Features with BEST SKEW Improvement:\n",
      "                       before_skew  after_skew  skew_improvement  method\n",
      "daily_return           2131.687999    0.788241       2130.899758  zscore\n",
      "volatility_20           508.418004    1.752982        506.665022  robust\n",
      "MA_60_slope              78.938633    0.293164         78.645469  zscore\n",
      "high_low_ratio           21.816197    0.245994         21.570202  robust\n",
      "parkinson_volatility     14.469786    0.225535         14.244251  robust\n",
      "price_to_MA60             6.647672   -0.137004          6.510668  zscore\n",
      "price_to_MA20             5.875331   -0.214904          5.660427  zscore\n",
      "price_to_MA5              3.227106   -0.186682          3.040425  zscore\n",
      "downside_deviation_10     4.493302    1.775871          2.717431  robust\n",
      "MTM                      -1.686393   -0.094967          1.591425  zscore\n",
      "\n",
      "‚úÖ Features with BEST MAX Value Reduction:\n",
      "                 before_max  after_max  max_improvement    method\n",
      "recent_high_20  1010.080017   4.000000      1006.080017    robust\n",
      "MTM              385.799999   4.000000       381.799999    zscore\n",
      "daily_return     257.333339   4.000000       253.333339    zscore\n",
      "K                113.888872   2.072968       111.815903  standard\n",
      "RSI_14           100.000000   2.856150        97.143850    zscore\n",
      "PSY              100.000000   3.493141        96.506859    zscore\n",
      "MA_60_slope       68.811440   4.000000        64.811440    zscore\n",
      "volatility_20     57.542185   4.000000        53.542185    robust\n",
      "price_to_MA60     39.164111   3.361687        35.802424    zscore\n",
      "price_to_MA20     17.663456   3.354998        14.308457    zscore\n",
      "\n",
      "‚ö†Ô∏è  Features Still Above Threshold (max > 50 after scaling):\n",
      "‚úì None! All features are within acceptable range.\n",
      "\n",
      "================================================================================\n",
      "üíæ SAVING RESULTS\n",
      "================================================================================\n",
      "‚úì Excel report saved: scaling_analysis_final.xlsx\n",
      "‚úì Scaled data saved: data_scaled_final.csv\n",
      "\n",
      "================================================================================\n",
      "üì∏ GENERATING VISUALIZATIONS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating plots: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:55<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved 25 visualizations to: Images_scaling_final/\n",
      "\n",
      "================================================================================\n",
      "üéØ FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "TRANSFORMATION SUMMARY:\n",
      "=======================\n",
      "‚úì Aggressive Winsorization: 6 features\n",
      "‚úì Cumulative ‚Üí % Change: 2 features\n",
      "‚úì Box-Cox Transform: 3 features\n",
      "‚úì Final Clipping: 4,750,854 values\n",
      "\n",
      "QUALITY METRICS:\n",
      "================\n",
      "‚úì Average Quality Score: 0.809\n",
      "‚úì Features Improved: 19 / 27 (70.4%)\n",
      "‚úì Average STD Improvement: 4.923\n",
      "‚úì Average SKEW Improvement: 120.641\n",
      "‚úì Average MAX Reduction: 92.11\n",
      "\n",
      "REMAINING ISSUES:\n",
      "=================\n",
      "\n",
      "‚úÖ All features are within acceptable ranges!\n",
      "\n",
      "\n",
      "FILES GENERATED:\n",
      "================\n",
      "1. Excel Report: scaling_analysis_final.xlsx\n",
      "2. Scaled Data: data_scaled_final.csv\n",
      "3. Visualizations: Images_scaling_final/ (25 images)\n",
      "\n",
      "NEXT STEPS:\n",
      "===========\n",
      "1. Review Excel report for detailed statistics\n",
      "2. Check visualizations for distribution changes\n",
      "3. Use df_final (or data_scaled_final.csv) for model training\n",
      "4. Monitor features in \"still need work\" list if any remain\n",
      "\n",
      "USAGE:\n",
      "======\n",
      "# To use the scaled data:\n",
      "df_scaled = pd.read_csv('data_scaled_final.csv')\n",
      "\n",
      "# Features are now normalized and ready for neural network training!\n",
      "\n",
      "\n",
      "‚úÖ ANALYSIS COMPLETE!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features_scaled.describe()",
   "id": "ac79b62b439c3d7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "cb8e9af69968a3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n‚úì Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "61c369a7c5beb523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n‚úì Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n‚úì Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\n‚úì Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "cd34fedd16ffb2a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n‚úì Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\n‚úì Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "3db3374b37d5161f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 5000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 ‚≠ê‚≠ê‚≠ê\n",
    "    'MOBV',                 # MI = 0.0209 ‚≠ê‚≠ê\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 ‚≠ê\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ac1312d7bbf04afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "5cacce17fbd4b72f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20','close_30d_future',\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\n‚úì Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "4177fe98c9aaf3a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "ef062214d42f8b5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "e51fd72a1a8f1763"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: DATA QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] DATA QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "# ============================================================\n",
    "feature_columns = [\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 ‚≠ê‚≠ê‚≠ê\n",
    "    'MOBV',                 # MI = 0.0209 ‚≠ê‚≠ê\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 ‚≠ê\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features\n",
    "\n",
    "]\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values in engineered features:\")\n",
    "missing_stats = df_features[feature_columns].isnull().sum()\n",
    "missing_pct = (missing_stats / len(df_features) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_stats,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df_features[feature_columns].describe())\n",
    "\n",
    "IMAGE_DIR = \"Images2\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Visualization: Distribution of features\n",
    "for feature in feature_columns:\n",
    "    data = df_features[feature].dropna()\n",
    "\n",
    "    # Remove extreme outliers for better visualization (keep 1st-99th percentile)\n",
    "    q1, q99 = data.quantile([0.01, 0.99])\n",
    "    data_filtered = data[(data >= q1) & (data <= q99)]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))  # wider figure\n",
    "    plt.hist(data_filtered, bins=100, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'{feature} Distribution\\n(1st-99th percentile)', fontsize=14)\n",
    "    plt.xlabel('Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(IMAGE_DIR, f'feature_{feature}.png'),\n",
    "        dpi=300,\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ],
   "id": "c24ea41df1ba4346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "46b315725a20227c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n‚úì Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "f9e8fcac0ffa929d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n‚úì Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n‚úì Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\n‚úì Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "f8a5745e7bf6e107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n‚úì Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\n‚úì Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "bc2220133a071d7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 1000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 ‚≠ê‚≠ê‚≠ê\n",
    "    'MOBV',                 # MI = 0.0209 ‚≠ê‚≠ê\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 ‚≠ê\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features:\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\",\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "eac3f8e6a29c7983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6686ea2b553342fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
