{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T13:55:03.866377900Z",
     "start_time": "2026-01-28T13:55:03.689132200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stock Price Prediction - Feature Analysis for RNN Sequence Selection\n",
    "# Analyzing features to determine optimal sequence length for LSTM/GRU models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STOCK PRICE PREDICTION - FEATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nObjective: Determine optimal sequence length for RNN input\")\n",
    "print(\"Target: Predict if close price > current price after 30 trading days\")\n",
    "print(\"=\"*80)"
   ],
   "id": "484f1387dbb2e619",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STOCK PRICE PREDICTION - FEATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Objective: Determine optimal sequence length for RNN input\n",
      "Target: Predict if close price > current price after 30 trading days\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T13:55:20.530406100Z",
     "start_time": "2026-01-28T13:55:05.779966900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "14e5e8d25a80716a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] LOADING DATA...\n",
      "Total records: 12,269,060\n",
      "Unique tickers: 4,925\n",
      "date range: 2010-01-04 00:00:00 to 2024-09-23 00:00:00\n",
      "\n",
      "Memory usage: 1645.91 MB\n",
      "\n",
      "================================================================================\n",
      "DATA OVERVIEW\n",
      "================================================================================\n",
      "     ticker       date       open       high        low      close     volume  \\\n",
      "0  ticker_1 2010-01-04  27.875437  28.009543  27.570655  27.662090  2142300.0   \n",
      "1  ticker_1 2010-01-05  27.729151  27.814489  27.131774  27.302454  2856000.0   \n",
      "2  ticker_1 2010-01-06  27.278065  27.729145  27.278065  27.595039  2035400.0   \n",
      "3  ticker_1 2010-01-07  27.637703  27.643798  27.375590  27.497503  1993400.0   \n",
      "4  ticker_1 2010-01-08  27.424356  27.613320  27.253676  27.582842  1306400.0   \n",
      "\n",
      "   dividends  stock_splits  missing_days    return  return_is_outlier  \n",
      "0        0.0           0.0             0       NaN              False  \n",
      "1        0.0           0.0             0 -0.013001              False  \n",
      "2        0.0           0.0             0  0.010716              False  \n",
      "3        0.0           0.0             0 -0.003535              False  \n",
      "4        0.0           0.0             0  0.003104              False  \n",
      "\n",
      "Data types:\n",
      "ticker                       object\n",
      "date                 datetime64[ns]\n",
      "open                        float64\n",
      "high                        float64\n",
      "low                         float64\n",
      "close                       float64\n",
      "volume                      float64\n",
      "dividends                   float64\n",
      "stock_splits                float64\n",
      "missing_days                  int64\n",
      "return                      float64\n",
      "return_is_outlier              bool\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "ticker                  0\n",
      "date                    0\n",
      "open                    0\n",
      "high                    0\n",
      "low                     0\n",
      "close                   0\n",
      "volume                  0\n",
      "dividends               0\n",
      "stock_splits            0\n",
      "missing_days            0\n",
      "return               4925\n",
      "return_is_outlier       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T13:56:02.884134100Z",
     "start_time": "2026-01-28T13:55:26.892087100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20',\n",
    "        'close_30d_future'\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\nâœ“ Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "6ec32e490f9d4f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[2] ENGINEERING FEATURES...\n",
      "================================================================================\n",
      " - Target variable...\n",
      "\n",
      "âœ“ Feature engineering complete!\n",
      "Total features created: 25\n",
      "Rows with complete features: 12,121,310\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "863e4fa68b614557"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "f1d478538e04da36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:01:26.235420300Z",
     "start_time": "2026-01-28T13:56:56.494368700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler as ZScoreScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "feature_columns = [\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',\n",
    "    'MOBV',\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',\n",
    "    'VHF',\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# Feature lists\n",
    "no_need_scaling = [\n",
    "    'is_up_day',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'price_position_20',\n",
    "]\n",
    "\n",
    "robust_scaling_features = [\n",
    "    'distance_from_high',\n",
    "    'downside_deviation_10',\n",
    "    'high_low_ratio',\n",
    "    'low_to_close_ratio',\n",
    "    'max_drawdown_20',\n",
    "    'parkinson_volatility',\n",
    "    'recent_high_20',\n",
    "    'volatility_20',\n",
    "    'VHF',\n",
    "    'MOBV',\n",
    "    'PVT_cumsum'\n",
    "]\n",
    "\n",
    "zscore_features = [\n",
    "    'ADTM',\n",
    "    'daily_return',\n",
    "    'MA_60_slope',\n",
    "    'MTM',\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'PSY',\n",
    "    'RSI_14',\n",
    "]\n",
    "\n",
    "standard_scaler_features = [\n",
    "    'K'\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 1. ANALYSIS BEFORE SCALING\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š ANALYSIS BEFORE SCALING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_distribution(df, feature, prefix=\"\"):\n",
    "    \"\"\"ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø©\"\"\"\n",
    "    data = df[feature].dropna()\n",
    "\n",
    "    stats_dict = {\n",
    "        'count': len(data),\n",
    "        'mean': data.mean(),\n",
    "        'std': data.std(),\n",
    "        'min': data.min(),\n",
    "        'q1': data.quantile(0.01),\n",
    "        'q5': data.quantile(0.05),\n",
    "        'q25': data.quantile(0.25),\n",
    "        'median': data.median(),\n",
    "        'q75': data.quantile(0.75),\n",
    "        'q95': data.quantile(0.95),\n",
    "        'q99': data.quantile(0.99),\n",
    "        'max': data.max(),\n",
    "        'skewness': stats.skew(data),\n",
    "        'kurtosis': stats.kurtosis(data),\n",
    "        'n_outliers_3std': ((data < (data.mean() - 3*data.std())) |\n",
    "                            (data > (data.mean() + 3*data.std()))).sum(),\n",
    "        'outlier_pct': ((data < (data.mean() - 3*data.std())) |\n",
    "                        (data > (data.mean() + 3*data.std()))).sum() / len(data) * 100\n",
    "    }\n",
    "\n",
    "    return pd.Series(stats_dict)\n",
    "\n",
    "# Create analysis dataframe\n",
    "before_stats = {}\n",
    "for feature in feature_columns:\n",
    "    before_stats[feature] = analyze_distribution(df_features, feature, \"before\")\n",
    "\n",
    "df_before = pd.DataFrame(before_stats).T\n",
    "df_before['scaling_method'] = df_before.index.map(\n",
    "    lambda x: 'none' if x in no_need_scaling\n",
    "    else 'robust' if x in robust_scaling_features\n",
    "    else 'zscore' if x in zscore_features\n",
    "    else 'standard'\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ” Features with HIGH OUTLIERS (>5% outliers):\")\n",
    "high_outliers = df_before[df_before['outlier_pct'] > 5.0].sort_values('outlier_pct', ascending=False)\n",
    "print(high_outliers[['outlier_pct', 'skewness', 'kurtosis', 'min', 'max', 'scaling_method']])\n",
    "\n",
    "print(\"\\nðŸ” Features with EXTREME SKEWNESS (|skew| > 2):\")\n",
    "extreme_skew = df_before[abs(df_before['skewness']) > 2].sort_values('skewness', key=abs, ascending=False)\n",
    "print(extreme_skew[['skewness', 'kurtosis', 'min', 'max', 'scaling_method']])\n",
    "\n",
    "print(\"\\nðŸ” Features with EXTREME VALUES:\")\n",
    "extreme_vals = df_before[(df_before['max'] > 1000) | (df_before['min'] < -1000)]\n",
    "print(extreme_vals[['min', 'max', 'mean', 'std', 'scaling_method']])\n",
    "\n",
    "# ============================================================\n",
    "# 2. APPLY WINSORIZATION (OPTIONAL - Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”§ APPLYING WINSORIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_winsorized = df_features.copy()\n",
    "\n",
    "# Apply winsorization to features with high outliers\n",
    "features_to_winsorize = [\n",
    "    'daily_return', 'MA_60_slope', 'MTM', 'PVT_cumsum', 'MOBV',\n",
    "    'recent_high_20', 'volatility_20', 'parkinson_volatility'\n",
    "]\n",
    "\n",
    "for feature in features_to_winsorize:\n",
    "    if feature in df_winsorized.columns:\n",
    "        lower = df_winsorized[feature].quantile(0.01)\n",
    "        upper = df_winsorized[feature].quantile(0.99)\n",
    "        df_winsorized[feature] = df_winsorized[feature].clip(lower, upper)\n",
    "        print(f\"âœ“ Winsorized {feature}: [{lower:.4f}, {upper:.4f}]\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. APPLY SCALING (with and without winsorization)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš™ï¸ APPLYING SCALING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Version 1: Direct scaling (current approach)\n",
    "df_scaled_v1 = df_features.copy()\n",
    "\n",
    "robust_scaler_v1 = RobustScaler()\n",
    "df_scaled_v1[robust_scaling_features] = robust_scaler_v1.fit_transform(\n",
    "    df_features[robust_scaling_features]\n",
    ")\n",
    "\n",
    "z_scaler_v1 = ZScoreScaler()\n",
    "df_scaled_v1[zscore_features] = z_scaler_v1.fit_transform(\n",
    "    df_features[zscore_features]\n",
    ")\n",
    "\n",
    "std_scaler_v1 = StandardScaler()\n",
    "df_scaled_v1[standard_scaler_features] = std_scaler_v1.fit_transform(\n",
    "    df_features[standard_scaler_features]\n",
    ")\n",
    "\n",
    "print(\"âœ“ Version 1: Direct scaling (NO winsorization)\")\n",
    "\n",
    "# Version 2: Scaling after winsorization\n",
    "df_scaled_v2 = df_winsorized.copy()\n",
    "\n",
    "robust_scaler_v2 = RobustScaler()\n",
    "df_scaled_v2[robust_scaling_features] = robust_scaler_v2.fit_transform(\n",
    "    df_winsorized[robust_scaling_features]\n",
    ")\n",
    "\n",
    "z_scaler_v2 = ZScoreScaler()\n",
    "df_scaled_v2[zscore_features] = z_scaler_v2.fit_transform(\n",
    "    df_winsorized[zscore_features]\n",
    ")\n",
    "\n",
    "std_scaler_v2 = StandardScaler()\n",
    "df_scaled_v2[standard_scaler_features] = std_scaler_v2.fit_transform(\n",
    "    df_winsorized[standard_scaler_features]\n",
    ")\n",
    "\n",
    "print(\"âœ“ Version 2: Scaling WITH winsorization\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. ANALYSIS AFTER SCALING\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ANALYSIS AFTER SCALING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze both versions\n",
    "after_stats_v1 = {}\n",
    "after_stats_v2 = {}\n",
    "\n",
    "for feature in feature_columns:\n",
    "    after_stats_v1[feature] = analyze_distribution(df_scaled_v1, feature, \"v1\")\n",
    "    after_stats_v2[feature] = analyze_distribution(df_scaled_v2, feature, \"v2\")\n",
    "\n",
    "df_after_v1 = pd.DataFrame(after_stats_v1).T\n",
    "df_after_v2 = pd.DataFrame(after_stats_v2).T\n",
    "\n",
    "# Compare mean and std\n",
    "print(\"\\nðŸ“ˆ VERSION 1 (No Winsorization) - Features with |mean| > 0.1 or std far from 1:\")\n",
    "bad_v1 = df_after_v1[\n",
    "    ((abs(df_after_v1['mean']) > 0.1) |\n",
    "     (df_after_v1['std'] < 0.5) |\n",
    "     (df_after_v1['std'] > 2.0)) &\n",
    "    (~df_after_v1.index.isin(no_need_scaling))\n",
    "]\n",
    "print(bad_v1[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "print(\"\\nðŸ“ˆ VERSION 2 (With Winsorization) - Features with |mean| > 0.1 or std far from 1:\")\n",
    "bad_v2 = df_after_v2[\n",
    "    ((abs(df_after_v2['mean']) > 0.1) |\n",
    "     (df_after_v2['std'] < 0.5) |\n",
    "     (df_after_v2['std'] > 2.0)) &\n",
    "    (~df_after_v2.index.isin(no_need_scaling))\n",
    "]\n",
    "print(bad_v2[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# ============================================================\n",
    "# 5. DETAILED COMPARISON\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”¬ DETAILED COMPARISON: Before vs After\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'before_mean': df_before['mean'],\n",
    "    'before_std': df_before['std'],\n",
    "    'before_skew': df_before['skewness'],\n",
    "    'v1_mean': df_after_v1['mean'],\n",
    "    'v1_std': df_after_v1['std'],\n",
    "    'v1_skew': df_after_v1['skewness'],\n",
    "    'v2_mean': df_after_v2['mean'],\n",
    "    'v2_std': df_after_v2['std'],\n",
    "    'v2_skew': df_after_v2['skewness'],\n",
    "    'method': df_before['scaling_method']\n",
    "})\n",
    "\n",
    "# Show features that improved with winsorization\n",
    "print(\"\\nâœ… Features that IMPROVED with winsorization:\")\n",
    "improved = comparison_df[\n",
    "    (abs(comparison_df['v2_mean']) < abs(comparison_df['v1_mean'])) |\n",
    "    (abs(comparison_df['v2_std'] - 1.0) < abs(comparison_df['v1_std'] - 1.0))\n",
    "]\n",
    "print(improved[['before_std', 'v1_mean', 'v1_std', 'v2_mean', 'v2_std', 'method']])\n",
    "\n",
    "# ============================================================\n",
    "# 6. SAVE DETAILED REPORT\n",
    "# ============================================================\n",
    "report_path = \"scaling_analysis_report.xlsx\"\n",
    "with pd.ExcelWriter(report_path, engine='openpyxl') as writer:\n",
    "    df_before.to_excel(writer, sheet_name='Before_Scaling')\n",
    "    df_after_v1.to_excel(writer, sheet_name='After_V1_NoWinsor')\n",
    "    df_after_v2.to_excel(writer, sheet_name='After_V2_WithWinsor')\n",
    "    comparison_df.to_excel(writer, sheet_name='Comparison')\n",
    "\n",
    "print(f\"\\nðŸ’¾ Detailed report saved to: {report_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. VISUALIZATION: Side-by-side comparison\n",
    "# ============================================================\n",
    "IMAGE_DIR = \"Images_scaling_analysis\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“¸ GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for feature in feature_columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    # Determine scaling method\n",
    "    if feature in robust_scaling_features:\n",
    "        method = 'RobustScaler'\n",
    "    elif feature in zscore_features:\n",
    "        method = 'StandardScaler (Z-Score)'\n",
    "    elif feature in standard_scaler_features:\n",
    "        method = 'StandardScaler'\n",
    "    else:\n",
    "        method = 'No Scaling'\n",
    "\n",
    "    fig.suptitle(f'{feature} - {method}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Original\n",
    "    axes[0, 0].hist(df_features[feature].dropna(), bins=100, alpha=0.7,\n",
    "                    color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Original Distribution', fontsize=12)\n",
    "    axes[0, 0].set_xlabel('Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axvline(df_features[feature].mean(), color='red',\n",
    "                       linestyle='--', label=f'Mean: {df_features[feature].mean():.2f}')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # 2. Winsorized\n",
    "    axes[0, 1].hist(df_winsorized[feature].dropna(), bins=100, alpha=0.7,\n",
    "                    color='lightgreen', edgecolor='black')\n",
    "    axes[0, 1].set_title('After Winsorization (1%-99%)', fontsize=12)\n",
    "    axes[0, 1].set_xlabel('Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].axvline(df_winsorized[feature].mean(), color='red',\n",
    "                       linestyle='--', label=f'Mean: {df_winsorized[feature].mean():.2f}')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # 3. Scaled V1 (no winsor)\n",
    "    axes[1, 0].hist(df_scaled_v1[feature].dropna(), bins=100, alpha=0.7,\n",
    "                    color='orange', edgecolor='black')\n",
    "    axes[1, 0].set_title('Scaled (NO Winsorization)', fontsize=12)\n",
    "    axes[1, 0].set_xlabel('Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    mean_v1 = df_scaled_v1[feature].mean()\n",
    "    std_v1 = df_scaled_v1[feature].std()\n",
    "    axes[1, 0].axvline(mean_v1, color='red', linestyle='--',\n",
    "                       label=f'Mean: {mean_v1:.3f}, Std: {std_v1:.3f}')\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # 4. Scaled V2 (with winsor)\n",
    "    axes[1, 1].hist(df_scaled_v2[feature].dropna(), bins=100, alpha=0.7,\n",
    "                    color='purple', edgecolor='black')\n",
    "    axes[1, 1].set_title('Scaled (WITH Winsorization)', fontsize=12)\n",
    "    axes[1, 1].set_xlabel('Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    mean_v2 = df_scaled_v2[feature].mean()\n",
    "    std_v2 = df_scaled_v2[feature].std()\n",
    "    axes[1, 1].axvline(mean_v2, color='red', linestyle='--',\n",
    "                       label=f'Mean: {mean_v2:.3f}, Std: {std_v2:.3f}')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'{feature}_comparison.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved: {feature}_comparison.png\")\n",
    "\n",
    "print(\"\\nâœ… ALL DONE! Check the images and Excel report for detailed analysis.\")"
   ],
   "id": "f833ad515343a7cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“Š ANALYSIS BEFORE SCALING\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Features with HIGH OUTLIERS (>5% outliers):\n",
      "Empty DataFrame\n",
      "Columns: [outlier_pct, skewness, kurtosis, min, max, scaling_method]\n",
      "Index: []\n",
      "\n",
      "ðŸ” Features with EXTREME SKEWNESS (|skew| > 2):\n",
      "                          skewness      kurtosis           min           max  \\\n",
      "daily_return           2131.687999  6.183759e+06 -9.750000e-01  2.573333e+02   \n",
      "volatility_20           508.418004  3.244742e+05  0.000000e+00  5.754219e+01   \n",
      "MA_60_slope              78.938633  3.239143e+04 -9.329281e-01  6.881144e+01   \n",
      "PVT_cumsum               29.463500  1.268447e+03 -3.776399e+08  4.411416e+09   \n",
      "MOBV                     24.705407  1.218629e+03 -7.958665e+09  2.102035e+10   \n",
      "high_low_ratio           21.816197  2.971159e+03  0.000000e+00  1.048571e+01   \n",
      "parkinson_volatility     14.469786  1.920736e+03  0.000000e+00  4.552673e+00   \n",
      "price_to_MA60             6.647672  7.637033e+02 -9.602471e-01  3.916411e+01   \n",
      "price_to_MA20             5.875331  4.820860e+02 -9.601241e-01  1.766346e+01   \n",
      "downside_deviation_10     4.493302  4.411218e+01  0.000000e+00  6.521096e-01   \n",
      "price_to_MA5              3.227106  1.890409e+02 -9.584027e-01  3.922198e+00   \n",
      "recent_high_20            3.209126  1.463794e+01  1.060400e-01  1.010080e+03   \n",
      "distance_from_high       -2.500268  9.413822e+00 -9.923533e-01  2.222220e-02   \n",
      "low_to_close_ratio       -2.464992  9.985222e+00  0.000000e+00  1.000000e+00   \n",
      "max_drawdown_20          -2.022940  6.033688e+00 -9.891804e-01  0.000000e+00   \n",
      "\n",
      "                      scaling_method  \n",
      "daily_return                  zscore  \n",
      "volatility_20                 robust  \n",
      "MA_60_slope                   zscore  \n",
      "PVT_cumsum                    robust  \n",
      "MOBV                          robust  \n",
      "high_low_ratio                robust  \n",
      "parkinson_volatility          robust  \n",
      "price_to_MA60                 zscore  \n",
      "price_to_MA20                 zscore  \n",
      "downside_deviation_10         robust  \n",
      "price_to_MA5                  zscore  \n",
      "recent_high_20                robust  \n",
      "distance_from_high            robust  \n",
      "low_to_close_ratio            robust  \n",
      "max_drawdown_20               robust  \n",
      "\n",
      "ðŸ” Features with EXTREME VALUES:\n",
      "                         min           max          mean           std  \\\n",
      "recent_high_20  1.060400e-01  1.010080e+03  3.313228e+01  4.181002e+01   \n",
      "PVT_cumsum     -3.776399e+08  4.411416e+09  7.364432e+06  7.580017e+07   \n",
      "MOBV           -7.958665e+09  2.102035e+10  5.156853e+07  2.903180e+08   \n",
      "\n",
      "               scaling_method  \n",
      "recent_high_20         robust  \n",
      "PVT_cumsum             robust  \n",
      "MOBV                   robust  \n",
      "\n",
      "================================================================================\n",
      "ðŸ”§ APPLYING WINSORIZATION\n",
      "================================================================================\n",
      "âœ“ Winsorized daily_return: [-0.0880, 0.0983]\n",
      "âœ“ Winsorized MA_60_slope: [-0.2892, 0.3350]\n",
      "âœ“ Winsorized MTM: [-12.2649, 11.9100]\n",
      "âœ“ Winsorized PVT_cumsum: [-18050260.6833, 140806099.8055]\n",
      "âœ“ Winsorized MOBV: [-215318413.3600, 804541150.2400]\n",
      "âœ“ Winsorized recent_high_20: [1.0300, 216.0000]\n",
      "âœ“ Winsorized volatility_20: [0.0032, 0.1133]\n",
      "âœ“ Winsorized parkinson_volatility: [0.0030, 0.0996]\n",
      "\n",
      "================================================================================\n",
      "âš™ï¸ APPLYING SCALING\n",
      "================================================================================\n",
      "âœ“ Version 1: Direct scaling (NO winsorization)\n",
      "âœ“ Version 2: Scaling WITH winsorization\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š ANALYSIS AFTER SCALING\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ˆ VERSION 1 (No Winsorization) - Features with |mean| > 0.1 or std far from 1:\n",
      "                           mean        std         min          max\n",
      "high_low_ratio         0.357731   1.401324   -0.873960   364.803681\n",
      "volatility_20          0.342344   4.452897   -0.996340  2955.379422\n",
      "parkinson_volatility   0.317105   1.200338   -0.990346   252.955639\n",
      "recent_high_20         0.462078   1.390664   -0.636424    32.956826\n",
      "distance_from_high    -0.336966   1.033752  -10.842989     0.837674\n",
      "low_to_close_ratio    -0.288149   1.001486  -11.361911     0.721212\n",
      "max_drawdown_20       -0.268612   0.944818   -7.957861     0.829416\n",
      "downside_deviation_10  0.316503   1.186579   -0.862599    56.735632\n",
      "PVT_cumsum             5.859994  60.832609 -303.121055  3540.284369\n",
      "MOBV                   1.389463   8.698217 -238.605120   629.635112\n",
      "VHF                    0.129680   0.780533   -2.169402     4.696880\n",
      "\n",
      "ðŸ“ˆ VERSION 2 (With Winsorization) - Features with |mean| > 0.1 or std far from 1:\n",
      "                           mean        std        min         max\n",
      "high_low_ratio         0.357731   1.401324  -0.873960  364.803681\n",
      "volatility_20          0.289471   1.006155  -0.832678    4.824485\n",
      "parkinson_volatility   0.292957   0.989607  -0.822940    4.564659\n",
      "recent_high_20         0.444563   1.289083  -0.605692    6.544533\n",
      "distance_from_high    -0.336966   1.033752 -10.842989    0.837674\n",
      "low_to_close_ratio    -0.288149   1.001486 -11.361911    0.721212\n",
      "max_drawdown_20       -0.268612   0.944818  -7.957861    0.829416\n",
      "downside_deviation_10  0.316503   1.186579  -0.862599   56.735632\n",
      "PVT_cumsum             3.256725  15.129344 -14.536295  112.952169\n",
      "MOBV                   1.154366   3.909406  -6.606736   23.949274\n",
      "VHF                    0.129680   0.780533  -2.169402    4.696880\n",
      "\n",
      "================================================================================\n",
      "ðŸ”¬ DETAILED COMPARISON: Before vs After\n",
      "================================================================================\n",
      "\n",
      "âœ… Features that IMPROVED with winsorization:\n",
      "                        before_std       v1_mean     v1_std       v2_mean  \\\n",
      "daily_return          8.726612e-02  5.339440e-18   1.000000  2.373085e-18   \n",
      "MA_60_slope           1.276520e-01 -2.401047e-18   1.000000  0.000000e+00   \n",
      "volatility_20         8.667011e-02  3.423442e-01   4.452897  2.894712e-01   \n",
      "parkinson_volatility  2.151932e-02  3.171052e-01   1.200338  2.929569e-01   \n",
      "recent_high_20        4.181002e+01  4.620782e-01   1.390664  4.445635e-01   \n",
      "PVT_cumsum            7.580017e+07  5.859994e+00  60.832609  3.256725e+00   \n",
      "MOBV                  2.903180e+08  1.389463e+00   8.698217  1.154366e+00   \n",
      "\n",
      "                         v2_std  method  \n",
      "daily_return           1.000000  zscore  \n",
      "MA_60_slope            1.000000  zscore  \n",
      "volatility_20          1.006155  robust  \n",
      "parkinson_volatility   0.989607  robust  \n",
      "recent_high_20         1.289083  robust  \n",
      "PVT_cumsum            15.129344  robust  \n",
      "MOBV                   3.909406  robust  \n",
      "\n",
      "ðŸ’¾ Detailed report saved to: scaling_analysis_report.xlsx\n",
      "\n",
      "================================================================================\n",
      "ðŸ“¸ GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "âœ“ Saved: daily_return_comparison.png\n",
      "âœ“ Saved: high_low_ratio_comparison.png\n",
      "âœ“ Saved: price_to_MA5_comparison.png\n",
      "âœ“ Saved: price_to_MA20_comparison.png\n",
      "âœ“ Saved: price_to_MA60_comparison.png\n",
      "âœ“ Saved: MA_60_slope_comparison.png\n",
      "âœ“ Saved: volatility_20_comparison.png\n",
      "âœ“ Saved: RSI_14_comparison.png\n",
      "âœ“ Saved: parkinson_volatility_comparison.png\n",
      "âœ“ Saved: recent_high_20_comparison.png\n",
      "âœ“ Saved: distance_from_high_comparison.png\n",
      "âœ“ Saved: low_to_close_ratio_comparison.png\n",
      "âœ“ Saved: price_position_20_comparison.png\n",
      "âœ“ Saved: max_drawdown_20_comparison.png\n",
      "âœ“ Saved: downside_deviation_10_comparison.png\n",
      "âœ“ Saved: month_sin_comparison.png\n",
      "âœ“ Saved: month_cos_comparison.png\n",
      "âœ“ Saved: is_up_day_comparison.png\n",
      "âœ“ Saved: PVT_cumsum_comparison.png\n",
      "âœ“ Saved: MOBV_comparison.png\n",
      "âœ“ Saved: MTM_comparison.png\n",
      "âœ“ Saved: ADTM_comparison.png\n",
      "âœ“ Saved: PSY_comparison.png\n",
      "âœ“ Saved: VHF_comparison.png\n",
      "âœ“ Saved: K_comparison.png\n",
      "\n",
      "âœ… ALL DONE! Check the images and Excel report for detailed analysis.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features_scaled.describe()",
   "id": "ac79b62b439c3d7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "cb8e9af69968a3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "61c369a7c5beb523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "cd34fedd16ffb2a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "3db3374b37d5161f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 5000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 â­â­â­\n",
    "    'MOBV',                 # MI = 0.0209 â­â­\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 â­\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ac1312d7bbf04afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "5cacce17fbd4b72f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20','close_30d_future',\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\nâœ“ Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "4177fe98c9aaf3a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "ef062214d42f8b5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "e51fd72a1a8f1763"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: DATA QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] DATA QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "# ============================================================\n",
    "feature_columns = [\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 â­â­â­\n",
    "    'MOBV',                 # MI = 0.0209 â­â­\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 â­\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features\n",
    "\n",
    "]\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values in engineered features:\")\n",
    "missing_stats = df_features[feature_columns].isnull().sum()\n",
    "missing_pct = (missing_stats / len(df_features) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_stats,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df_features[feature_columns].describe())\n",
    "\n",
    "IMAGE_DIR = \"Images2\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Visualization: Distribution of features\n",
    "for feature in feature_columns:\n",
    "    data = df_features[feature].dropna()\n",
    "\n",
    "    # Remove extreme outliers for better visualization (keep 1st-99th percentile)\n",
    "    q1, q99 = data.quantile([0.01, 0.99])\n",
    "    data_filtered = data[(data >= q1) & (data <= q99)]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))  # wider figure\n",
    "    plt.hist(data_filtered, bins=100, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'{feature} Distribution\\n(1st-99th percentile)', fontsize=14)\n",
    "    plt.xlabel('Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(IMAGE_DIR, f'feature_{feature}.png'),\n",
    "        dpi=300,\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ],
   "id": "c24ea41df1ba4346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "46b315725a20227c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "f9e8fcac0ffa929d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "f8a5745e7bf6e107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "bc2220133a071d7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 1000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 â­â­â­\n",
    "    'MOBV',                 # MI = 0.0209 â­â­\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 â­\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features:\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\",\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "eac3f8e6a29c7983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6686ea2b553342fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
