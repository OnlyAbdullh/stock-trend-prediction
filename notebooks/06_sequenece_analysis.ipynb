{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# Stock Price Prediction - Feature Analysis for RNN Sequence Selection\n",
    "# Analyzing features to determine optimal sequence length for LSTM/GRU models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STOCK PRICE PREDICTION - FEATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nObjective: Determine optimal sequence length for RNN input\")\n",
    "print(\"Target: Predict if close price > current price after 30 trading days\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Convert Date to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Sort by Ticker and Date\n",
    "df = df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['Ticker'].nunique():,}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "103ad25fb7ef92a9",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create all derived features for analysis\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Sort to ensure proper calculation\n",
    "    df = df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    # Group by ticker for calculations\n",
    "    grouped = df.groupby('Ticker')\n",
    "\n",
    "    print(\"\\nCalculating features:\")\n",
    "\n",
    "    # 1. Daily Return: (Close - Open) / Open\n",
    "    print(\"  - daily_return\")\n",
    "    df['daily_return'] = (df['Close'] - df['Open']) / df['Open']\n",
    "\n",
    "    # 2. High-Low Ratio: (High - Low) / Close\n",
    "    print(\"  - high_low_ratio\")\n",
    "    df['high_low_ratio'] = (df['High'] - df['Low']) / df['Close']\n",
    "\n",
    "    # 3. Return 30 days: (Close_t - Close_t-30) / Close_t-30\n",
    "    print(\"  - return_30\")\n",
    "    df['close_30d_ago'] = grouped['Close'].shift(30)\n",
    "    df['return_30'] = (df['Close'] - df['close_30d_ago']) / df['close_30d_ago']\n",
    "\n",
    "    # 4-6. Moving Averages\n",
    "    print(\"  - MA_5, MA_10, MA_30\")\n",
    "    df['MA_5'] = grouped['Close'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())\n",
    "    df['MA_10'] = grouped['Close'].transform(lambda x: x.rolling(window=10, min_periods=1).mean())\n",
    "    df['MA_30'] = grouped['Close'].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "    # 7. Standard Deviation 10 days\n",
    "    print(\"  - STD_10\")\n",
    "    df['STD_10'] = grouped['Close'].transform(lambda x: x.rolling(window=10, min_periods=1).std())\n",
    "\n",
    "    # 8. Log Volume\n",
    "    print(\"  - log_volume\")\n",
    "    df['log_volume'] = np.log(df['Volume'] + 1)\n",
    "\n",
    "    # 9. Volume Ratio: Volume / Rolling Mean Volume (10 days)\n",
    "    print(\"  - volume_ratio\")\n",
    "    df['volume_ma_10'] = grouped['Volume'].transform(lambda x: x.rolling(window=10, min_periods=1).mean())\n",
    "    df['volume_ratio'] = df['Volume'] / df['volume_ma_10']\n",
    "\n",
    "    # 10. Dividend Yield\n",
    "    print(\"  - dividend_yield\")\n",
    "    df['dividend_yield'] = df['Dividends'] / df['Close']\n",
    "\n",
    "    # 11. Days since last split\n",
    "    print(\"  - day_since_last_split\")\n",
    "    df['has_split'] = (df['Stock Splits'] > 0).astype(int)\n",
    "    df['day_since_last_split'] = 0\n",
    "\n",
    "    for ticker in df['Ticker'].unique():\n",
    "        ticker_mask = df['Ticker'] == ticker\n",
    "        ticker_data = df[ticker_mask].copy()\n",
    "        split_dates = ticker_data[ticker_data['has_split'] == 1].index\n",
    "\n",
    "        for idx in ticker_data.index:\n",
    "            previous_splits = split_dates[split_dates < idx]\n",
    "            if len(previous_splits) > 0:\n",
    "                last_split_idx = previous_splits[-1]\n",
    "                days_diff = idx - last_split_idx\n",
    "                df.loc[idx, 'day_since_last_split'] = days_diff\n",
    "            else:\n",
    "                df.loc[idx, 'day_since_last_split'] = 999  # No previous split\n",
    "\n",
    "    # Create target variable: Will price be higher in 30 days?\n",
    "    print(\"  - target (price > current after 30 days)\")\n",
    "    df['close_30d_future'] = grouped['Close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['Close']).astype(int)\n",
    "\n",
    "    # Drop temporary columns\n",
    "    df = df.drop(['close_30d_ago', 'volume_ma_10', 'has_split', 'close_30d_future'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\n✓ Feature engineering complete!\")\n",
    "print(f\"Total features created: 11\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "cf32a9290742fcf2",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: DATA QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] DATA QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List of engineered features\n",
    "feature_columns = [\n",
    "    'daily_return', 'high_low_ratio', 'return_30',\n",
    "    'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "    'log_volume', 'volume_ratio', 'dividend_yield', 'day_since_last_split'\n",
    "]\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values in engineered features:\")\n",
    "missing_stats = df_features[feature_columns].isnull().sum()\n",
    "missing_pct = (missing_stats / len(df_features) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_stats,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df_features[feature_columns].describe())\n",
    "\n",
    "# Visualization: Distribution of features\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(feature_columns):\n",
    "    data = df_features[feature].dropna()\n",
    "\n",
    "    # Remove extreme outliers for better visualization (keep 99% of data)\n",
    "    q1, q99 = data.quantile([0.01, 0.99])\n",
    "    data_filtered = data[(data >= q1) & (data <= q99)]\n",
    "\n",
    "    axes[idx].hist(data_filtered, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{feature}\\n(1st-99th percentile)', fontsize=10)\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('01_feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Saved: 01_feature_distributions.png\")\n",
    "plt.show()"
   ],
   "id": "945cc2f9a1981407",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# Sample data for faster computation (use 500k records)\n",
    "sample_size = min(500000, len(df_features))\n",
    "df_sample = df_features.dropna(subset=feature_columns).sample(n=sample_size, random_state=42)\n",
    "\n",
    "max_lags = 60  # Analyze up to 60 days lag\n",
    "\n",
    "# Create autocorrelation plots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "autocorr_results = {}\n",
    "\n",
    "for idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    # Calculate autocorrelation\n",
    "    data = df_sample[feature].dropna()\n",
    "    autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "\n",
    "    # Store results\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # Plot\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "    axes[idx].bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    axes[idx].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[idx].axhline(y=0.05, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    axes[idx].axhline(y=-0.05, color='red', linestyle='--', linewidth=1)\n",
    "    axes[idx].set_title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Lag (days)')\n",
    "    axes[idx].set_ylabel('Autocorrelation')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    # Find optimal lag (where autocorr drops below threshold)\n",
    "    threshold = 0.05\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  Low autocorrelation (independent)\")\n",
    "\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('02_autocorrelation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Saved: 02_autocorrelation_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# Summary table of autocorrelation decay\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    # Find lag where autocorr first drops below 0.05\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < 0.05)[0]\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))"
   ],
   "id": "15dcc6d4e09c5fdd",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# Calculate target-lag correlations\n",
    "target_lag_results = {}\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature}\")\n",
    "\n",
    "    correlations = []\n",
    "    lags = range(1, max_lags + 1)\n",
    "\n",
    "    # Sample ticker for analysis\n",
    "    sample_ticker = df_features['Ticker'].iloc[0]\n",
    "    ticker_data = df_features[df_features['Ticker'] == sample_ticker].copy()\n",
    "    ticker_data = ticker_data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    for lag in lags:\n",
    "        lagged_feature = ticker_data[feature].shift(lag)\n",
    "        valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "        if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "            corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "            correlations.append(corr)\n",
    "        else:\n",
    "            correlations.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations\n",
    "\n",
    "    # Plot\n",
    "    axes[idx].plot(lags, correlations, marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[idx].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[idx].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[idx].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[idx].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Lag (days)')\n",
    "    axes[idx].set_ylabel('Correlation with Target')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    # Find peak correlation\n",
    "    max_corr_idx = np.argmax(np.abs(correlations))\n",
    "    max_corr = correlations[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_target_lag_correlation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Saved: 03_target_lag_correlation.png\")\n",
    "plt.show()"
   ],
   "id": "831aa8e2da2b9308",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# Calculate rolling mean and std for different windows\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Sample one ticker for analysis\n",
    "sample_ticker_data = df_features[df_features['Ticker'] == df_features['Ticker'].iloc[0]].copy()\n",
    "sample_ticker_data = sample_ticker_data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"Analyzing rolling stats: {feature}\")\n",
    "\n",
    "    for window in windows:\n",
    "        rolling_mean = sample_ticker_data[feature].rolling(window=window).mean()\n",
    "        rolling_std = sample_ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "        # Calculate stability (std of rolling mean)\n",
    "        mean_stability = rolling_mean.std()\n",
    "        std_stability = rolling_std.std()\n",
    "\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(mean_stability)\n",
    "        rolling_stats_results[feature]['std_std'].append(std_stability)\n",
    "\n",
    "# Plot rolling statistics\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('04_rolling_statistics.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Saved: 04_rolling_statistics.png\")\n",
    "plt.show()"
   ],
   "id": "5b7b00585797e35c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: FEATURE CORRELATION HEATMAP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] FEATURE CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_sample = df_features[feature_columns + ['target']].dropna().sample(n=min(100000, len(df_features)), random_state=42)\n",
    "correlation_matrix = corr_sample.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.3f',\n",
    "            cmap='coolwarm', center=0, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('05_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: 05_correlation_heatmap.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print high correlations with target\n",
    "print(\"\\nCorrelations with Target (sorted by absolute value):\")\n",
    "target_corr = correlation_matrix['target'].drop('target').sort_values(key=abs, ascending=False)\n",
    "print(target_corr)"
   ],
   "id": "5ce85e7cb0d219ca",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: RECOMMENDATIONS AND SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[8] RECOMMENDATIONS FOR RNN SEQUENCE LENGTH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze autocorrelation decay to recommend sequence length\n",
    "avg_decay_point = decay_df['Decay_Point'].mean()\n",
    "recommended_seq_min = int(avg_decay_point * 0.7)\n",
    "recommended_seq_optimal = int(avg_decay_point)\n",
    "recommended_seq_max = int(avg_decay_point * 1.3)\n",
    "\n",
    "print(f\"\\nBased on autocorrelation analysis:\")\n",
    "print(f\"  • Average autocorrelation decay point: {avg_decay_point:.1f} days\")\n",
    "print(f\"  • Recommended minimum sequence length: {recommended_seq_min} days\")\n",
    "print(f\"  • Recommended optimal sequence length: {recommended_seq_optimal} days\")\n",
    "print(f\"  • Recommended maximum sequence length: {recommended_seq_max} days\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Find features with strongest autocorrelation\n",
    "strong_autocorr = decay_df.nlargest(3, 'Lag_30')['Feature'].tolist()\n",
    "print(f\"\\n1. Features with strongest temporal dependencies:\")\n",
    "for feat in strong_autocorr:\n",
    "    print(f\"   • {feat}\")\n",
    "\n",
    "# Find features most correlated with target\n",
    "strong_target = target_corr.head(3)\n",
    "print(f\"\\n2. Features most correlated with target:\")\n",
    "for feat, corr in strong_target.items():\n",
    "    print(f\"   • {feat}: {corr:.4f}\")\n",
    "\n",
    "# Identify potential issues\n",
    "print(f\"\\n3. ⚠️  WARNINGS:\")\n",
    "\n",
    "# Check return_30\n",
    "if 'return_30' in feature_columns:\n",
    "    print(f\"   • return_30: POTENTIAL DATA LEAKAGE!\")\n",
    "    print(f\"     This feature uses close price from 30 days ago,\")\n",
    "    print(f\"     which overlaps with your 30-day prediction target.\")\n",
    "    print(f\"     Recommendation: Use return_20 or return_15 instead.\")\n",
    "\n",
    "# Check dividend_yield sparsity\n",
    "div_nonzero = (df_features['dividend_yield'] > 0).sum()\n",
    "div_pct = div_nonzero / len(df_features) * 100\n",
    "print(f\"   • dividend_yield: {div_pct:.2f}% non-zero values (very sparse)\")\n",
    "if div_pct < 1:\n",
    "    print(f\"     Recommendation: Consider removing or separate treatment\")\n",
    "\n",
    "# Check day_since_last_split sparsity\n",
    "split_occurred = (df_features['day_since_last_split'] < 999).sum()\n",
    "split_pct = split_occurred / len(df_features) * 100\n",
    "print(f\"   • day_since_last_split: Only {split_pct:.2f}% of stocks have splits\")\n",
    "print(f\"     Recommendation: Consider removing this feature\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FINAL RECOMMENDATIONS:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"\"\"\n",
    "1. SEQUENCE LENGTH:\n",
    "   Use {recommended_seq_optimal} days as your RNN input sequence length\n",
    "   (Range: {recommended_seq_min}-{recommended_seq_max} days for experimentation)\n",
    "\n",
    "2. CRITICAL FEATURES (include in model):\n",
    "   {', '.join(strong_autocorr[:5])}\n",
    "\n",
    "3. FEATURES TO MODIFY:\n",
    "   • Replace 'return_30' with 'return_20' or 'return_15'\n",
    "   • Consider removing 'dividend_yield' and 'day_since_last_split'\n",
    "\n",
    "4. MODEL ARCHITECTURE SUGGESTIONS:\n",
    "   • Use LSTM or GRU layers\n",
    "   • Input shape: (batch_size, {recommended_seq_optimal}, num_features)\n",
    "   • Consider bidirectional layers for better temporal modeling\n",
    "   • Use dropout (0.2-0.3) to prevent overfitting\n",
    "\n",
    "5. PREPROCESSING:\n",
    "   • Normalize/standardize features per ticker\n",
    "   • Handle missing values with forward fill (within ticker)\n",
    "   • Remove first 30 days per ticker (insufficient history)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  • 01_feature_distributions.png\")\n",
    "print(\"  • 02_autocorrelation_analysis.png\")\n",
    "print(\"  • 03_target_lag_correlation.png\")\n",
    "print(\"  • 04_rolling_statistics.png\")\n",
    "print(\"  • 05_correlation_heatmap.png\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ],
   "id": "f7c527494662c007"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
