{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Stock Price Prediction - Feature Analysis for RNN Sequence Selection\n",
    "# Analyzing features to determine optimal sequence length for LSTM/GRU models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STOCK PRICE PREDICTION - FEATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nObjective: Determine optimal sequence length for RNN input\")\n",
    "print(\"Target: Predict if close price > current price after 30 trading days\")\n",
    "print(\"=\"*80)"
   ],
   "id": "484f1387dbb2e619"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "14e5e8d25a80716a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20',\n",
    "        'close_30d_future'\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\n✓ Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "6ec32e490f9d4f43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "863e4fa68b614557"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "f1d478538e04da36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler as ZScoreScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "feature_columns = [\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 ⭐⭐⭐\n",
    "    'MOBV',                 # MI = 0.0209 ⭐⭐\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 ⭐\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features\n",
    "\n",
    "]\n",
    "\n",
    "# todo: here the normalization start\n",
    "# ============================================================\n",
    "# Feature lists (with correct commas)\n",
    "no_need_scaling = [\n",
    "    'is_up_day',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'price_position_20',\n",
    "]\n",
    "\n",
    "robust_scaling_features = [\n",
    "    'distance_from_high',\n",
    "    'downside_deviation_10',\n",
    "    'high_low_ratio',\n",
    "    'low_to_close_ratio',\n",
    "    'max_drawdown_20',\n",
    "    'parkinson_volatility',\n",
    "    'recent_high_20',\n",
    "    'volatility_20',\n",
    "    'VHF',\n",
    "    'MOBV',\n",
    "    'PVT_cumsum'\n",
    "]\n",
    "\n",
    "zscore_features = [\n",
    "    'ADTM',\n",
    "    'daily_return',\n",
    "    'MA_60_slope',\n",
    "    'MTM',\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'PSY',\n",
    "    'RSI_14',\n",
    "    # 'MOBV',          # optional: also in robust_scaling\n",
    "    # 'PVT_cumsum'     # optional: also in robust_scaling\n",
    "]\n",
    "\n",
    "standard_scaler_features = [\n",
    "    'K'\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# Create a copy for scaled data\n",
    "df_features_scaled = df_features.copy()\n",
    "\n",
    "# Apply scalers\n",
    "robust_scaler = RobustScaler()\n",
    "df_features_scaled[robust_scaling_features] = robust_scaler.fit_transform(\n",
    "    df_features[robust_scaling_features]\n",
    ")\n",
    "\n",
    "z_scaler = ZScoreScaler()\n",
    "df_features_scaled[zscore_features] = z_scaler.fit_transform(\n",
    "    df_features[zscore_features]\n",
    ")\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "df_features_scaled[standard_scaler_features] = std_scaler.fit_transform(\n",
    "    df_features[standard_scaler_features]\n",
    ")\n",
    "\n",
    "# todo: here normalization end\n",
    "# ============================================================\n",
    "# Visualization: unified function\n",
    "IMAGE_DIR = \"Images4\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "for feature in feature_columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))  # 1 row, 2 columns\n",
    "\n",
    "    # Original distribution\n",
    "    axes[0].hist(df_features[feature].dropna(), bins=100, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title(f\"{feature} Original\", fontsize=12)\n",
    "    axes[0].set_xlabel('Value')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Determine which scaler was applied\n",
    "    if feature in robust_scaling_features:\n",
    "        scaled_label = 'Robust Scaled'\n",
    "    elif feature in zscore_features:\n",
    "        scaled_label = 'Z-Score Scaled'\n",
    "    elif feature in standard_scaler_features:\n",
    "        scaled_label = 'Standard Scaled'\n",
    "    else:\n",
    "        scaled_label = None\n",
    "\n",
    "    # Scaled distribution (if any)\n",
    "    if scaled_label:\n",
    "        axes[1].hist(df_features_scaled[feature].dropna(), bins=100, alpha=0.7, color='orange', edgecolor='black')\n",
    "        axes[1].set_title(f\"{feature} {scaled_label}\", fontsize=12)\n",
    "        axes[1].set_xlabel('Value')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1].hist(df_features[feature].dropna(), bins=100, alpha=0.7, color='orange', edgecolor='black')\n",
    "        axes[1].set_title(f\"{feature} (No Scaling)\", fontsize=12)\n",
    "        axes[1].set_xlabel('Value')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'{feature}_before_after.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ],
   "id": "f833ad515343a7cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features_scaled.describe()",
   "id": "ac79b62b439c3d7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "cb8e9af69968a3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n✓ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "61c369a7c5beb523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n✓ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\n✓ Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "cd34fedd16ffb2a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n✓ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\n✓ Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "3db3374b37d5161f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 5000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 ⭐⭐⭐\n",
    "    'MOBV',                 # MI = 0.0209 ⭐⭐\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 ⭐\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ac1312d7bbf04afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "5cacce17fbd4b72f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20','close_30d_future',\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\n✓ Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "4177fe98c9aaf3a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "ef062214d42f8b5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "e51fd72a1a8f1763"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: DATA QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] DATA QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "# ============================================================\n",
    "feature_columns = [\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 ⭐⭐⭐\n",
    "    'MOBV',                 # MI = 0.0209 ⭐⭐\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 ⭐\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features\n",
    "\n",
    "]\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values in engineered features:\")\n",
    "missing_stats = df_features[feature_columns].isnull().sum()\n",
    "missing_pct = (missing_stats / len(df_features) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_stats,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df_features[feature_columns].describe())\n",
    "\n",
    "IMAGE_DIR = \"Images2\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Visualization: Distribution of features\n",
    "for feature in feature_columns:\n",
    "    data = df_features[feature].dropna()\n",
    "\n",
    "    # Remove extreme outliers for better visualization (keep 1st-99th percentile)\n",
    "    q1, q99 = data.quantile([0.01, 0.99])\n",
    "    data_filtered = data[(data >= q1) & (data <= q99)]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))  # wider figure\n",
    "    plt.hist(data_filtered, bins=100, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'{feature} Distribution\\n(1st-99th percentile)', fontsize=14)\n",
    "    plt.xlabel('Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(IMAGE_DIR, f'feature_{feature}.png'),\n",
    "        dpi=300,\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ],
   "id": "c24ea41df1ba4346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "46b315725a20227c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n✓ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "f9e8fcac0ffa929d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n✓ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\n✓ Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "f8a5745e7bf6e107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n✓ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\n✓ Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "bc2220133a071d7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 1000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # ════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 ⭐⭐⭐\n",
    "    'MOBV',                 # MI = 0.0209 ⭐⭐\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 ⭐\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features:\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\",\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "eac3f8e6a29c7983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6686ea2b553342fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
