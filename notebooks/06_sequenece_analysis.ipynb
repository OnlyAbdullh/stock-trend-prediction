{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:46:36.311350500Z",
     "start_time": "2026-01-28T14:46:36.175022100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stock Price Prediction - Feature Analysis for RNN Sequence Selection\n",
    "# Analyzing features to determine optimal sequence length for LSTM/GRU models\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STOCK PRICE PREDICTION - FEATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nObjective: Determine optimal sequence length for RNN input\")\n",
    "print(\"Target: Predict if close price > current price after 30 trading days\")\n",
    "print(\"=\"*80)"
   ],
   "id": "484f1387dbb2e619",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STOCK PRICE PREDICTION - FEATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Objective: Determine optimal sequence length for RNN input\n",
      "Target: Predict if close price > current price after 30 trading days\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:46:51.889592300Z",
     "start_time": "2026-01-28T14:46:37.335710400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "14e5e8d25a80716a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] LOADING DATA...\n",
      "Total records: 12,269,060\n",
      "Unique tickers: 4,925\n",
      "date range: 2010-01-04 00:00:00 to 2024-09-23 00:00:00\n",
      "\n",
      "Memory usage: 1645.91 MB\n",
      "\n",
      "================================================================================\n",
      "DATA OVERVIEW\n",
      "================================================================================\n",
      "     ticker       date       open       high        low      close     volume  \\\n",
      "0  ticker_1 2010-01-04  27.875437  28.009543  27.570655  27.662090  2142300.0   \n",
      "1  ticker_1 2010-01-05  27.729151  27.814489  27.131774  27.302454  2856000.0   \n",
      "2  ticker_1 2010-01-06  27.278065  27.729145  27.278065  27.595039  2035400.0   \n",
      "3  ticker_1 2010-01-07  27.637703  27.643798  27.375590  27.497503  1993400.0   \n",
      "4  ticker_1 2010-01-08  27.424356  27.613320  27.253676  27.582842  1306400.0   \n",
      "\n",
      "   dividends  stock_splits  missing_days    return  return_is_outlier  \n",
      "0        0.0           0.0             0       NaN              False  \n",
      "1        0.0           0.0             0 -0.013001              False  \n",
      "2        0.0           0.0             0  0.010716              False  \n",
      "3        0.0           0.0             0 -0.003535              False  \n",
      "4        0.0           0.0             0  0.003104              False  \n",
      "\n",
      "Data types:\n",
      "ticker                       object\n",
      "date                 datetime64[ns]\n",
      "open                        float64\n",
      "high                        float64\n",
      "low                         float64\n",
      "close                       float64\n",
      "volume                      float64\n",
      "dividends                   float64\n",
      "stock_splits                float64\n",
      "missing_days                  int64\n",
      "return                      float64\n",
      "return_is_outlier              bool\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "ticker                  0\n",
      "date                    0\n",
      "open                    0\n",
      "high                    0\n",
      "low                     0\n",
      "close                   0\n",
      "volume                  0\n",
      "dividends               0\n",
      "stock_splits            0\n",
      "missing_days            0\n",
      "return               4925\n",
      "return_is_outlier       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:47:40.861890700Z",
     "start_time": "2026-01-28T14:47:04.333410200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20',\n",
    "        'close_30d_future'\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\nâœ“ Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "6ec32e490f9d4f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[2] ENGINEERING FEATURES...\n",
      "================================================================================\n",
      " - Target variable...\n",
      "\n",
      "âœ“ Feature engineering complete!\n",
      "Total features created: 25\n",
      "Rows with complete features: 12,121,310\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "863e4fa68b614557"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "f1d478538e04da36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:48:02.639931500Z",
     "start_time": "2026-01-28T14:48:02.416686100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Improved Scaling Analysis with Advanced Outlier Handling\n",
    "=========================================================\n",
    "This script performs comprehensive normalization analysis with:\n",
    "1. Winsorization (enhanced)\n",
    "2. Log transformation for skewed features\n",
    "3. Special handling for cumulative features\n",
    "4. Three-way comparison\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler as ZScoreScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE DEFINITIONS\n",
    "# ============================================================\n",
    "\n",
    "feature_columns = [\n",
    "    # Price Features (2)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (6)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (2)\n",
    "    'PVT_cumsum',\n",
    "    'MOBV',\n",
    "\n",
    "    # Directional Movement (1)\n",
    "    'MTM',\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',\n",
    "    'VHF',\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',\n",
    "]\n",
    "\n",
    "# Feature categorization\n",
    "no_need_scaling = [\n",
    "    'is_up_day',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'price_position_20',\n",
    "]\n",
    "\n",
    "robust_scaling_features = [\n",
    "    'distance_from_high',\n",
    "    'downside_deviation_10',\n",
    "    'high_low_ratio',\n",
    "    'low_to_close_ratio',\n",
    "    'max_drawdown_20',\n",
    "    'parkinson_volatility',\n",
    "    'recent_high_20',\n",
    "    'volatility_20',\n",
    "    'VHF',\n",
    "    'MOBV',\n",
    "    'PVT_cumsum'\n",
    "]\n",
    "\n",
    "zscore_features = [\n",
    "    'ADTM',\n",
    "    'daily_return',\n",
    "    'MA_60_slope',\n",
    "    'MTM',\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'PSY',\n",
    "    'RSI_14',\n",
    "]\n",
    "\n",
    "standard_scaler_features = [\n",
    "    'K'\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def analyze_distribution(df, feature, prefix=\"\"):\n",
    "    \"\"\"Comprehensive distribution analysis\"\"\"\n",
    "    data = df[feature].dropna()\n",
    "\n",
    "    stats_dict = {\n",
    "        'count': len(data),\n",
    "        'mean': data.mean(),\n",
    "        'std': data.std(),\n",
    "        'min': data.min(),\n",
    "        'q1': data.quantile(0.01),\n",
    "        'q5': data.quantile(0.05),\n",
    "        'q25': data.quantile(0.25),\n",
    "        'median': data.median(),\n",
    "        'q75': data.quantile(0.75),\n",
    "        'q95': data.quantile(0.95),\n",
    "        'q99': data.quantile(0.99),\n",
    "        'max': data.max(),\n",
    "        'skewness': stats.skew(data),\n",
    "        'kurtosis': stats.kurtosis(data),\n",
    "        'n_outliers_3std': ((data < (data.mean() - 3*data.std())) |\n",
    "                            (data > (data.mean() + 3*data.std()))).sum(),\n",
    "        'outlier_pct': ((data < (data.mean() - 3*data.std())) |\n",
    "                        (data > (data.mean() + 3*data.std()))).sum() / len(data) * 100\n",
    "    }\n",
    "\n",
    "    return pd.Series(stats_dict)\n",
    "\n",
    "\n",
    "def apply_winsorization(df, features, lower_pct=0.01, upper_pct=0.99):\n",
    "    \"\"\"Apply winsorization to specified features\"\"\"\n",
    "    df_winsor = df.copy()\n",
    "    winsor_params = {}\n",
    "\n",
    "    for feature in tqdm(features, desc=\"Winsorizing\"):\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        lower = df[feature].quantile(lower_pct)\n",
    "        upper = df[feature].quantile(upper_pct)\n",
    "        df_winsor[feature] = df[feature].clip(lower, upper)\n",
    "        winsor_params[feature] = {'lower': lower, 'upper': upper}\n",
    "\n",
    "    return df_winsor, winsor_params\n",
    "\n",
    "\n",
    "def apply_log_transform(df, features):\n",
    "    \"\"\"Apply log transformation for highly skewed features\"\"\"\n",
    "    df_log = df.copy()\n",
    "    log_params = {}\n",
    "\n",
    "    for feature in tqdm(features, desc=\"Log transforming\"):\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Handle negative values: log(|x|) * sign(x)\n",
    "        data = df[feature]\n",
    "        df_log[feature] = np.log1p(np.abs(data)) * np.sign(data)\n",
    "        log_params[feature] = 'applied'\n",
    "\n",
    "    return df_log, log_params\n",
    "\n",
    "\n",
    "def handle_cumulative_features(df, ticker_col='ticker'):\n",
    "    \"\"\"Convert cumulative features to differences\"\"\"\n",
    "    df_diff = df.copy()\n",
    "    cumulative_features = ['PVT_cumsum', 'MOBV']\n",
    "\n",
    "    for feature in cumulative_features:\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Calculate percentage change per ticker\n",
    "        df_diff[f'{feature}_pct'] = df.groupby(ticker_col)[feature].pct_change()\n",
    "        # Fill first NaN with 0\n",
    "        df_diff[f'{feature}_pct'] = df_diff[f'{feature}_pct'].fillna(0)\n",
    "\n",
    "        # Replace inf with 0\n",
    "        df_diff[f'{feature}_pct'] = df_diff[f'{feature}_pct'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df_diff\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. INITIAL ANALYSIS\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š ENHANCED SCALING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStep 1: Analyzing original distribution...\")\n",
    "\n",
    "# Assuming df_features is already loaded\n",
    "# If not, uncomment and load:\n",
    "# df_features = pd.read_csv('data/processed/data.csv')\n",
    "\n",
    "before_stats = {}\n",
    "for feature in tqdm(feature_columns, desc=\"Analyzing features\"):\n",
    "    before_stats[feature] = analyze_distribution(df_features, feature)\n",
    "\n",
    "df_before = pd.DataFrame(before_stats).T\n",
    "df_before['scaling_method'] = df_before.index.map(\n",
    "    lambda x: 'none' if x in no_need_scaling\n",
    "    else 'robust' if x in robust_scaling_features\n",
    "    else 'zscore' if x in zscore_features\n",
    "    else 'standard'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” OUTLIER ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ”¥ Features with HIGH OUTLIERS (>2% outliers):\")\n",
    "high_outliers = df_before[df_before['outlier_pct'] > 2.0].sort_values('outlier_pct', ascending=False)\n",
    "if len(high_outliers) > 0:\n",
    "    print(high_outliers[['outlier_pct', 'skewness', 'min', 'max', 'scaling_method']])\n",
    "else:\n",
    "    print(\"None found!\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Features with EXTREME SKEWNESS (|skew| > 5):\")\n",
    "extreme_skew = df_before[abs(df_before['skewness']) > 5].sort_values('skewness', key=abs, ascending=False)\n",
    "if len(extreme_skew) > 0:\n",
    "    print(extreme_skew[['skewness', 'kurtosis', 'outlier_pct', 'scaling_method']])\n",
    "else:\n",
    "    print(\"None found!\")\n",
    "\n",
    "print(\"\\nðŸ’¥ Features with EXTREME VALUES:\")\n",
    "extreme_vals = df_before[(df_before['max'] > 100) | (df_before['min'] < -10)]\n",
    "if len(extreme_vals) > 0:\n",
    "    print(extreme_vals[['min', 'max', 'mean', 'std', 'outlier_pct', 'scaling_method']])\n",
    "else:\n",
    "    print(\"None found!\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. PREPARE THREE VERSIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”§ PREPARING THREE VERSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# VERSION 1: Basic Winsorization (1% - 99%)\n",
    "print(\"\\n[V1] Basic Winsorization (1%-99%)...\")\n",
    "features_to_winsorize_v1 = [\n",
    "    'daily_return', 'MA_60_slope', 'MTM', 'PVT_cumsum', 'MOBV',\n",
    "    'recent_high_20', 'volatility_20', 'parkinson_volatility',\n",
    "    'high_low_ratio', 'distance_from_high', 'low_to_close_ratio',\n",
    "    'downside_deviation_10', 'VHF'\n",
    "]\n",
    "df_v1, params_v1 = apply_winsorization(df_features, features_to_winsorize_v1, 0.01, 0.99)\n",
    "\n",
    "print(\"\\nWinsorization parameters (V1):\")\n",
    "for feat, vals in params_v1.items():\n",
    "    print(f\"  {feat:25s}: [{vals['lower']:>12.4f}, {vals['upper']:>12.4f}]\")\n",
    "\n",
    "\n",
    "# VERSION 2: Aggressive Winsorization (0.5% - 99.5%)\n",
    "print(\"\\n[V2] Aggressive Winsorization (0.5%-99.5%)...\")\n",
    "features_to_winsorize_v2 = features_to_winsorize_v1.copy()\n",
    "df_v2, params_v2 = apply_winsorization(df_features, features_to_winsorize_v2, 0.005, 0.995)\n",
    "\n",
    "print(\"\\nWinsorization parameters (V2):\")\n",
    "for feat, vals in params_v2.items():\n",
    "    print(f\"  {feat:25s}: [{vals['lower']:>12.4f}, {vals['upper']:>12.4f}]\")\n",
    "\n",
    "\n",
    "# VERSION 3: Hybrid (Winsor + Log + Cumulative handling)\n",
    "print(\"\\n[V3] Hybrid Approach (Winsor + Log Transform + Cumulative Diff)...\")\n",
    "\n",
    "# Step 1: Handle cumulative features first\n",
    "if 'ticker' in df_features.columns:\n",
    "    print(\"  â†’ Converting cumulative features to percentage change...\")\n",
    "    df_v3 = handle_cumulative_features(df_features, 'ticker')\n",
    "\n",
    "    # Replace original cumulative features with their diff versions\n",
    "    if 'PVT_cumsum_pct' in df_v3.columns:\n",
    "        df_v3['PVT_cumsum'] = df_v3['PVT_cumsum_pct']\n",
    "        robust_scaling_features_v3 = [f for f in robust_scaling_features if f != 'PVT_cumsum']\n",
    "        robust_scaling_features_v3.append('PVT_cumsum')\n",
    "\n",
    "    if 'MOBV_pct' in df_v3.columns:\n",
    "        df_v3['MOBV'] = df_v3['MOBV_pct']\n",
    "        robust_scaling_features_v3 = [f for f in robust_scaling_features_v3 if f != 'MOBV']\n",
    "        robust_scaling_features_v3.append('MOBV')\n",
    "else:\n",
    "    df_v3 = df_features.copy()\n",
    "    robust_scaling_features_v3 = robust_scaling_features.copy()\n",
    "\n",
    "# Step 2: Log transform for highly skewed features (skew > 10)\n",
    "highly_skewed = df_before[abs(df_before['skewness']) > 10].index.tolist()\n",
    "features_to_log = [f for f in highly_skewed if f in ['high_low_ratio', 'volatility_20',\n",
    "                                                       'parkinson_volatility', 'recent_high_20']]\n",
    "if features_to_log:\n",
    "    print(f\"  â†’ Log transforming: {features_to_log}\")\n",
    "    df_v3, log_params = apply_log_transform(df_v3, features_to_log)\n",
    "\n",
    "# Step 3: Aggressive winsorization\n",
    "features_to_winsorize_v3 = [f for f in features_to_winsorize_v2 if f not in features_to_log]\n",
    "df_v3, params_v3 = apply_winsorization(df_v3, features_to_winsorize_v3, 0.005, 0.995)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. APPLY SCALING TO ALL VERSIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš™ï¸ APPLYING SCALING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def apply_scaling(df, robust_feats, zscore_feats, standard_feats):\n",
    "    \"\"\"Apply appropriate scaling to features\"\"\"\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    # RobustScaler\n",
    "    if robust_feats:\n",
    "        robust_scaler = RobustScaler()\n",
    "        valid_robust = [f for f in robust_feats if f in df.columns]\n",
    "        if valid_robust:\n",
    "            df_scaled[valid_robust] = robust_scaler.fit_transform(df[valid_robust])\n",
    "\n",
    "    # Z-Score (StandardScaler)\n",
    "    if zscore_feats:\n",
    "        z_scaler = ZScoreScaler()\n",
    "        valid_zscore = [f for f in zscore_feats if f in df.columns]\n",
    "        if valid_zscore:\n",
    "            df_scaled[valid_zscore] = z_scaler.fit_transform(df[valid_zscore])\n",
    "\n",
    "    # StandardScaler\n",
    "    if standard_feats:\n",
    "        std_scaler = StandardScaler()\n",
    "        valid_std = [f for f in standard_feats if f in df.columns]\n",
    "        if valid_std:\n",
    "            df_scaled[valid_std] = std_scaler.fit_transform(df[valid_std])\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "print(\"\\n[V1] Scaling...\")\n",
    "df_scaled_v1 = apply_scaling(df_v1, robust_scaling_features, zscore_features, standard_scaler_features)\n",
    "\n",
    "print(\"[V2] Scaling...\")\n",
    "df_scaled_v2 = apply_scaling(df_v2, robust_scaling_features, zscore_features, standard_scaler_features)\n",
    "\n",
    "print(\"[V3] Scaling...\")\n",
    "df_scaled_v3 = apply_scaling(df_v3, robust_scaling_features_v3, zscore_features, standard_scaler_features)\n",
    "\n",
    "print(\"âœ“ All versions scaled!\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. ANALYSIS AFTER SCALING\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ANALYSIS AFTER SCALING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "after_stats_v1 = {}\n",
    "after_stats_v2 = {}\n",
    "after_stats_v3 = {}\n",
    "\n",
    "for feature in tqdm(feature_columns, desc=\"Analyzing scaled features\"):\n",
    "    if feature in df_scaled_v1.columns:\n",
    "        after_stats_v1[feature] = analyze_distribution(df_scaled_v1, feature)\n",
    "    if feature in df_scaled_v2.columns:\n",
    "        after_stats_v2[feature] = analyze_distribution(df_scaled_v2, feature)\n",
    "    if feature in df_scaled_v3.columns:\n",
    "        after_stats_v3[feature] = analyze_distribution(df_scaled_v3, feature)\n",
    "\n",
    "df_after_v1 = pd.DataFrame(after_stats_v1).T\n",
    "df_after_v2 = pd.DataFrame(after_stats_v2).T\n",
    "df_after_v3 = pd.DataFrame(after_stats_v3).T\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. QUALITY METRICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ QUALITY METRICS (Features that should be normalized)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_quality_score(df_after, features_to_check):\n",
    "    \"\"\"Calculate normalization quality score\"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    for feat in features_to_check:\n",
    "        if feat not in df_after.index:\n",
    "            continue\n",
    "\n",
    "        mean = df_after.loc[feat, 'mean']\n",
    "        std = df_after.loc[feat, 'std']\n",
    "\n",
    "        # Score: closer to (0, 1) is better\n",
    "        mean_score = max(0, 1 - abs(mean) * 2)  # Penalty if |mean| > 0.5\n",
    "        std_score = max(0, 1 - abs(std - 1.0) * 2)  # Penalty if std far from 1\n",
    "\n",
    "        total_score = (mean_score + std_score) / 2\n",
    "        scores[feat] = {\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'mean_score': mean_score,\n",
    "            'std_score': std_score,\n",
    "            'total_score': total_score\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(scores).T\n",
    "\n",
    "# Features that should be normalized (exclude 'none' category)\n",
    "features_to_evaluate = [f for f in feature_columns if f not in no_need_scaling]\n",
    "\n",
    "print(\"\\nðŸ“Š VERSION 1 (Basic Winsor):\")\n",
    "quality_v1 = calculate_quality_score(df_after_v1, features_to_evaluate)\n",
    "print(f\"Average Score: {quality_v1['total_score'].mean():.3f}\")\n",
    "print(\"\\nWorst performers:\")\n",
    "print(quality_v1.nsmallest(5, 'total_score')[['mean', 'std', 'total_score']])\n",
    "\n",
    "print(\"\\nðŸ“Š VERSION 2 (Aggressive Winsor):\")\n",
    "quality_v2 = calculate_quality_score(df_after_v2, features_to_evaluate)\n",
    "print(f\"Average Score: {quality_v2['total_score'].mean():.3f}\")\n",
    "print(\"\\nWorst performers:\")\n",
    "print(quality_v2.nsmallest(5, 'total_score')[['mean', 'std', 'total_score']])\n",
    "\n",
    "print(\"\\nðŸ“Š VERSION 3 (Hybrid):\")\n",
    "quality_v3 = calculate_quality_score(df_after_v3, features_to_evaluate)\n",
    "print(f\"Average Score: {quality_v3['total_score'].mean():.3f}\")\n",
    "print(\"\\nWorst performers:\")\n",
    "print(quality_v3.nsmallest(5, 'total_score')[['mean', 'std', 'total_score']])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. COMPARISON TABLE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ DETAILED COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'before_mean': df_before['mean'],\n",
    "    'before_std': df_before['std'],\n",
    "    'before_skew': df_before['skewness'],\n",
    "    'before_outliers': df_before['outlier_pct'],\n",
    "\n",
    "    'v1_mean': df_after_v1['mean'],\n",
    "    'v1_std': df_after_v1['std'],\n",
    "    'v1_skew': df_after_v1['skewness'],\n",
    "\n",
    "    'v2_mean': df_after_v2['mean'],\n",
    "    'v2_std': df_after_v2['std'],\n",
    "    'v2_skew': df_after_v2['skewness'],\n",
    "\n",
    "    'v3_mean': df_after_v3['mean'],\n",
    "    'v3_std': df_after_v3['std'],\n",
    "    'v3_skew': df_after_v3['skewness'],\n",
    "\n",
    "    'method': df_before['scaling_method']\n",
    "})\n",
    "\n",
    "# Find features that improved most\n",
    "comparison['v1_improvement'] = abs(comparison['before_std'] - 1.0) - abs(comparison['v1_std'] - 1.0)\n",
    "comparison['v2_improvement'] = abs(comparison['before_std'] - 1.0) - abs(comparison['v2_std'] - 1.0)\n",
    "comparison['v3_improvement'] = abs(comparison['before_std'] - 1.0) - abs(comparison['v3_std'] - 1.0)\n",
    "\n",
    "print(\"\\nâœ… Features with BEST improvement (V3 vs V1):\")\n",
    "improved = comparison.sort_values('v3_improvement', ascending=False).head(10)\n",
    "print(improved[['before_std', 'v1_std', 'v2_std', 'v3_std', 'method']])\n",
    "\n",
    "print(\"\\nâš ï¸ Features that still need work (worst in V3):\")\n",
    "needs_work = comparison[~comparison.index.isin(no_need_scaling)]\n",
    "needs_work['v3_quality'] = abs(needs_work['v3_mean']) + abs(needs_work['v3_std'] - 1.0)\n",
    "worst = needs_work.sort_values('v3_quality', ascending=False).head(10)\n",
    "print(worst[['v3_mean', 'v3_std', 'v3_skew', 'method']])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. SAVE RESULTS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ’¾ SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_file = \"enhanced_scaling_analysis_report.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    df_before.to_excel(writer, sheet_name='00_Before_Scaling')\n",
    "    df_after_v1.to_excel(writer, sheet_name='01_V1_BasicWinsor')\n",
    "    df_after_v2.to_excel(writer, sheet_name='02_V2_AggressiveWinsor')\n",
    "    df_after_v3.to_excel(writer, sheet_name='03_V3_Hybrid')\n",
    "    comparison.to_excel(writer, sheet_name='04_Comparison')\n",
    "    quality_v1.to_excel(writer, sheet_name='05_Quality_V1')\n",
    "    quality_v2.to_excel(writer, sheet_name='06_Quality_V2')\n",
    "    quality_v3.to_excel(writer, sheet_name='07_Quality_V3')\n",
    "\n",
    "print(f\"âœ“ Report saved: {output_file}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. VISUALIZATIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“¸ GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "IMAGE_DIR = \"Images_enhanced_scaling\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "for feature in tqdm(feature_columns, desc=\"Creating plots\"):\n",
    "    if feature not in df_features.columns:\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "    # Determine scaling method\n",
    "    if feature in robust_scaling_features:\n",
    "        method = 'RobustScaler'\n",
    "    elif feature in zscore_features:\n",
    "        method = 'StandardScaler (Z-Score)'\n",
    "    elif feature in standard_scaler_features:\n",
    "        method = 'StandardScaler'\n",
    "    else:\n",
    "        method = 'No Scaling'\n",
    "\n",
    "    fig.suptitle(f'{feature} - {method}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Original\n",
    "    axes[0, 0].hist(df_features[feature].dropna(), bins=100, alpha=0.7,\n",
    "                    color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('ORIGINAL', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    mean_orig = df_features[feature].mean()\n",
    "    std_orig = df_features[feature].std()\n",
    "    skew_orig = stats.skew(df_features[feature].dropna())\n",
    "    axes[0, 0].text(0.02, 0.98,\n",
    "                    f'Mean: {mean_orig:.3f}\\nStd: {std_orig:.3f}\\nSkew: {skew_orig:.2f}',\n",
    "                    transform=axes[0, 0].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    # 2. V1 (Basic Winsor)\n",
    "    if feature in df_scaled_v1.columns:\n",
    "        axes[0, 1].hist(df_scaled_v1[feature].dropna(), bins=100, alpha=0.7,\n",
    "                        color='lightgreen', edgecolor='black')\n",
    "        axes[0, 1].set_title('V1: Basic Winsor (1%-99%)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Value')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        mean_v1 = df_scaled_v1[feature].mean()\n",
    "        std_v1 = df_scaled_v1[feature].std()\n",
    "        skew_v1 = stats.skew(df_scaled_v1[feature].dropna())\n",
    "        axes[0, 1].text(0.02, 0.98,\n",
    "                        f'Mean: {mean_v1:.3f}\\nStd: {std_v1:.3f}\\nSkew: {skew_v1:.2f}',\n",
    "                        transform=axes[0, 1].transAxes, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "    # 3. V2 (Aggressive Winsor)\n",
    "    if feature in df_scaled_v2.columns:\n",
    "        axes[1, 0].hist(df_scaled_v2[feature].dropna(), bins=100, alpha=0.7,\n",
    "                        color='orange', edgecolor='black')\n",
    "        axes[1, 0].set_title('V2: Aggressive Winsor (0.5%-99.5%)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Value')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        mean_v2 = df_scaled_v2[feature].mean()\n",
    "        std_v2 = df_scaled_v2[feature].std()\n",
    "        skew_v2 = stats.skew(df_scaled_v2[feature].dropna())\n",
    "        axes[1, 0].text(0.02, 0.98,\n",
    "                        f'Mean: {mean_v2:.3f}\\nStd: {std_v2:.3f}\\nSkew: {skew_v2:.2f}',\n",
    "                        transform=axes[1, 0].transAxes, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='orange', alpha=0.5))\n",
    "\n",
    "    # 4. V3 (Hybrid)\n",
    "    if feature in df_scaled_v3.columns:\n",
    "        axes[1, 1].hist(df_scaled_v3[feature].dropna(), bins=100, alpha=0.7,\n",
    "                        color='purple', edgecolor='black')\n",
    "        axes[1, 1].set_title('V3: Hybrid (Winsor+Log+Diff)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Value')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        mean_v3 = df_scaled_v3[feature].mean()\n",
    "        std_v3 = df_scaled_v3[feature].std()\n",
    "        skew_v3 = stats.skew(df_scaled_v3[feature].dropna())\n",
    "        axes[1, 1].text(0.02, 0.98,\n",
    "                        f'Mean: {mean_v3:.3f}\\nStd: {std_v3:.3f}\\nSkew: {skew_v3:.2f}',\n",
    "                        transform=axes[1, 1].transAxes, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='purple', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'{feature}_comparison.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"âœ“ Saved {len(feature_columns)} visualizations to: {IMAGE_DIR}/\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. FINAL RECOMMENDATIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "SUMMARY OF RESULTS:\n",
    "==================\n",
    "âœ“ Version 1 (Basic Winsor):      Average Quality Score = {quality_v1['total_score'].mean():.3f}\n",
    "âœ“ Version 2 (Aggressive Winsor): Average Quality Score = {quality_v2['total_score'].mean():.3f}\n",
    "âœ“ Version 3 (Hybrid):            Average Quality Score = {quality_v3['total_score'].mean():.3f}\n",
    "\n",
    "BEST APPROACH: {'V3 (Hybrid)' if quality_v3['total_score'].mean() > max(quality_v1['total_score'].mean(), quality_v2['total_score'].mean()) else 'V2 (Aggressive Winsor)' if quality_v2['total_score'].mean() > quality_v1['total_score'].mean() else 'V1 (Basic Winsor)'}\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Review the Excel report: {output_file}\n",
    "2. Check visualizations in: {IMAGE_DIR}/\n",
    "3. Focus on features in \"needs work\" list\n",
    "4. Consider additional transformations for worst performers\n",
    "\n",
    "KEY INSIGHTS:\n",
    "- Cumulative features (PVT_cumsum, MOBV) benefit from percentage change conversion\n",
    "- Highly skewed features (skew > 10) benefit from log transformation\n",
    "- Aggressive winsorization (0.5%-99.5%) improves most features\n",
    "- Some features may need custom handling based on domain knowledge\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ… ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ],
   "id": "f833ad515343a7cb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features_scaled.describe()",
   "id": "ac79b62b439c3d7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "cb8e9af69968a3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "61c369a7c5beb523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "cd34fedd16ffb2a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "3db3374b37d5161f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 5000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 â­â­â­\n",
    "    'MOBV',                 # MI = 0.0209 â­â­\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 â­\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ac1312d7bbf04afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA LOADING AND INITIAL PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1] LOADING DATA...\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/interim/train_clean_after_2010_and_bad_tickers.csv')\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['open'] != 0]\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "5cacce17fbd4b72f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] ENGINEERING FEATURES...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features and keep only selected high-quality features\n",
    "    in the order they were originally created.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    print(\" - Target variable...\")\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1/(4*np.log(2)) *\n",
    "            ((np.log(x['high']/(x['low']+1e-8)))**2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "        (df['close'] - df['recent_low_20']) /\n",
    "        (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                  np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    temp_cols = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20','close_30d_future',\n",
    "    ]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "    # df = df[ ['ticker', 'date'] + feature_columns_order ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_features(df)\n",
    "\n",
    "print(\"\\nâœ“ Feature engineering complete!\")\n",
    "print(f\"Total features created: 25\")\n",
    "print(f\"Rows with complete features: {df_features.dropna().shape[0]:,}\")"
   ],
   "id": "4177fe98c9aaf3a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_features.describe()",
   "id": "ef062214d42f8b5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "missing_summary = (\n",
    "    df_features.isna()\n",
    "      .sum()\n",
    "      .to_frame(name='missing_count')\n",
    "      .assign(missing_pct=lambda x: x['missing_count'] / len(df) * 100)\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .reset_index(names='column')\n",
    ")\n",
    "\n",
    "print(missing_summary)"
   ],
   "id": "e51fd72a1a8f1763"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: DATA QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] DATA QUALITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "# ============================================================\n",
    "feature_columns = [\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 â­â­â­\n",
    "    'MOBV',                 # MI = 0.0209 â­â­\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 â­\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features\n",
    "\n",
    "]\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values in engineered features:\")\n",
    "missing_stats = df_features[feature_columns].isnull().sum()\n",
    "missing_pct = (missing_stats / len(df_features) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_stats,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df_features[feature_columns].describe())\n",
    "\n",
    "IMAGE_DIR = \"Images2\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Visualization: Distribution of features\n",
    "for feature in feature_columns:\n",
    "    data = df_features[feature].dropna()\n",
    "\n",
    "    # Remove extreme outliers for better visualization (keep 1st-99th percentile)\n",
    "    q1, q99 = data.quantile([0.01, 0.99])\n",
    "    data_filtered = data[(data >= q1) & (data <= q99)]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))  # wider figure\n",
    "    plt.hist(data_filtered, bins=100, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'{feature} Distribution\\n(1st-99th percentile)', fontsize=14)\n",
    "    plt.xlabel('Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(IMAGE_DIR, f'feature_{feature}.png'),\n",
    "        dpi=300,\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ],
   "id": "c24ea41df1ba4346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: AUTOCORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing how each feature correlates with its past values\")\n",
    "print(\"This helps determine optimal sequence length for RNN\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 200    # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60             # Analyze up to 60 days lag\n",
    "threshold = 0.05\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = (\n",
    "    df_features['ticker']\n",
    "    .dropna()\n",
    "    .unique()[:NUM_TICKERS_TO_USE]\n",
    ")\n",
    "\n",
    "df_sample = (\n",
    "    df_features\n",
    "    .loc[df_features['ticker'].isin(selected_tickers)]\n",
    "    .dropna(subset=feature_columns)\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTOCORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "autocorr_results = {}\n",
    "# feature_columns = [\n",
    "#     'daily_return', 'high_low_ratio', 'return_30',\n",
    "#     #'MA_5', 'MA_10', 'MA_30', 'STD_10',\n",
    "#     'log_volume', 'volume_ratio'\n",
    "#     #, 'dividend_yield'\n",
    "# ]\n",
    "\n",
    "for feature in feature_columns:\n",
    "    print(f\"\\nAnalyzing autocorrelation: {feature}\")\n",
    "\n",
    "    per_ticker_acfs = []\n",
    "\n",
    "    for ticker, g in df_sample.groupby('ticker'):\n",
    "        data = g[feature].dropna()\n",
    "\n",
    "        if len(data) <= max_lags:\n",
    "            continue\n",
    "\n",
    "        autocorr_values = acf(data, nlags=max_lags, fft=True)\n",
    "        per_ticker_acfs.append(autocorr_values)\n",
    "\n",
    "    if len(per_ticker_acfs) == 0:\n",
    "        print(\"  Not enough data\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate across tickers (median preserves typical temporal behavior)\n",
    "    autocorr_values = np.median(per_ticker_acfs, axis=0)\n",
    "    autocorr_results[feature] = autocorr_values\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT (single plot per feature)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    lags = np.arange(len(autocorr_values))\n",
    "\n",
    "    plt.bar(lags, autocorr_values, width=0.8, alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', linewidth=1, label='Threshold (0.05)')\n",
    "    plt.axhline(y=-threshold, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.title(f'Autocorrelation: {feature}', fontsize=11, fontweight='bold')\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, f'autocorrelation_{feature}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND OPTIMAL LAG\n",
    "    # ----------------------------------------------------------------------------\n",
    "    significant_lags = np.where(np.abs(autocorr_values) > threshold)[0]\n",
    "\n",
    "    if len(significant_lags) > 1:\n",
    "        optimal_lag = significant_lags[-1]\n",
    "        print(f\"  Optimal lag: {optimal_lag} days (autocorr = {autocorr_values[optimal_lag]:.4f})\")\n",
    "    else:\n",
    "        print(f\"  low autocorrelation (independent)\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE OF AUTOCORRELATION DECAY\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOCORRELATION DECAY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "decay_summary = []\n",
    "\n",
    "for feature, autocorr_vals in autocorr_results.items():\n",
    "    below_threshold = np.where(np.abs(autocorr_vals[1:]) < threshold)[0]\n",
    "\n",
    "    if len(below_threshold) > 0:\n",
    "        decay_lag = below_threshold[0] + 1\n",
    "    else:\n",
    "        decay_lag = max_lags\n",
    "\n",
    "    decay_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Lag_10': autocorr_vals[10],\n",
    "        'Lag_20': autocorr_vals[20],\n",
    "        'Lag_30': autocorr_vals[30],\n",
    "        'Decay_Point': decay_lag\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_summary)\n",
    "print(decay_df.to_string(index=False))\n"
   ],
   "id": "46b315725a20227c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TARGET-LAG CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] TARGET-LAG CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing correlation between lagged features and target variable\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match SECTION 4 for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG CORRELATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_lag_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing target correlation: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    correlations_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute correlation per ticker\n",
    "        per_ticker_corrs = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need at least 30 samples\n",
    "                corr = ticker_data.loc[valid_mask, 'target'].corr(lagged_feature[valid_mask])\n",
    "                per_ticker_corrs.append(corr)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_corrs) > 0:\n",
    "            correlations_per_lag.append(np.median(per_ticker_corrs))\n",
    "        else:\n",
    "            correlations_per_lag.append(0)\n",
    "\n",
    "    target_lag_results[feature] = correlations_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), correlations_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[plot_idx_in_figure].set_title(f'Target Correlation: {feature}', fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('Correlation with Target')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK CORRELATION\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_corr_idx = np.argmax(np.abs(correlations_per_lag))\n",
    "    max_corr = correlations_per_lag[max_corr_idx]\n",
    "    print(f\"  Peak correlation: {max_corr:.4f} at lag {max_corr_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'03_target_lag_correlation_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "f9e8fcac0ffa929d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TARGET-LAG MUTUAL INFORMATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6] TARGET-LAG MUTUAL INFORMATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing mutual information between lagged features and binary target\")\n",
    "\n",
    "feature_columns = [\n",
    "    # Raw Features\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\"\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50  # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS ARE USED\n",
    "max_lags = 60            # Should match previous sections for consistency\n",
    "PLOTS_PER_FIGURE = 12    # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA (KEEP TEMPORAL ORDER)\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# TARGET-LAG MUTUAL INFORMATION COMPUTATION\n",
    "# ----------------------------------------------------------------------------\n",
    "target_mi_results = {}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"\\nAnalyzing MI with target: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    mi_scores_per_lag = []\n",
    "\n",
    "    # For each lag\n",
    "    for lag in range(1, max_lags + 1):\n",
    "\n",
    "        # Compute MI per ticker\n",
    "        per_ticker_mi = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            lagged_feature = ticker_data[feature].shift(lag)\n",
    "            valid_mask = ticker_data['target'].notna() & lagged_feature.notna()\n",
    "\n",
    "            if valid_mask.sum() > 30:  # Need sufficient samples per ticker\n",
    "                X_lag = lagged_feature[valid_mask].values.reshape(-1, 1)\n",
    "                y_lag = ticker_data.loc[valid_mask, 'target'].values\n",
    "\n",
    "                # Calculate MI for this ticker\n",
    "                mi = mutual_info_classif(X_lag, y_lag,\n",
    "                                        discrete_features=False,\n",
    "                                        n_neighbors=3,\n",
    "                                        random_state=42)[0]\n",
    "                per_ticker_mi.append(mi)\n",
    "\n",
    "        # Aggregate across tickers (median is robust)\n",
    "        if len(per_ticker_mi) > 0:\n",
    "            mi_scores_per_lag.append(np.median(per_ticker_mi))\n",
    "        else:\n",
    "            mi_scores_per_lag.append(0)\n",
    "\n",
    "    target_mi_results[feature] = mi_scores_per_lag\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # PLOT\n",
    "    # ----------------------------------------------------------------------------\n",
    "    axes[plot_idx_in_figure].plot(range(1, max_lags + 1), mi_scores_per_lag,\n",
    "                   marker='o', markersize=3, linewidth=1.5, color='darkblue')\n",
    "    axes[plot_idx_in_figure].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "    # Add threshold line (optional - 0.01 is a reasonable baseline)\n",
    "    axes[plot_idx_in_figure].axhline(y=0.01, color='red', linestyle='--',\n",
    "                     linewidth=1, alpha=0.5, label='Threshold')\n",
    "\n",
    "    axes[plot_idx_in_figure].set_title(f'Mutual Information: {feature}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[plot_idx_in_figure].set_xlabel('Lag (days)')\n",
    "    axes[plot_idx_in_figure].set_ylabel('MI Score')\n",
    "    axes[plot_idx_in_figure].grid(True, alpha=0.3)\n",
    "    axes[plot_idx_in_figure].set_ylim(bottom=0)  # MI is always non-negative\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # FIND PEAK MI\n",
    "    # ----------------------------------------------------------------------------\n",
    "    max_mi_idx = np.argmax(mi_scores_per_lag)\n",
    "    max_mi = mi_scores_per_lag[max_mi_idx]\n",
    "    print(f\"  Peak MI: {max_mi:.4f} at lag {max_mi_idx + 1}\")\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'04_target_lag_mutual_information_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Mutual Information analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: COMPARISON PLOT - CORRELATION vs MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BONUS: CORRELATION vs MUTUAL INFORMATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison plot for top features by MI (select top 12)\n",
    "# Sort features by max MI score\n",
    "feature_max_mi = {feat: max(target_mi_results[feat])\n",
    "                  for feat in feature_columns if feat in target_mi_results}\n",
    "top_features = sorted(feature_max_mi.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "comparison_features = [feat for feat, _ in top_features]\n",
    "\n",
    "# Calculate number of comparison figures needed\n",
    "num_comp_plots = len(comparison_features)\n",
    "num_comp_figures = int(np.ceil(num_comp_plots / PLOTS_PER_FIGURE))\n",
    "\n",
    "for comp_fig_idx in range(num_comp_figures):\n",
    "    start_idx = comp_fig_idx * PLOTS_PER_FIGURE\n",
    "    end_idx = min(start_idx + PLOTS_PER_FIGURE, num_comp_plots)\n",
    "    features_in_figure = comparison_features[start_idx:end_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features_in_figure):\n",
    "        if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "            # Create twin axis\n",
    "            ax1 = axes[idx]\n",
    "            ax2 = ax1.twinx()\n",
    "\n",
    "            # Plot correlation\n",
    "            lags = range(1, max_lags + 1)\n",
    "            line1 = ax1.plot(lags, target_lag_results[feature],\n",
    "                            color='blue', marker='o', markersize=2,\n",
    "                            linewidth=1.5, label='Correlation', alpha=0.7)\n",
    "            ax1.axhline(y=0, color='blue', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "            ax1.set_xlabel('Lag (days)', fontsize=10)\n",
    "            ax1.set_ylabel('Correlation', color='blue', fontsize=10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Plot MI\n",
    "            line2 = ax2.plot(lags, target_mi_results[feature],\n",
    "                            color='red', marker='s', markersize=2,\n",
    "                            linewidth=1.5, label='Mutual Information', alpha=0.7)\n",
    "            ax2.set_ylabel('Mutual Information', color='red', fontsize=10)\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(bottom=0)\n",
    "\n",
    "            # Title and legend\n",
    "            ax1.set_title(f'{feature}: Correlation vs MI',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Combined legend\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for unused_idx in range(len(features_in_figure), PLOTS_PER_FIGURE):\n",
    "        axes[unused_idx].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = f'05_correlation_vs_MI_comparison_part{comp_fig_idx+1}.png'\n",
    "    plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Saved: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TOP PREDICTIVE LAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature in target_lag_results and feature in target_mi_results:\n",
    "\n",
    "        # Best correlation\n",
    "        corr_values = target_lag_results[feature]\n",
    "        best_corr_idx = np.argmax(np.abs(corr_values))\n",
    "        best_corr = corr_values[best_corr_idx]\n",
    "\n",
    "        # Best MI\n",
    "        mi_values = target_mi_results[feature]\n",
    "        best_mi_idx = np.argmax(mi_values)\n",
    "        best_mi = mi_values[best_mi_idx]\n",
    "\n",
    "        summary_data.append({\n",
    "            'Feature': feature,\n",
    "            'Best_Corr': best_corr,\n",
    "            'Best_Corr_Lag': best_corr_idx + 1,\n",
    "            'Best_MI': best_mi,\n",
    "            'Best_MI_Lag': best_mi_idx + 1\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best_MI', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features by Mutual Information:\")\n",
    "print(summary_df.head(20).to_string(index=False))  # Show top 20\n",
    "print(f\"\\n... and {len(summary_df) - 20} more features\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('target_lag_analysis_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: target_lag_analysis_summary.csv\")"
   ],
   "id": "f8a5745e7bf6e107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: ROLLING STATISTICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7] ROLLING STATISTICS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Analyzing stability of features across different window sizes\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "NUM_TICKERS_TO_USE = 50   # <<< CHANGE THIS TO CONTROL HOW MANY TICKERS TO INCLUDE\n",
    "windows = [5, 10, 15, 20, 30, 45, 60]\n",
    "PLOTS_PER_FIGURE = 12     # Number of subplots per figure (4x3 grid)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "df_features = df_features.sort_values(['ticker', 'date'])\n",
    "selected_tickers = df_features['ticker'].dropna().unique()[:NUM_TICKERS_TO_USE]\n",
    "df_sample = df_features.loc[df_features['ticker'].isin(selected_tickers)].copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CALCULATE ROLLING STATISTICS\n",
    "# ----------------------------------------------------------------------------\n",
    "rolling_stats_results = {feature: {'windows': [], 'mean_std': [], 'std_std': []}\n",
    "                         for feature in feature_columns}\n",
    "\n",
    "# Calculate how many figures we need\n",
    "num_features = len(feature_columns)\n",
    "num_figures = int(np.ceil(num_features / PLOTS_PER_FIGURE))\n",
    "\n",
    "print(f\"\\nTotal features: {num_features}\")\n",
    "print(f\"Plots per figure: {PLOTS_PER_FIGURE}\")\n",
    "print(f\"Number of figures to create: {num_figures}\")\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    print(f\"Analyzing rolling stats: {feature} ({feature_idx+1}/{num_features})\")\n",
    "\n",
    "    for window in windows:\n",
    "\n",
    "        per_ticker_mean_std = []\n",
    "        per_ticker_std_std = []\n",
    "\n",
    "        for ticker, g in df_sample.groupby('ticker'):\n",
    "            ticker_data = g.sort_values('date').reset_index(drop=True)\n",
    "            rolling_mean = ticker_data[feature].rolling(window=window).mean()\n",
    "            rolling_std = ticker_data[feature].rolling(window=window).std()\n",
    "\n",
    "            mean_stability = rolling_mean.std()\n",
    "            std_stability = rolling_std.std()\n",
    "\n",
    "            per_ticker_mean_std.append(mean_stability)\n",
    "            per_ticker_std_std.append(std_stability)\n",
    "\n",
    "        # Aggregate across tickers (median)\n",
    "        rolling_stats_results[feature]['windows'].append(window)\n",
    "        rolling_stats_results[feature]['mean_std'].append(np.median(per_ticker_mean_std))\n",
    "        rolling_stats_results[feature]['std_std'].append(np.median(per_ticker_std_std))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PLOT\n",
    "# ----------------------------------------------------------------------------\n",
    "# Initialize first figure\n",
    "fig_idx = 0\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "plot_idx_in_figure = 0\n",
    "\n",
    "for feature_idx, feature in enumerate(feature_columns):\n",
    "    ax = axes[plot_idx_in_figure]\n",
    "\n",
    "    windows_list = rolling_stats_results[feature]['windows']\n",
    "    mean_std_list = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_list = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    line1 = ax.plot(windows_list, mean_std_list, 'b-o', label='Rolling Mean Std', linewidth=2)\n",
    "    line2 = ax2.plot(windows_list, std_std_list, 'r-s', label='Rolling Std Std', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Window Size (days)')\n",
    "    ax.set_ylabel('Std of Rolling Mean', color='b')\n",
    "    ax2.set_ylabel('Std of Rolling Std', color='r')\n",
    "    ax.set_title(f'Rolling Statistics: {feature}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=8)\n",
    "\n",
    "    plot_idx_in_figure += 1\n",
    "\n",
    "    # Check if we need to save current figure and start a new one\n",
    "    if plot_idx_in_figure == PLOTS_PER_FIGURE or feature_idx == num_features - 1:\n",
    "        # Hide any unused subplots in the current figure\n",
    "        for unused_idx in range(plot_idx_in_figure, PLOTS_PER_FIGURE):\n",
    "            axes[unused_idx].remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename = f'06_rolling_statistics_part{fig_idx+1}.png'\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, filename), dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Start new figure if there are more features to plot\n",
    "        if feature_idx < num_features - 1:\n",
    "            fig_idx += 1\n",
    "            fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "            axes = axes.flatten()\n",
    "            plot_idx_in_figure = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Rolling statistics analysis complete! Created {fig_idx + 1} figure(s)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: SUMMARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: FEATURE STABILITY ACROSS WINDOWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_summary = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    mean_std_values = rolling_stats_results[feature]['mean_std']\n",
    "    std_std_values = rolling_stats_results[feature]['std_std']\n",
    "\n",
    "    # Calculate stability metrics (lower is more stable)\n",
    "    avg_mean_stability = np.mean(mean_std_values)\n",
    "    avg_std_stability = np.mean(std_std_values)\n",
    "\n",
    "    # Calculate how much stability changes across windows (consistency)\n",
    "    mean_stability_variance = np.std(mean_std_values)\n",
    "    std_stability_variance = np.std(std_std_values)\n",
    "\n",
    "    stability_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Avg_Mean_Stability': avg_mean_stability,\n",
    "        'Avg_Std_Stability': avg_std_stability,\n",
    "        'Mean_Stability_Variance': mean_stability_variance,\n",
    "        'Std_Stability_Variance': std_stability_variance\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_summary)\n",
    "stability_df = stability_df.sort_values('Avg_Mean_Stability')\n",
    "\n",
    "print(\"\\nMost Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLeast Stable Features (by Rolling Mean):\")\n",
    "print(stability_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "stability_df.to_csv('rolling_statistics_summary.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: rolling_statistics_summary.csv\")"
   ],
   "id": "bc2220133a071d7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: Correlation between features\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "RANDOM_STATE = 42\n",
    "N_TICKERS_SAMPLE = 1000\n",
    "MIN_ROWS_PER_TICKER = 100\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "# Mask correlations with absolute value <= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAMPLE TICKERS\n",
    "feature_columns = [\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # PREVIOUSLY VALIDATED FEATURES (28 features)\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "    # Price Features (3)\n",
    "    'daily_return',\n",
    "    'high_low_ratio',\n",
    "\n",
    "    # MA-Based (4)\n",
    "    'price_to_MA5',\n",
    "    'price_to_MA20',\n",
    "    'price_to_MA60',\n",
    "    'MA_60_slope',\n",
    "\n",
    "    # Volatility (3)\n",
    "    'volatility_20',\n",
    "    'RSI_14',\n",
    "    'parkinson_volatility',\n",
    "\n",
    "    # Critical Features (4)\n",
    "    'recent_high_20',\n",
    "    'distance_from_high',\n",
    "    'low_to_close_ratio',\n",
    "    'price_position_20',\n",
    "    'max_drawdown_20',\n",
    "    'downside_deviation_10',\n",
    "\n",
    "    # Temporal (3)\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'is_up_day',\n",
    "\n",
    "    # Volume Price Index (3) - Highest MI!\n",
    "    'PVT_cumsum',           # MI = 0.0426 â­â­â­\n",
    "    'MOBV',                 # MI = 0.0209 â­â­\n",
    "\n",
    "    # Directional Movement (4)\n",
    "    'MTM',                  # MI = 0.0127 â­\n",
    "\n",
    "    # OverBought & OverSold (1)\n",
    "    'ADTM',                 # MI = 0.0104\n",
    "\n",
    "    # Energy & Volatility (2)\n",
    "    'PSY',                  # MI = 0.0085\n",
    "    'VHF',                  # MI = 0.0088\n",
    "\n",
    "    # Stochastic (1)\n",
    "    'K',                    # MI = 0.0083\n",
    "\n",
    "    # Raw Features:\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"dividends\",\n",
    "    \"stock_splits\",\n",
    "]\n",
    "# ============================================================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "valid_tickers = (\n",
    "    df_features.groupby('ticker')\n",
    "      .size()\n",
    "      .loc[lambda x: x >= MIN_ROWS_PER_TICKER]\n",
    "      .index\n",
    ")\n",
    "\n",
    "sample_tickers = rng.choice(\n",
    "    valid_tickers,\n",
    "    size=min(N_TICKERS_SAMPLE, len(valid_tickers)),\n",
    "    replace=False\n",
    ")\n",
    "df_sample = df_features[df_features['ticker'].isin(sample_tickers)]\n",
    "\n",
    "print(f\"Using {df_sample['ticker'].nunique()} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# PER-TICKER CORRELATION\n",
    "# ============================================================\n",
    "corr_matrices = []\n",
    "\n",
    "\n",
    "\n",
    "for ticker, g in df_sample.groupby('ticker'):\n",
    "    feature_df = g[feature_columns].dropna()\n",
    "\n",
    "    if len(feature_df) < 30:\n",
    "        continue\n",
    "\n",
    "    corr = feature_df.corr(method='pearson')\n",
    "    corr_matrices.append(corr.values)\n",
    "\n",
    "corr_matrices = np.array(corr_matrices)\n",
    "\n",
    "print(f\"Computed correlations for {corr_matrices.shape[0]} tickers\")\n",
    "\n",
    "# ============================================================\n",
    "# AGGREGATE (MEAN CORRELATION)\n",
    "# ============================================================\n",
    "mean_corr = np.nanmean(corr_matrices, axis=0)\n",
    "\n",
    "mean_corr_df = pd.DataFrame(\n",
    "    mean_corr,\n",
    "    index=feature_columns,\n",
    "    columns=feature_columns\n",
    ")\n",
    "\n",
    "mask = mean_corr_df.abs() <= THRESHOLD\n",
    "np.fill_diagonal(mask.values, True)  # optional: hide diagonal\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAP\n",
    "# ============================================================\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(\n",
    "    mean_corr_df,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Mean Pearson Correlation'},\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Feature Correlations |corr| > {THRESHOLD}\\n\"\n",
    "    f\"({len(sample_tickers)} Randomly Sampled Tickers)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "eac3f8e6a29c7983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6686ea2b553342fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
