{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Load & Clean (Run Once, Do Not Edit)",
   "id": "7bb3d136ed8dc73b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def verify_columns_and_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "        .str.replace(r\"[^\\w]\", \"\", regex=True)\n",
    "        .str.lower()\n",
    "    )\n",
    "    required_cols = [\"date\", \"ticker\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"dividends\", \"stock_splits\"]\n",
    "\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    price_cols = [\"open\", \"high\", \"low\", \"close\"]\n",
    "    for c in price_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df[\"volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\")\n",
    "    df[\"ticker\"] = df[\"ticker\"].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(\n",
    "        df: pd.DataFrame,\n",
    "        price_cols=(\"open\", \"high\", \"low\", \"close\"),\n",
    "        max_na_fraction=0.10,\n",
    ") -> pd.DataFrame:\n",
    "    price_cols = list(price_cols)\n",
    "\n",
    "    removed_tickers = 0\n",
    "    removed_rows = 0\n",
    "\n",
    "    def process_ticker(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        nonlocal removed_tickers, removed_rows\n",
    "\n",
    "        rows_before = len(g)\n",
    "        g = g.sort_values(\"date\")\n",
    "\n",
    "        while not g.empty and g[price_cols].iloc[0].isna().any():\n",
    "            g = g.iloc[1:]\n",
    "\n",
    "        while not g.empty and g[price_cols].iloc[-1].isna().any():\n",
    "            g = g.iloc[:-1]\n",
    "\n",
    "        if g.empty:\n",
    "            removed_tickers += 1\n",
    "            removed_rows += rows_before\n",
    "            return g\n",
    "\n",
    "        na_fraction = g[price_cols].isna().mean().mean()\n",
    "\n",
    "        if na_fraction > max_na_fraction:\n",
    "            removed_tickers += 1\n",
    "            removed_rows += rows_before\n",
    "            return g.iloc[0:0]\n",
    "\n",
    "        g[price_cols] = g[price_cols].ffill().bfill()\n",
    "        removed_rows += (rows_before - len(g))\n",
    "\n",
    "        return g\n",
    "\n",
    "    df_clean = (\n",
    "        df\n",
    "        .groupby(\"ticker\", group_keys=False)\n",
    "        .apply(process_ticker)\n",
    "    )\n",
    "    return df_clean  # TODO : check this\n",
    "\n",
    "\n",
    "def drop_ticker_date_duplicates(\n",
    "        df: pd.DataFrame,\n",
    "        max_duplicates_per_ticker: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    dup_counts = (\n",
    "        df.groupby([\"ticker\", \"date\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"n\")\n",
    "    )\n",
    "    bad_tickers = (\n",
    "        dup_counts[dup_counts[\"n\"] > 1]\n",
    "        .groupby(\"ticker\")[\"n\"]\n",
    "        .sum()\n",
    "    )\n",
    "    bad_tickers = bad_tickers[bad_tickers > max_duplicates_per_ticker].index\n",
    "    df = df[~df[\"ticker\"].isin(bad_tickers)]\n",
    "    df = df.drop_duplicates(subset=[\"ticker\", \"date\"], keep=\"first\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_invalid_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cond_open = df[\"open\"] != 0\n",
    "    cond_close = df[\"close\"] != 0\n",
    "    cond_high_low = df[\"high\"] >= df[\"low\"]\n",
    "    cond_open_range = (df[\"open\"] >= df[\"low\"]) & (df[\"open\"] <= df[\"high\"])\n",
    "    cond_volume = df[\"volume\"] > 0\n",
    "    valid_mask = cond_open & cond_close & cond_high_low & cond_open_range & cond_volume\n",
    "\n",
    "    return df[valid_mask].copy()\n",
    "\n",
    "\n",
    "def filter_by_start_date(df: pd.DataFrame, start_date: str) -> pd.DataFrame:\n",
    "    return df[df[\"date\"] >= start_date]\n",
    "\n",
    "\n",
    "def remove_corrupted_tickers_df(\n",
    "        df: pd.DataFrame,\n",
    "        price_col: str = \"close\",\n",
    "        iqr_factor: float = 1.5,\n",
    "        threshold: float = 25.0,\n",
    ") -> tuple[pd.DataFrame, list[str]]:\n",
    "    \"\"\"\n",
    "    يحسب العوائد + القيم المتطرفة لكل سهم داخلياً،\n",
    "    ثم يحذف الأسهم التي نسبة القيم المتطرفة فيها تتجاوز threshold٪.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "    df[\"return\"] = (\n",
    "        df.groupby(\"ticker\")[price_col]\n",
    "        .pct_change()\n",
    "    )\n",
    "\n",
    "    def mark_outliers(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        valid = g[\"return\"].dropna()\n",
    "\n",
    "        if valid.empty:\n",
    "            g[\"return_is_outlier\"] = False\n",
    "            return g\n",
    "\n",
    "        q1 = valid.quantile(0.25)\n",
    "        q3 = valid.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - iqr_factor * iqr\n",
    "        upper = q3 + iqr_factor * iqr\n",
    "\n",
    "        g[\"return_is_outlier\"] = (g[\"return\"] < lower) | (g[\"return\"] > upper)\n",
    "        g.loc[g[\"return\"].isna(), \"return_is_outlier\"] = False\n",
    "        return g\n",
    "\n",
    "    df_marked = (\n",
    "        df\n",
    "        .groupby(\"ticker\", group_keys=False)\n",
    "        .apply(mark_outliers)\n",
    "    )\n",
    "\n",
    "    summary = (\n",
    "        df_marked\n",
    "        .groupby(\"ticker\")\n",
    "        .agg(\n",
    "            n_rows=(\"return\", \"count\"),\n",
    "            n_outliers=(\"return_is_outlier\", \"sum\"),\n",
    "        )\n",
    "    )\n",
    "    summary[\"outliers_ratio\"] = summary[\"n_outliers\"] / summary[\"n_rows\"] * 100\n",
    "\n",
    "    bad_tickers = summary[summary[\"outliers_ratio\"] > threshold].index.tolist()\n",
    "\n",
    "    df_cleaned = df_marked[~df_marked[\"ticker\"].isin(bad_tickers)].copy()\n",
    "\n",
    "    return df_cleaned, bad_tickers\n",
    "\n",
    "\n",
    "def run_basic_clean_df(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = verify_columns_and_types(df_raw)\n",
    "    df = filter_after_2010_df(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = remove_invalid_rows(df)\n",
    "    df = drop_ticker_date_duplicates(df)\n",
    "    df = remove_global_gaps(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_after_2010_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return filter_by_start_date(df, \"2010-01-01\")\n",
    "\n",
    "\n",
    "def remove_global_gaps(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.sort_values([\"ticker\", \"date\"]).copy()\n",
    "\n",
    "    # mark dates that have missing days before them\n",
    "    df[\"prev_date\"] = df.groupby(\"ticker\")[\"date\"].shift(1)\n",
    "    df[\"gap_days\"] = (df[\"date\"] - df[\"prev_date\"]).dt.days\n",
    "    df[\"missing_days\"] = (df[\"gap_days\"] - 1).fillna(0).astype(int)\n",
    "    gap_ratio_per_date = df[df[\"missing_days\"] >= 1].groupby(\"date\").size() / df.groupby(\"date\").size()\n",
    "    gap_ratio_per_date = gap_ratio_per_date.dropna()\n",
    "    global_gap_dates = gap_ratio_per_date[gap_ratio_per_date >= 0.8].index  # index is date here\n",
    "    df = df.drop(columns=['prev_date', 'gap_days'])\n",
    "    df.loc[df[\"date\"].isin(global_gap_dates), \"missing_days\"] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/predicting-stock-trends-rise-or-fall/train.csv')\n",
    "df = run_basic_clean_df(df)\n",
    "\n",
    "df, removed_tickers = remove_corrupted_tickers_df(\n",
    "    df,\n",
    "    price_col=\"close\",\n",
    "    iqr_factor=1.5,\n",
    "    threshold=25.0,\n",
    ")\n",
    "\n",
    "final_interim_path = \"/kaggle/data/interim/\"\n",
    "import os\n",
    "\n",
    "os.makedirs(final_interim_path, exist_ok=True)\n",
    "df.to_csv(final_interim_path + 'data.csv', index=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction (Run Once, Do Not Edit)",
   "id": "a84bd8137a568a04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "656d0fd7580416ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generating Samples (Do Not Edit)",
   "id": "b406d233f2da3047"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def build_samples(window_size=60, feature_cols=None, horizon=30):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = []\n",
    "\n",
    "    df = pd.read_csv('/kaggle/data/processed/data.csv')\n",
    "\n",
    "    ticker_data = {}\n",
    "    samples = []\n",
    "\n",
    "    for ticker, group in tqdm(df.groupby('ticker'), desc=\"Processing tickers\"):\n",
    "        group = group.sort_values('date').reset_index(drop=True)\n",
    "        n = len(group)\n",
    "\n",
    "        if n < window_size + horizon:\n",
    "            continue\n",
    "\n",
    "        close = group['close'].values.astype(np.float32)\n",
    "        missing = group[\"missing_days\"].values.astype(np.int8)\n",
    "        bad = (missing > 0).astype(np.int32)\n",
    "        bad_cumsum = np.cumsum(bad)\n",
    "\n",
    "        def has_gap(a, b):\n",
    "            return bad_cumsum[b] - (bad_cumsum[a - 1] if a > 0 else 0) > 0\n",
    "\n",
    "        ticker_data[ticker] = group[feature_cols].values.astype(np.float32)\n",
    "\n",
    "        dates = group['date'].values\n",
    "        for i in range(window_size, n - horizon):\n",
    "            seq_start = i - window_size + 1\n",
    "            seq_end = i + horizon\n",
    "\n",
    "            if has_gap(seq_start, seq_end):\n",
    "                continue\n",
    "\n",
    "            label = 1 if close[i + horizon] > close[i] else 0\n",
    "\n",
    "            date = dates[i]\n",
    "            samples.append((ticker, i, label, date))\n",
    "    StockDataset.ticker_data = ticker_data\n",
    "    print(f\"✓ Processed {len(samples):,} samples from {len(ticker_data)} tickers\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "def split_samples_time_based(\n",
    "        samples: List[Tuple[str, int, int, object]],\n",
    "        train_ratio: float = 0.7,\n",
    "        val_ratio: float = 0.15,\n",
    "):\n",
    "    samples_sorted = sorted(samples, key=lambda x: x[3])\n",
    "\n",
    "    n = len(samples_sorted)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    train_samples = samples_sorted[:n_train]\n",
    "    val_samples = samples_sorted[n_train:n_train + n_val]\n",
    "    test_samples = samples_sorted[n_train + n_val:]\n",
    "    return train_samples, val_samples, test_samples\n"
   ],
   "id": "caaf483cb5cb2ede"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset (Do Not Edit)",
   "id": "700b08dc68782b60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    ticker_data = {}\n",
    "\n",
    "    def __init__(self, samples, window_size=60, horizon=30):\n",
    "        self.samples = samples\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ticker, i, y, date = self.samples[idx]\n",
    "        data = StockDataset.ticker_data[ticker]\n",
    "        X = data[i - self.window_size + 1:i + 1].copy()  # i+1 is excluded\n",
    "        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32)\n"
   ],
   "id": "ecc828e5bf125260"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "eea46ee1c306e4c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int = 128,\n",
    "            num_layers: int = 2,\n",
    "            bidirectional: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.actual_hidden_size = hidden_size * self.num_directions\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.actual_hidden_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif \"bias\" in name:\n",
    "                param.data.zero_()\n",
    "            elif name.startswith(\"fc\") and \"weight\" in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        gru_out, h_n = self.gru(x)\n",
    "        # h_n: (num_layers*num_directions, batch, hidden_size)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            h_forward = h_n[-2, :, :]\n",
    "            h_backward = h_n[-1, :, :]\n",
    "            context = torch.cat([h_forward, h_backward], dim=1)  # (batch, 2*hidden)\n",
    "        else:\n",
    "            context = h_n[-1, :, :]\n",
    "\n",
    "        out = self.fc1(context)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out.squeeze(-1)  # (batch,)\n",
    "\n",
    "\n"
   ],
   "id": "3dd457673f88df05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Setup",
   "id": "fe1f4060159be40d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### GPU configs (Do not Edit)",
   "id": "d1218d4abdf50b46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "# torch.backends.cudnn.benchmark = True\n"
   ],
   "id": "567fbee5b3fd1561"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### create datasets   **Editable**",
   "id": "cef3990e53843bce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "window_size = 60  # editable\n",
    "horizon = 30  # Do not edit this\n",
    "feature = []  # editable\n",
    "samples = build_samples(window_size, feature, horizon)\n",
    "\n",
    "train_s, val_s, test_s = split_samples_time_based(samples)\n",
    "\n",
    "train_ds = StockDataset(train_s, window_size=window_size, horizon=horizon)\n",
    "val_ds = StockDataset(val_s, window_size=window_size, horizon=horizon)\n",
    "test_ds = StockDataset(test_s, window_size=window_size, horizon=horizon)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=4,  # can be edited , depending on hardware\n",
    "                          pin_memory=True,\n",
    "                          persistent_workers=True,\n",
    "                          prefetch_factor=3\n",
    "\n",
    "                          )\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=4,  # can be edited , depending on hardware\n",
    "                        pin_memory=True,\n",
    "                        persistent_workers=True,\n",
    "                        prefetch_factor=3)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=4,  # can be edited , depending on hardware\n",
    "                         pin_memory=True,\n",
    "                         persistent_workers=True,\n",
    "                         prefetch_factor=3)\n",
    "\n",
    "INPUT_SIZE = len(feature)\n",
    "SEQ_LEN = window_size\n",
    "BATCH_SIZE = batch_size\n",
    "\n",
    "\n"
   ],
   "id": "48bda7ff6d11c61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### model config",
   "id": "470acc51b37abbfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = GRUModel(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    ").to(device)"
   ],
   "id": "c62234e11f292f43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### training loop",
   "id": "3a82e13768c92ab0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loss_fun = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "id": "ee0cb0e06c2f7a0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Training Functions",
   "id": "12915d7f1906d9c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    train: bool = True,\n",
    "    optimizer: Optional[torch.optim.Optimizer] = None,\n",
    ") -> Dict[str, float]:\n",
    "    if train:\n",
    "        model.train()\n",
    "        context = torch.enable_grad()\n",
    "    else:\n",
    "        model.eval()\n",
    "        context = torch.no_grad()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    with context:\n",
    "        for X, y in loader:\n",
    "            X = X.to(device,non_blocking=True)          # (batch, seq_len, input_size)\n",
    "            y = y.to(device,non_blocking=True).float()  # (batch,)\n",
    "\n",
    "            logits = model(X)         # (batch,)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            running_correct += (preds == y).sum().item()\n",
    "            running_total += y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc = running_correct / running_total\n",
    "\n",
    "    return {\"loss\": epoch_loss, \"acc\": epoch_acc}\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    num_epochs: int,\n",
    "    lr: float = 1e-3,\n",
    "):\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    history: Dict[str, List[float]] = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "    }\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state_dict = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_metrics = run_epoch(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            criterion=criterion,\n",
    "            train=True,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "\n",
    "        val_metrics = run_epoch(\n",
    "            model=model,\n",
    "            loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            train=False,\n",
    "            optimizer=None,\n",
    "        )\n",
    "\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "\n",
    "        if val_metrics[\"loss\"] < best_val_loss:\n",
    "            best_val_loss = val_metrics[\"loss\"]\n",
    "            best_state_dict = model.state_dict()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"train_loss={train_metrics['loss']:.4f}  \"\n",
    "            f\"train_acc={train_metrics['acc']:.4f}  \"\n",
    "            f\"val_loss={val_metrics['loss']:.4f}  \"\n",
    "            f\"val_acc={val_metrics['acc']:.4f}\"\n",
    "        )\n",
    "\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    return model, history\n",
    "model, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=30,\n",
    "        lr=1e-2,\n",
    "    )"
   ],
   "id": "3a18f3d8c8334017"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "939b1fb648a20d7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Testing",
   "id": "1ab4fd836e45e861"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in tqdm(loader, desc=\"Evaluating\"):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            logits = model(xb)\n",
    "\n",
    "        preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "        correct += (preds == yb.long()).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total"
   ],
   "id": "cc0e17701912e489"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
