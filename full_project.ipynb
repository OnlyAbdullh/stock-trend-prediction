{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec605fc",
   "metadata": {},
   "source": [
    "# Data Cleaning & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f7ced79a4d0e1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T17:05:35.075309900Z",
     "start_time": "2026-01-30T17:05:35.033643900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def verify_columns_and_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "        .str.replace(r\"[^\\w]\", \"\", regex=True)\n",
    "        .str.lower()\n",
    "    )\n",
    "    required_cols = [\"date\", \"ticker\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"dividends\", \"stock_splits\"]\n",
    "\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    price_cols = [\"open\", \"high\", \"low\", \"close\"]\n",
    "    for c in price_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df[\"volume\"] = pd.to_numeric(df[\"volume\"], errors=\"coerce\")\n",
    "    df[\"ticker\"] = df[\"ticker\"].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(\n",
    "        df: pd.DataFrame,\n",
    "        price_cols=(\"open\", \"high\", \"low\", \"close\"),\n",
    "        max_na_fraction=0.10,\n",
    ") -> pd.DataFrame:\n",
    "    price_cols = list(price_cols)\n",
    "\n",
    "    removed_tickers = 0\n",
    "    removed_rows = 0\n",
    "\n",
    "    def process_ticker(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        nonlocal removed_tickers, removed_rows\n",
    "\n",
    "        rows_before = len(g)\n",
    "        g = g.sort_values(\"date\")\n",
    "\n",
    "        while not g.empty and g[price_cols].iloc[0].isna().any():\n",
    "            g = g.iloc[1:]\n",
    "\n",
    "        while not g.empty and g[price_cols].iloc[-1].isna().any():\n",
    "            g = g.iloc[:-1]\n",
    "\n",
    "        if g.empty:\n",
    "            removed_tickers += 1\n",
    "            removed_rows += rows_before\n",
    "            return g\n",
    "\n",
    "        na_fraction = g[price_cols].isna().mean().mean()\n",
    "\n",
    "        if na_fraction > max_na_fraction:\n",
    "            removed_tickers += 1\n",
    "            removed_rows += rows_before\n",
    "            return g.iloc[0:0]\n",
    "\n",
    "        g[price_cols] = g[price_cols].ffill().bfill()\n",
    "        removed_rows += (rows_before - len(g))\n",
    "\n",
    "        return g\n",
    "\n",
    "    df_clean = (\n",
    "        df\n",
    "        .groupby(\"ticker\", group_keys=False)\n",
    "        .apply(process_ticker)\n",
    "    )\n",
    "    return df_clean # TODO : check this\n",
    "\n",
    "\n",
    "def drop_ticker_date_duplicates(\n",
    "    df: pd.DataFrame,\n",
    "    max_duplicates_per_ticker: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    dup_counts = (\n",
    "        df.groupby([\"ticker\", \"date\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"n\")\n",
    "    )\n",
    "    bad_tickers = (\n",
    "        dup_counts[dup_counts[\"n\"] > 1]\n",
    "        .groupby(\"ticker\")[\"n\"]\n",
    "        .sum()\n",
    "    )\n",
    "    bad_tickers = bad_tickers[bad_tickers > max_duplicates_per_ticker].index\n",
    "    df = df[~df[\"ticker\"].isin(bad_tickers)]\n",
    "    df = df.drop_duplicates(subset=[\"ticker\", \"date\"], keep=\"first\")\n",
    "    return df\n",
    "\n",
    "def remove_invalid_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    cond_open = df[\"open\"] != 0\n",
    "    cond_close = df[\"close\"] != 0\n",
    "    cond_high_low = df[\"high\"] >= df[\"low\"]\n",
    "    cond_open_range = (df[\"open\"] >= df[\"low\"]) & (df[\"open\"] <= df[\"high\"])\n",
    "    cond_volume = df[\"volume\"] > 0\n",
    "    valid_mask = cond_open & cond_close & cond_high_low & cond_open_range & cond_volume\n",
    "\n",
    "    return df[valid_mask]\n",
    "\n",
    "def filter_by_start_date(df: pd.DataFrame, start_date: str) -> pd.DataFrame:\n",
    "    return df[df[\"date\"] >= start_date]\n",
    "\n",
    "def remove_corrupted_tickers_df(\n",
    "    df: pd.DataFrame,\n",
    "    price_col: str = \"close\",\n",
    "    iqr_factor: float = 1.5,\n",
    "    threshold: float = 25.0,\n",
    ") -> tuple[pd.DataFrame, list[str]]:\n",
    "    \"\"\"\n",
    "    يحسب العوائد + القيم المتطرفة لكل سهم داخلياً،\n",
    "    ثم يحذف الأسهم التي نسبة القيم المتطرفة فيها تتجاوز threshold٪.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "    df[\"return\"] = (\n",
    "        df.groupby(\"ticker\")[price_col]\n",
    "        .pct_change()\n",
    "    )\n",
    "\n",
    "    def mark_outliers(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        valid = g[\"return\"].dropna()\n",
    "\n",
    "        if valid.empty:\n",
    "            g[\"return_is_outlier\"] = False\n",
    "            return g\n",
    "\n",
    "        q1 = valid.quantile(0.25)\n",
    "        q3 = valid.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - iqr_factor * iqr\n",
    "        upper = q3 + iqr_factor * iqr\n",
    "\n",
    "        g[\"return_is_outlier\"] = (g[\"return\"] < lower) | (g[\"return\"] > upper)\n",
    "        g.loc[g[\"return\"].isna(), \"return_is_outlier\"] = False\n",
    "        return g\n",
    "\n",
    "    df_marked = (\n",
    "        df\n",
    "        .groupby(\"ticker\", group_keys=False)\n",
    "        .apply(mark_outliers)\n",
    "    )\n",
    "\n",
    "    summary = (\n",
    "        df_marked\n",
    "        .groupby(\"ticker\")\n",
    "        .agg(\n",
    "            n_rows=(\"return\", \"count\"),\n",
    "            n_outliers=(\"return_is_outlier\", \"sum\"),\n",
    "        )\n",
    "    )\n",
    "    summary[\"outliers_ratio\"] = summary[\"n_outliers\"] / summary[\"n_rows\"] * 100\n",
    "\n",
    "    bad_tickers = summary[summary[\"outliers_ratio\"] > threshold].index.tolist()\n",
    "\n",
    "    df_cleaned = df_marked[~df_marked[\"ticker\"].isin(bad_tickers)]\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "\n",
    "def filter_after_2010_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return filter_by_start_date(df, \"2010-01-01\")\n",
    "\n",
    "def remove_global_gaps(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df.sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "    # mark dates that have missing days before them\n",
    "    df[\"prev_date\"] = df.groupby(\"ticker\")[\"date\"].shift(1)\n",
    "    df[\"gap_days\"] = (df[\"date\"] - df[\"prev_date\"]).dt.days\n",
    "    df[\"missing_days\"] = (df[\"gap_days\"] - 1).fillna(0).astype(int)\n",
    "    gap_ratio_per_date = df[df[\"missing_days\"] >= 1].groupby(\"date\").size() / df.groupby(\"date\").size()\n",
    "    gap_ratio_per_date = gap_ratio_per_date.dropna()\n",
    "    global_gap_dates = gap_ratio_per_date[gap_ratio_per_date >= 0.8].index  # index is date here\n",
    "    df = df.drop(columns=['prev_date', 'gap_days'])\n",
    "    df.loc[df[\"date\"].isin(global_gap_dates), \"missing_days\"] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_features(df):\n",
    "\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    grouped = df.groupby('ticker')\n",
    "\n",
    "    # ========================================================================\n",
    "    # TARGET VARIABLE\n",
    "    # ========================================================================\n",
    "    df['close_30d_future'] = grouped['close'].shift(-30)\n",
    "    df['target'] = (df['close_30d_future'] > df['close']).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Price Features\n",
    "    # ------------------------------\n",
    "    df['daily_return'] = grouped['close'].pct_change()\n",
    "    df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "\n",
    "    # ------------------------------\n",
    "    # Moving Averages\n",
    "    # ------------------------------\n",
    "    df['MA_5'] = grouped['close'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "    df['MA_20'] = grouped['close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['MA_60'] = grouped['close'].transform(lambda x: x.rolling(60, min_periods=1).mean())\n",
    "\n",
    "    # ------------------------------\n",
    "    # MA-Based Features\n",
    "    # ------------------------------\n",
    "    df['price_to_MA5'] = (df['close'] - df['MA_5']) / (df['MA_5'] + 1e-8)\n",
    "    df['price_to_MA20'] = (df['close'] - df['MA_20']) / (df['MA_20'] + 1e-8)\n",
    "    df['price_to_MA60'] = (df['close'] - df['MA_60']) / (df['MA_60'] + 1e-8)\n",
    "    df['MA_60_slope'] = grouped['MA_60'].pct_change(30)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volatility Features\n",
    "    # ------------------------------\n",
    "    df['volatility_20'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.rolling(20, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    def calculate_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "        rs = gain / (loss + 1e-8)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI_14'] = grouped['close'].transform(lambda x: calculate_rsi(x, 14))\n",
    "\n",
    "    df['parkinson_volatility'] = grouped.apply(\n",
    "        lambda x: np.sqrt(\n",
    "            1 / (4 * np.log(2)) *\n",
    "            ((np.log(x['high'] / (x['low'] + 1e-8))) ** 2).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Support/Resistance & Risk\n",
    "    # ------------------------------\n",
    "    df['recent_high_20'] = grouped['high'].transform(lambda x: x.rolling(20, min_periods=1).max())\n",
    "    df['recent_low_20'] = grouped['low'].transform(lambda x: x.rolling(20, min_periods=1).min())\n",
    "    df['distance_from_high'] = (df['close'] - df['recent_high_20']) / (df['recent_high_20'] + 1e-8)\n",
    "    df['low_to_close_ratio'] = df['recent_low_20'] / (df['close'] + 1e-8)\n",
    "    df['price_position_20'] = (\n",
    "            (df['close'] - df['recent_low_20']) /\n",
    "            (df['recent_high_20'] - df['recent_low_20'] + 1e-8)\n",
    "    )\n",
    "\n",
    "    def max_drawdown(series, window):\n",
    "        roll_max = series.rolling(window, min_periods=1).max()\n",
    "        drawdown = (series - roll_max) / (roll_max + 1e-8)\n",
    "        return drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    df['max_drawdown_20'] = grouped['close'].transform(lambda x: max_drawdown(x, 20))\n",
    "    df['downside_deviation_10'] = grouped['daily_return'].transform(\n",
    "        lambda x: x.where(x < 0, 0).rolling(10, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Temporal\n",
    "    # ------------------------------\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['is_up_day'] = (df['daily_return'] > 0).astype(int)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Volume Price Index (NEW)\n",
    "    # ------------------------------\n",
    "    df['price_change'] = grouped['close'].pct_change()\n",
    "    df['PVT'] = (df['price_change'] * df['volume']).fillna(0)\n",
    "    df['PVT_cumsum'] = grouped['PVT'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    df['MOBV_signal'] = np.where(df['price_change'] > 0, df['volume'],\n",
    "                                 np.where(df['price_change'] < 0, -df['volume'], 0))\n",
    "    df['MOBV'] = grouped['MOBV_signal'].transform(lambda x: x.cumsum())\n",
    "\n",
    "    # ------------------------------\n",
    "    # Directional Movement\n",
    "    # ------------------------------\n",
    "    df['MTM'] = df['close'] - grouped['close'].shift(12)\n",
    "\n",
    "    # ------------------------------\n",
    "    # OverBought & OverSold\n",
    "    # ------------------------------\n",
    "    df['DTM'] = np.where(df['open'] <= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['high'] - df['open'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DBM'] = np.where(df['open'] >= grouped['open'].shift(1),\n",
    "                         0,\n",
    "                         np.maximum(df['open'] - df['low'], df['open'] - grouped['open'].shift(1)))\n",
    "    df['DTM_sum'] = grouped['DTM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['DBM_sum'] = grouped['DBM'].transform(lambda x: x.rolling(23, min_periods=1).sum())\n",
    "    df['ADTM'] = (df['DTM_sum'] - df['DBM_sum']) / (df['DTM_sum'] + df['DBM_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Energy & Volatility\n",
    "    # ------------------------------\n",
    "    df['PSY'] = grouped['is_up_day'].transform(lambda x: x.rolling(12, min_periods=1).mean()) * 100\n",
    "\n",
    "    df['highest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).max())\n",
    "    df['lowest_close'] = grouped['close'].transform(lambda x: x.rolling(28, min_periods=1).min())\n",
    "    df['close_diff_sum'] = grouped['close'].transform(lambda x: x.diff().abs().rolling(28, min_periods=1).sum())\n",
    "    df['VHF'] = (df['highest_close'] - df['lowest_close']) / (df['close_diff_sum'] + 1e-8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Stochastic\n",
    "    # ------------------------------\n",
    "    df['lowest_low_9'] = grouped['low'].transform(lambda x: x.rolling(9, min_periods=1).min())\n",
    "    df['highest_high_9'] = grouped['high'].transform(lambda x: x.rolling(9, min_periods=1).max())\n",
    "    df['K'] = ((df['close'] - df['lowest_low_9']) / (df['highest_high_9'] - df['lowest_low_9'] + 1e-8)) * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup temporary columns 41 - 16 = 26\n",
    "    # ------------------------------\n",
    "    drop_columns = [\n",
    "        'MA_5', 'MA_20', 'MA_60',\n",
    "        'price_change', 'PVT', 'MOBV_signal',\n",
    "        'DTM', 'DBM', 'DTM_sum', 'DBM_sum',\n",
    "        'highest_close', 'lowest_close', 'close_diff_sum',\n",
    "        'lowest_low_9', 'highest_high_9', 'recent_low_20',\n",
    "        'close_30d_future', 'target'\n",
    "        , 'low', 'high', 'open', 'volume', 'dividends',\n",
    "        'stock_splits', 'return', 'return_is_outlier']\n",
    "    df = df.drop(columns=drop_columns, errors='igonre')\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    df = df.drop(columns=drop_columns, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def clean_and_build_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = verify_columns_and_types(df_raw)\n",
    "    df = filter_after_2010_df(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = remove_invalid_rows(df)\n",
    "    df = drop_ticker_date_duplicates(df)\n",
    "    df = remove_global_gaps(df)\n",
    "    df = remove_corrupted_tickers_df(\n",
    "        df,\n",
    "        price_col=\"close\",\n",
    "        iqr_factor=1.5,\n",
    "        threshold=25.0,\n",
    "    )\n",
    "    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "    df = engineer_features(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6945a15d570af167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T17:11:18.248786600Z",
     "start_time": "2026-01-30T17:10:42.554725500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = clean_and_build_features(pd.read_csv('data/raw/train.csv'))\n",
    "# df.to_csv('data/processed/data.csv')\n",
    "df = pd.read_csv('data/processed/data_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1251e657a42b92",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e538b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int = 128,\n",
    "            num_layers: int = 2,\n",
    "            dropout: float = 0.3,\n",
    "            bidirectional: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.actual_hidden_size = hidden_size * self.num_directions\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(self.actual_hidden_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.actual_hidden_size, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(16, 4)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif \"bias\" in name:\n",
    "                param.data.zero_()\n",
    "            elif name.startswith(\"fc\") and \"weight\" in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gru_out, h_n = self.gru(x)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            h_forward = h_n[-2, :, :]\n",
    "            h_backward = h_n[-1, :, :]\n",
    "            context = torch.cat([h_forward, h_backward], dim=1)\n",
    "        else:\n",
    "            context = h_n[-1, :, :]\n",
    "\n",
    "        context = self.batch_norm(context)\n",
    "\n",
    "        out = self.fc1(context)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920649c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a983bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class StockDataset(Dataset):  \n",
    "    def __init__(self, samples, window_size=60, horizon=30, ticker_data=None):\n",
    "        self.samples = samples\n",
    "        self.ticker_data = ticker_data\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ticker, i, y, date = self.samples[idx]\n",
    "        data = self.ticker_data[ticker]\n",
    "        X = data[i - self.window_size + 1:i + 1].copy()\n",
    "        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924e6c6",
   "metadata": {},
   "source": [
    "# Splitting Data & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038a9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MaxAbsScaler\n",
    "def split_dataframe_by_date(\n",
    "    df: pd.DataFrame,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "\n",
    "    train_df = df.iloc[:train_end].copy()\n",
    "    val_df = df.iloc[train_end:val_end].copy()\n",
    "    test_df = df.iloc[val_end:].copy()\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Mode 1\n",
    "no_need_scaling = [\n",
    "    \"is_up_day\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"price_position_20\",\n",
    "]\n",
    "\n",
    "robust_scaling_features = [\n",
    "    \"distance_from_high\",\n",
    "    \"downside_deviation_10\",\n",
    "    \"high_low_ratio\",\n",
    "    \"low_to_close_ratio\",\n",
    "    \"max_drawdown_20\",\n",
    "    \"parkinson_volatility\",\n",
    "    \"recent_high_20\",\n",
    "    \"volatility_20\",\n",
    "    \"VHF\",\n",
    "    \"MOBV\",\n",
    "    \"PVT_cumsum\",\n",
    "]\n",
    "\n",
    "zscore_features = [\n",
    "    \"ADTM\",\n",
    "    \"daily_return\",\n",
    "    \"MA_60_slope\",\n",
    "    \"MTM\",\n",
    "    \"price_to_MA5\",\n",
    "    \"price_to_MA20\",\n",
    "    \"price_to_MA60\",\n",
    "    \"PSY\",\n",
    "    \"RSI_14\",\n",
    "]\n",
    "standard_scaler_features = [\"K\"]\n",
    "\n",
    "\n",
    "# Mode 2\n",
    "z_only = [\"ADTM\", \"K\", \"price_position_20\", \"RSI_14\"]\n",
    "z_plus_q = [\n",
    "    \"daily_return\", \"MA_60_slope\", \"MOBV\", \"MTM\",\n",
    "    \"price_to_MA5\", \"price_to_MA20\", \"price_to_MA60\",\n",
    "    \"PVT_cumsum\", \"recent_high_20\"\n",
    "]\n",
    "robust_only = [\"max_drawdown_20\", \"VHF\"]\n",
    "robust_plus_q = [\n",
    "    \"distance_from_high\", \"downside_deviation_10\",\n",
    "    \"high_low_ratio\", \"low_to_close_ratio\", \"parkinson_volatility\"\n",
    "]\n",
    "robust_plus_z = [\"volatility_20\"]\n",
    "max_abs_only = [\"PSY\"]\n",
    "no_scaling_mode2 = [\"is_up_day\", \"month_sin\", \"month_cos\"]\n",
    "\n",
    "def normalize_df(df_train, df_val, df_test, norm_mode=\"norm1\"):\n",
    "    if norm_mode == \"norm1\":\n",
    "        return normalize_df_mode1(df_train, df_val, df_test)\n",
    "    elif norm_mode == \"norm2\":\n",
    "        return normalize_df_mode2(df_train, df_val, df_test)\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def normalize_df_mode1(df_train, df_val, df_test):\n",
    "    scalars = {\n",
    "        \"robust\": RobustScaler().fit(df_train[robust_scaling_features]),\n",
    "        \"z\": StandardScaler().fit(df_train[zscore_features]),\n",
    "        \"std\": StandardScaler().fit(df_train[standard_scaler_features]),\n",
    "    }\n",
    "    return (\n",
    "        normalize_df_sc1(df_train, scalars),\n",
    "        normalize_df_sc1(df_val, scalars),\n",
    "        normalize_df_sc1(df_test, scalars),\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_df_sc1(df, scalers):\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[robust_scaling_features] = scalers[\"robust\"].transform(\n",
    "        df[robust_scaling_features]\n",
    "    )\n",
    "    df_scaled[zscore_features] = scalers[\"z\"].transform(df[zscore_features])\n",
    "    df_scaled[standard_scaler_features] = scalers[\"std\"].transform(\n",
    "        df[standard_scaler_features]\n",
    "    )\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "def get_q_limits(df, columns):\n",
    "    limits = {}\n",
    "    for col in columns:\n",
    "        limits[col] = (df[col].quantile(0.01), df[col].quantile(0.99))\n",
    "    return limits\n",
    "\n",
    "\n",
    "def apply_clipping(df, limits):\n",
    "    df_clipped = df.copy()\n",
    "    for col, (low, high) in limits.items():\n",
    "        if col in df_clipped.columns:\n",
    "            df_clipped[col] = df_clipped[col].clip(lower=low, upper=high)\n",
    "    return df_clipped\n",
    "\n",
    "\n",
    "def normalize_df_mode2(df_train, df_val, df_test):\n",
    "    q_columns = z_plus_q + robust_plus_q\n",
    "    q_limits = get_q_limits(df_train, q_columns)\n",
    "\n",
    "    train_c = apply_clipping(df_train, q_limits)\n",
    "    val_c = apply_clipping(df_val, q_limits)\n",
    "    test_c = apply_clipping(df_test, q_limits)\n",
    "\n",
    "    scalers = {\n",
    "        \"z\": StandardScaler().fit(train_c[z_only + z_plus_q]),\n",
    "        \"robust\": RobustScaler().fit(train_c[robust_only + robust_plus_q]),\n",
    "        \"max_abs\": MaxAbsScaler().fit(train_c[max_abs_only]),\n",
    "        \"vol_robust\": RobustScaler().fit(train_c[robust_plus_z])\n",
    "    }\n",
    "\n",
    "    vol_robust_train = scalers[\"vol_robust\"].transform(train_c[robust_plus_z])\n",
    "    scalers[\"vol_z\"] = StandardScaler().fit(vol_robust_train)\n",
    "\n",
    "    return (\n",
    "        normalize_df_sc2(train_c, scalers),\n",
    "        normalize_df_sc2(val_c, scalers),\n",
    "        normalize_df_sc2(test_c, scalers)\n",
    "    )\n",
    "\n",
    "def normalize_df_sc2(df_in, scalers):\n",
    "    df_out = df_in.copy()\n",
    "    # Z-Score\n",
    "    df_out[z_only + z_plus_q] = scalers[\"z\"].transform(df_in[z_only + z_plus_q])\n",
    "    # Robust\n",
    "    df_out[robust_only + robust_plus_q] = scalers[\"robust\"].transform(df_in[robust_only + robust_plus_q])\n",
    "    # Max Abs\n",
    "    df_out[max_abs_only] = scalers[\"max_abs\"].transform(df_in[max_abs_only])\n",
    "    # Volatility (R + Z)\n",
    "    vol_r = scalers[\"vol_robust\"].transform(df_in[robust_plus_z])\n",
    "    df_out[robust_plus_z] = scalers[\"vol_z\"].transform(vol_r)\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1ed09",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874f0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "FEATURE_COLS = [\n",
    "        \"daily_return\",\n",
    "        \"high_low_ratio\",\n",
    "        \"price_to_MA5\",\n",
    "        \"price_to_MA20\",\n",
    "        \"price_to_MA60\",\n",
    "        \"MA_60_slope\",\n",
    "        \"volatility_20\",\n",
    "        \"RSI_14\",\n",
    "        \"parkinson_volatility\",\n",
    "        \"recent_high_20\",\n",
    "        \"distance_from_high\",\n",
    "        \"low_to_close_ratio\",\n",
    "        \"price_position_20\",\n",
    "        \"max_drawdown_20\",\n",
    "        \"downside_deviation_10\",\n",
    "        \"month_sin\",\n",
    "        \"month_cos\",\n",
    "        \"is_up_day\",\n",
    "        \"PVT_cumsum\",\n",
    "        \"MOBV\",\n",
    "        \"MTM\",\n",
    "        \"ADTM\",\n",
    "        \"PSY\",\n",
    "        \"VHF\",\n",
    "        \"K\",\n",
    "    ]\n",
    "def build_samples_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    window_size: int = 60,\n",
    "    horizon: int = 30,\n",
    ") -> Tuple[List, Dict]:\n",
    "    ticker_data = {}\n",
    "    samples = []\n",
    "\n",
    "\n",
    "    for ticker, group in tqdm(df.groupby(\"ticker\"), desc=\"Building samples\"):\n",
    "        group = group.sort_values(\"date\").reset_index(drop=True)\n",
    "        n = len(group)\n",
    "\n",
    "        if n < window_size + horizon:\n",
    "            continue\n",
    "\n",
    "        close = group[\"close\"].values.astype(np.float32)\n",
    "        missing = group[\"missing_days\"].values.astype(np.int8)\n",
    "        bad = (missing > 0).astype(np.int32)\n",
    "        bad_cumsum = np.cumsum(bad)\n",
    "\n",
    "        def has_gap(a, b):\n",
    "            return bad_cumsum[b] - (bad_cumsum[a - 1] if a > 0 else 0) > 0\n",
    "\n",
    "        # Store feature data\n",
    "        ticker_data[ticker] = group[FEATURE_COLS].values.astype(np.float32)\n",
    "\n",
    "        dates = group[\"date\"].values\n",
    "        for i in range(window_size - 1, n - horizon):\n",
    "            seq_start = i - window_size + 1\n",
    "            seq_end = i + horizon\n",
    "\n",
    "            if has_gap(seq_start, seq_end):\n",
    "                continue\n",
    "\n",
    "            label = 1 if close[i + horizon] > close[i] else 0\n",
    "            date = dates[i]\n",
    "            samples.append((ticker, i, label, date))\n",
    "\n",
    "    return samples, ticker_data\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5f977",
   "metadata": {},
   "source": [
    "# Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    name: str = \"default\"\n",
    "    window_size: int = 60\n",
    "    # Model\n",
    "    model_type: str = \"gru\"   \n",
    "    hidden_size: int = 32\n",
    "    num_layers: int = 2\n",
    "    bidirectional: bool = False\n",
    "    dropout: float = 0.4\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    optimizer: str = \"Adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a634326",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7fc539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(history, filename=\"training_curves.png\"):\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # ---- Loss plot ----\n",
    "    axes[0].plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "    axes[0].plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # ---- Accuracy plot ----\n",
    "    axes[1].plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
    "    axes[1].plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Accuracy\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d8043f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_MIXED_PRECISION = torch.cuda.is_available()\n",
    "if USE_MIXED_PRECISION:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "print(device, USE_MIXED_PRECISION)\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    train: bool = True,\n",
    "    optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "    scaler: Optional[torch.amp.GradScaler] = None,\n",
    ") -> Dict[str, float]:\n",
    "    if train:\n",
    "        model.train()\n",
    "        context = torch.enable_grad()\n",
    "    else:\n",
    "        model.eval()\n",
    "        context = torch.no_grad()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    progress_bar = tqdm(loader, desc=\"train\" if train else \"val\")\n",
    "\n",
    "    with context:\n",
    "        for idx, (X, y) in enumerate(progress_bar):\n",
    "            X = X.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True).float()\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "            if USE_MIXED_PRECISION and train:\n",
    "                with torch.amp.autocast(\"cuda\"):\n",
    "                    logits = model(X)\n",
    "                    loss = criterion(logits, y)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                logits = model(X)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "                if train:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "            if idx % 10 == 0:\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            running_correct += (preds == y).sum().item()\n",
    "            running_total += y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc = running_correct / running_total\n",
    "\n",
    "    return {\"loss\": epoch_loss, \"acc\": epoch_acc}\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    cfg: TrainingConfig,\n",
    "    optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "    history: Optional[Dict[str, List[float]]] = None,\n",
    "    best_val_loss: float = float(\"inf\"),\n",
    "    start_epoch: int = 0,\n",
    "):\n",
    "    model.to(device)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\") if USE_MIXED_PRECISION else None\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay\n",
    "        )\n",
    "\n",
    "    if history is None:\n",
    "        history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    best_state_dict = None\n",
    "\n",
    "    for epoch in range(start_epoch + 1, start_epoch + num_epochs + 1):\n",
    "        train_metrics = run_epoch(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            criterion=criterion,\n",
    "            train=True,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "        )\n",
    "        val_metrics = run_epoch(\n",
    "            model=model,\n",
    "            loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            train=False,\n",
    "            optimizer=None,\n",
    "            scaler=None,\n",
    "        )\n",
    "\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "\n",
    "        if val_metrics[\"loss\"] < best_val_loss:\n",
    "            best_val_loss = val_metrics[\"loss\"]\n",
    "            best_state_dict = model.state_dict()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"train_loss={train_metrics['loss']:.4f}  \"\n",
    "            f\"train_acc={train_metrics['acc']:.4f}  \"\n",
    "            f\"val_loss={val_metrics['loss']:.4f}  \"\n",
    "            f\"val_acc={val_metrics['acc']:.4f}\"\n",
    "        )\n",
    "\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    models_dir = \"models\"\n",
    "    vis_dir = \"figures\"\n",
    "\n",
    "    config_name = getattr(cfg, \"name\", \"unnamed\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    prefix = f\"gru_{config_name}_{timestamp}\"\n",
    "    filename = f\"{prefix}.pt\"\n",
    "    plot_name = f\"{prefix}.png\"\n",
    "    plot_training_curves(history, plot_name)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"config\": cfg.__dict__,\n",
    "            \"history\": history,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"use_mixed_precision\": USE_MIXED_PRECISION,\n",
    "        },\n",
    "        filename,\n",
    "    )\n",
    "    print(f\"\\nSaved checkpoint to: {filename}\")\n",
    "    print(f\"\\nSaved Plot to: {plot_name}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1794d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c951be9",
   "metadata": {},
   "source": [
    "### Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd9be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg  = TrainingConfig(\n",
    "    name=\"fifth\",\n",
    "    model_type=\"gru\",\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    bidirectional=False,\n",
    "    dropout=0.3,\n",
    "    batch_size=512,\n",
    "    learning_rate=3e-4,\n",
    "    window_size=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec49d1b",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = split_dataframe_by_date(\n",
    "df, train_ratio=0.7, val_ratio=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29748e4",
   "metadata": {},
   "source": [
    "### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df,test_df =  normalize_df(train_df, val_df,test_df,'norm1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b8000",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "train_samples, train_ticker_data = build_samples_from_df(\n",
    "    train_df, window_size=cfg.window_size, horizon=30\n",
    ")\n",
    "del train_df\n",
    "gc.collect()\n",
    "val_samples, val_ticker_data = build_samples_from_df(\n",
    "    val_df, window_size=cfg.window_size, horizon=30\n",
    ")\n",
    "del val_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb68562",
   "metadata": {},
   "source": [
    "### Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc33ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = StockDataset(\n",
    "    train_samples,\n",
    "    window_size=cfg.window_size,\n",
    "    horizon=30,\n",
    "    ticker_data=train_ticker_data,\n",
    ")\n",
    "val_ds = StockDataset(\n",
    "    val_samples, window_size=cfg.window_size, horizon=30, ticker_data=val_ticker_data\n",
    ")\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# workers = 2\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers=workers,\n",
    "    # pin_memory=True,\n",
    "    # prefetch_factor=2,\n",
    "    # persistent_workers=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    # num_workers=workers,\n",
    "    # persistent_workers=True,\n",
    "    # pin_memory=True,\n",
    "    # prefetch_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a6423",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25381cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _ = train_ds[0]\n",
    "input_size = x0.shape[-1]\n",
    "model = GRUModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=cfg.hidden_size,\n",
    "    num_layers=cfg.num_layers,\n",
    "    dropout=cfg.dropout,\n",
    "    bidirectional=cfg.bidirectional,\n",
    ").to(device)\n",
    "model, history = train_loop(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=5,\n",
    "            cfg=cfg,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d57d107",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading test data...\")\n",
    "test_df = pd.read_csv('data/raw/test.csv')\n",
    "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
    "ticker_dict = {}\n",
    "for ticker, group in tqdm(df.groupby('ticker'), desc=\"Processing tickers\"):\n",
    "    group = group.sort_values('date').reset_index(drop=True)\n",
    "    ticker_dict[ticker] = {\n",
    "        'dates': group['date'].values,\n",
    "        'features': group[FEATURE_COLS].values.astype(np.float32)\n",
    "    }\n",
    "\n",
    "model.eval()\n",
    "window_size = 60\n",
    "predictions = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Predicting\"):\n",
    "    ticker_id = row['ID']\n",
    "    target_date = pd.Timestamp(row['Date'])\n",
    "\n",
    "    if ticker_id not in ticker_dict:\n",
    "        predictions.append(1)\n",
    "        continue\n",
    "\n",
    "    ticker_data = ticker_dict[ticker_id]\n",
    "    dates = pd.to_datetime(ticker_data['dates'])\n",
    "    features = ticker_data['features']\n",
    "\n",
    "    mask = dates < target_date\n",
    "    if mask.sum() < window_size:\n",
    "        predictions.append(1)\n",
    "        continue\n",
    "\n",
    "    valid_indices = np.where(mask)[0]\n",
    "\n",
    "    last_window = valid_indices[-window_size:]\n",
    "\n",
    "    X = features[last_window].copy()\n",
    "\n",
    "    X_tensor = torch.from_numpy(X).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda'):  \n",
    "            logits = model(X_tensor)\n",
    "\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "        pred = 1 if prob > 0.5 else 0\n",
    "\n",
    "    predictions.append(pred)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'Pred': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
